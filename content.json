{"meta":{"title":"LUOXIAO'S BLOG","subtitle":null,"description":null,"author":"Luoxiao","url":"luoxiao.cf","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2019-11-13T04:56:08.670Z","updated":"2019-11-13T04:56:08.670Z","comments":false,"path":"/404.html","permalink":"luoxiao.cf//404.html","excerpt":"","text":""},{"title":"关于","date":"2019-11-13T04:56:08.680Z","updated":"2019-11-13T04:56:08.680Z","comments":false,"path":"about/index.html","permalink":"luoxiao.cf/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2019-11-13T04:56:08.680Z","updated":"2019-11-13T04:56:08.680Z","comments":false,"path":"books/index.html","permalink":"luoxiao.cf/books/index.html","excerpt":"","text":"算法"},{"title":"友情链接","date":"2019-11-13T04:56:08.682Z","updated":"2019-11-13T04:56:08.682Z","comments":true,"path":"links/index.html","permalink":"luoxiao.cf/links/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-11-13T04:56:08.680Z","updated":"2019-11-13T04:56:08.680Z","comments":false,"path":"categories/index.html","permalink":"luoxiao.cf/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-11-13T04:56:08.682Z","updated":"2019-11-13T04:56:08.682Z","comments":false,"path":"repository/index.html","permalink":"luoxiao.cf/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-13T04:56:08.682Z","updated":"2019-11-13T04:56:08.682Z","comments":false,"path":"tags/index.html","permalink":"luoxiao.cf/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"神经网络：基本分类[转]","slug":"2019–08-20-神经网络：基本分类[转]","date":"2019-08-20T22:39:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/08/20/2019–08-20-神经网络：基本分类[转]/","link":"","permalink":"luoxiao.cf/2019/08/20/2019–08-20-神经网络：基本分类[转]/","excerpt":"","text":"代码式例：Run in Gogle Colab / View source on Github本指南转自TensorFlow官网。主要训练了一个神经网络模型，来对服装图像进行分类，例如运动鞋和衬衫。如果您不了解所有细节也不需要担心，这是一个对完整TensorFlow项目的简要概述，相关的细节会在需要时进行解释本指南使用tf.keras，这是一个用于在TensorFlow中构建和训练模型的高级API。1234567891011from __future__ import absolute_import, division, print_function, unicode_literals# 导入TensorFlow和tf.kerasimport tensorflow as tffrom tensorflow import keras# 导入辅助库import numpy as npimport matplotlib.pyplot as pltprint(tf.__version__)结果如下12.0.0-beta1 导入Fashion MNIST数据集本指南使用Fashion MNIST 数据集，其中包含了10个类别中共70,000张灰度图像。图像包含了低分辨率（28 x 28像素）的单个服装物品，如下所示:Fashion MNIST 旨在替代传统的MNIST数据集 — 它经常被作为机器学习在计算机视觉方向的&quot;Hello, World&quot;。MNIST数据集包含手写数字（0,1,2等）的图像，其格式与我们在此处使用的服装相同。本指南使用Fashion MNIST进行多样化，因为它比普通的MNIST更具挑战性。两个数据集都相对较小，用于验证算法是否按预期工作。它们是测试和调试代码的良好起点。我们将使用60,000张图像来训练网络和10,000张图像来评估网络模型学习图像分类任务的准确程度。您可以直接从TensorFlow使用Fashion MNIST，只需导入并加载数据123fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()结果:12345678Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz32768/29515 [=================================] - 0s 0us/stepDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz26427392/26421880 [==============================] - 0s 0us/stepDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz8192/5148 [===============================================] - 0s 0us/stepDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz4423680/4422102 [==============================] - 0s 0us/step加载数据集并返回四个NumPy数组:train_images和train_labels数组是训练集—这是模型用来学习的数据。模型通过测试集进行测试, 即test_images与 test_labels两个数组。图像是28x28 NumPy数组，像素值介于0到255之间。labels是一个整数数组，数值介于0到9之间。这对应了图像所代表的服装的类别:标签类别0T-shirt/top1Trouser2Pullover3Dress4Coat5Sandal6Shirt7Sneaker8Bag9Ankle boot每个图像都映射到一个标签。由于类别名称不包含在数据集中,因此把他们存储在这里以便在绘制图像时使用:1class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] 探索数据让我们在训练模型之前探索数据集的格式。以下显示训练集中有60,000个图像，每个图像表示为28 x 28像素:1train_images.shape同样，训练集中有60,000个标签:1len(train_labels)每个标签都是0到9之间的整数:1train_labels测试集中有10,000个图像。 同样，每个图像表示为28×28像素:1test_images.shape测试集包含10,000个图像标签:1len(test_labels) 数据预处理在训练网络之前必须对数据进行预处理。 如果您检查训练集中的第一个图像，您将看到像素值落在0到255的范围内:12345plt.figure()plt.imshow(train_images[0])plt.colorbar()plt.grid(False)plt.show()在馈送到神经网络模型之前，我们将这些值缩放到0到1的范围。为此，我们将像素值值除以255。重要的是，对训练集和测试集要以相同的方式进行预处理:123train_images = train_images / 255.0test_images = test_images / 255.0显示训练集中的前25个图像，并在每个图像下方显示类名。验证数据格式是否正确，我们是否已准备好构建和训练网络。123456789plt.figure(figsize=(10,10))for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]])plt.show() 构建模型构建神经网络需要配置模型的层，然后编译模型。设置网络层一个神经网络最基本的组成部分便是网络层。网络层从提供给他们的数据中提取表示，并期望这些表示对当前的问题更加有意义大多数深度学习是由串连在一起的网络层所组成。大多数网络层，例如tf.keras.layers.Dense，具有在训练期间学习的参数。12345model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax)])网络中的第一层, tf.keras.layers.Flatten, 将图像格式从一个二维数组(包含着28x28个像素)转换成为一个包含着28 * 28 = 784个像素的一维数组。可以将这个网络层视为它将图像中未堆叠的像素排列在一起。这个网络层没有需要学习的参数;它仅仅对数据进行格式化。在像素被展平之后，网络由一个包含有两个tf.keras.layers.Dense网络层的序列组成。他们被称作稠密链接层或全连接层。 第一个Dense网络层包含有128个节点(或被称为神经元)。第二个(也是最后一个)网络层是一个包含10个节点的softmax层—它将返回包含10个概率分数的数组，总和为1。每个节点包含一个分数，表示当前图像属于10个类别之一的概率。编译模型在模型准备好进行训练之前，它还需要一些配置。这些是在模型的编译(compile)步骤中添加的:损失函数 —这可以衡量模型在培训过程中的准确程度。 我们希望将此函数最小化以&quot;驱使&quot;模型朝正确的方向拟合。优化器 —这就是模型根据它看到的数据及其损失函数进行更新的方式。’评价方式 —用于监控训练和测试步骤。以下示例使用准确率(accuracy)，即正确分类的图像的分数。123model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 训练模型训练神经网络模型需要以下步骤:1. 将训练数据提供给模型 - 在本案例中，他们是train_images和train_labels数组。 2. 模型学习如何将图像与其标签关联 3. 我们使用模型对测试集进行预测, 在本案例中为test_images数组。我们验证预测结果是否匹配test_labels数组中保存的标签。 4. 通过调用model.fit方法来训练模型 — 模型对训练数据进行&quot;拟合&quot;。 1model.fit(train_images, train_labels, epochs=5)结果:123456789101112131415161718WARNING: Logging before flag parsing goes to stderr.W0703 00:57:16.227570 140360607328000 deprecation.py:323] From /tmpfs/src/tf_docs_env/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.Instructions for updating:Use tf.where in 2.0, which has the same broadcast rule as np.whereTrain on 60000 samplesEpoch 1/560000/60000 [==============================] - 5s 85us/sample - loss: 0.4949 - accuracy: 0.8267Epoch 2/560000/60000 [==============================] - 6s 92us/sample - loss: 0.3753 - accuracy: 0.8641Epoch 3/560000/60000 [==============================] - 5s 89us/sample - loss: 0.3355 - accuracy: 0.8784Epoch 4/560000/60000 [==============================] - 4s 72us/sample - loss: 0.3136 - accuracy: 0.8855Epoch 5/560000/60000 [==============================] - 4s 69us/sample - loss: 0.2955 - accuracy: 0.8921&lt;tensorflow.python.keras.callbacks.History at 0x7fa81c2c1b38&gt;随着模型训练，将显示损失和准确率等指标。该模型在训练数据上达到约0.88(或88％)的准确度。 评估准确率接下来，比较模型在测试数据集上的执行情况:123test_loss, test_acc = model.evaluate(test_images, test_labels)print('Test accuracy:', test_acc)结果：1210000/10000 [==============================] - 1s 53us/sample - loss: 0.3489 - accuracy: 0.8713Test accuracy: 0.8713事实证明，测试数据集的准确性略低于训练数据集的准确性。训练精度和测试精度之间的差距是过拟合的一个例子。过拟合是指机器学习模型在新数据上的表现比在训练数据上表现更差。 进行预测通过训练模型，我们可以使用它来预测某些图像。1predictions = model.predict(test_images)在此，模型已经预测了测试集中每个图像的标签。我们来看看第一个预测:1predictions[0]结果：123array([6.6858855e-05, 2.5964803e-07, 5.3627105e-06, 4.5019146e-06, 2.7420206e-06, 4.7881842e-02, 2.3233067e-04, 5.4705784e-02, 8.5581087e-05, 8.9701480e-01], dtype=float32)预测是10个数字的数组。这些描述了模型的&quot;信心&quot;，即图像对应于10种不同服装中的每一种。我们可以看到哪个标签具有最高的置信度值：1np.argmax(predictions[0])9因此，模型最有信心的是这个图像是ankle boot，或者 class_names[9]。 我们可以检查测试标签，看看这是否正确:1test_labels[0]9我们可以用图表来查看全部10个类别123456789101112131415161718192021222324252627282930def plot_image(i, predictions_array, true_label, img): predictions_array, true_label, img = predictions_array[i], true_label[i], img[i] plt.grid(False) plt.xticks([]) plt.yticks([]) plt.imshow(img, cmap=plt.cm.binary) predicted_label = np.argmax(predictions_array) if predicted_label == true_label: color = 'blue' else: color = 'red' plt.xlabel(\"&#123;&#125; &#123;:2.0f&#125;% (&#123;&#125;)\".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label]), color=color)def plot_value_array(i, predictions_array, true_label): predictions_array, true_label = predictions_array[i], true_label[i] plt.grid(False) plt.xticks([]) plt.yticks([]) thisplot = plt.bar(range(10), predictions_array, color=\"#777777\") plt.ylim([0, 1]) predicted_label = np.argmax(predictions_array) thisplot[predicted_label].set_color('red') thisplot[true_label].set_color('blue')让我们看看第0个图像，预测和预测数组。1234567i = 0plt.figure(figsize=(6,3))plt.subplot(1,2,1)plot_image(i, predictions, test_labels, test_images)plt.subplot(1,2,2)plot_value_array(i, predictions, test_labels)plt.show()1234567i = 12plt.figure(figsize=(6,3))plt.subplot(1,2,1)plot_image(i, predictions, test_labels, test_images)plt.subplot(1,2,2)plot_value_array(i, predictions, test_labels)plt.show()让我们绘制几个图像及其预测结果。正确的预测标签是蓝色的，不正确的预测标签是红色的。该数字给出了预测标签的百分比(满分100)。请注意，即使非常自信，也可能出错。12345678910111213# 绘制前X个测试图像，预测标签和真实标签# 以蓝色显示正确的预测，红色显示不正确的预测num_rows = 5num_cols = 3num_images = num_rows*num_colsplt.figure(figsize=(2*2*num_cols, 2*num_rows))for i in range(num_images): plt.subplot(num_rows, 2*num_cols, 2*i+1) plot_image(i, predictions, test_labels, test_images) plt.subplot(num_rows, 2*num_cols, 2*i+2) plot_value_array(i, predictions, test_labels)plt.show()最后，使用训练的模型对单个图像进行预测。1234# 从测试数据集中获取图像img = test_images[0]print(img.shape)(28, 28)tf.keras模型经过优化，可以一次性对批量,或者一个集合的数据进行预测。因此，即使我们使用单个图像，我们也需要将其添加到列表中:1234# 将图像添加到批次中，即使它是唯一的成员。img = (np.expand_dims(img,0))print(img.shape)(1, 28, 28)现在来预测图像:123predictions_single = model.predict(img)print(predictions_single)结果：12[[6.6858927e-05 2.5964729e-07 5.3627055e-06 4.5019060e-06 2.7420206e-06 4.7881793e-02 2.3233047e-04 5.4705758e-02 8.5581087e-05 8.9701480e-01]]123plot_value_array(0, predictions_single, test_labels)plt.xticks(range(10), class_names, rotation=45)plt.show()model.predict返回一个包含列表的列表，每个图像对应一个列表的数据。获取批次中我们(仅有的)图像的预测:12prediction_result = np.argmax(predictions_single[0])print(prediction_result)9而且，和之前一样，模型预测标签为9。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"luoxiao.cf/categories/机器学习/"}],"tags":[]},{"title":"HiveSQL优化","slug":"2019–08-17-HiveSQL优化","date":"2019-08-17T21:54:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/08/17/2019–08-17-HiveSQL优化/","link":"","permalink":"luoxiao.cf/2019/08/17/2019–08-17-HiveSQL优化/","excerpt":"","text":"下面列举一些常用的SQL优化方案 sql引起的数据倾斜数据倾斜会导致某个Reduce运行过慢影响到整体的运行时长。通常在join和group by时，会出现这样的问题join引起的数据倾斜,下面操作会将一个job变为两个job执行HQL1234#如果是join过程出现倾斜，应该设置为trueset hive.optimize.skewjoin=true;#这个是join的键对应的记录条数超过这个值则会进行优化set hive.skewjoin.key=100000;group by key引起的数据倾斜1234# 如果group by过程出现倾斜应该设置为trueset hive.groupby.skewindata=true;#这个是group的键对应的记录条数超过这个值则会进行优化set hive.groupby.mapaggr.checkinterval=100000; mapjoin(map端执行join）针对应用场景合理使用MapJoin也很重要。Map Join可以解决数据倾斜问题，因为没有Reduce Task了;只运行Map Task，相比多运行Reduce Task来说省时间。启动方式一：(自动)123set.hive.auto.convert.join = true;#默认值是25mb小表小于25mb自动启动mapjoin hive.mapjoin.smalltable.filesize=25000000启动方式二：(手动）1select /*+mapjoin(A)*/ f.a,f.b from A t join B f on (f.a=t.a) bucketjoin合理利用桶分区很重要，因为它可以避免全表检索，在大数据场景中全表检索意味着什么应该可以想象…在满足下面两个情况时使用：1.两个表以相同方式划分桶2.两个表的桶个数是倍数关系1234create table order(cid int,price float) clustered by(cid) into 32 buckets;create table customer(id int,first string) clustered by(id) into 32/64 buckets;select price from order t join customer s on t.cid=s.id; where条件优化尽可能早的筛掉更多的数据。优化前（关系数据库不用考虑会自动优化）：1select m.cid,u.id from order m join customer u on m.cid =u.id where m.dt='2019-08-18';优化后(where条件在map端执行而不是在reduce端执行）：1select m.cid,u.id from （select * from order where dt='2019-08-18'） m join customer u on m.cid =u.id; count distinct优化只有一个reduce，先去重再count负担比较大;解决方案是：启动两个job，一个job负责子查询(可以有多个reduce)，另一个job负责count(1)优化前：1select count(distinct id) from tablename;优化后：12345select count(1) from (select distinct id from tablename) tmp;select count(1) from (select id from tablename group by id) tmp;set mapred.reduce.tasks=3; 合理使用union all优化前：1select a,sum(b),count(distinct c),count(distinct d) from test group by a;优化后：1234567select a,sum(b) as b,count(c) as c,count(d) as d from（select a, 0 as b,c,null as d from test group by a,cunion allselect a,0 as b, null as c,d from test group by a,dunion allselect a, b,null as c ,null as d from test) tmp group by a;","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[]},{"title":"HDMI连接Raspberry","slug":"2019–07-29-HDMI连接Raspberry","date":"2019-07-29T22:26:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/29/2019–07-29-HDMI连接Raspberry/","link":"","permalink":"luoxiao.cf/2019/07/29/2019–07-29-HDMI连接Raspberry/","excerpt":"","text":"通常情况下，树莓派会自动检测显示器的类型并修改配置。但有时，自动检测的结果可能不正确。如果你的树莓派连接到电视上但没有任何显示的话，你要考虑手动修改树莓派的显示配置了下面我们手动修改/boot/config.txt文件。记得修改前备份一个，以下是参数文件：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# For more options and information see# http://rpf.io/configtxt# Some settings may impact device functionality. See link above for details# uncomment if you get no picture on HDMI for a default \"safe\" mode#hdmi_safe=1# uncomment this if your display has a black border of unused pixels visible# and your display can output without overscandisable_overscan=1# uncomment the following to adjust overscan. Use positive numbers if console# goes off screen, and negative if there is too much border#overscan_left=16#overscan_right=16#overscan_top=16#overscan_bottom=16# uncomment to force a console size. By default it will be display's size minus# overscan.#framebuffer_width=1280#framebuffer_height=720# uncomment if hdmi display is not detected and composite is being outputhdmi_force_hotplug=1# uncomment to force a specific HDMI mode (this will force VGA)hdmi_group=1hdmi_mode=4# uncomment to force a HDMI mode rather than DVI. This can make audio work in# DMT (computer monitor) modeshdmi_drive=2# uncomment to increase signal to HDMI, if you have interference, blanking, or# no displayconfig_hdmi_boost=4# uncomment for composite PAL#sdtv_mode=2#uncomment to overclock the arm. 700 MHz is the default.#arm_freq=800# Uncomment some or all of these to enable the optional hardware interfaces#dtparam=i2c_arm=on#dtparam=i2s=on#dtparam=spi=on# Uncomment this to enable the lirc-rpi module#dtoverlay=lirc-rpi# Additional overlays and parameters are documented /boot/overlays/README# Enable audio (loads snd_bcm2835)dtparam=audio=onstart_x=1gpu_mem=128enable_uart=1#disable_camera_led=1hdmi_ignore_edid=0xa5000080看这个https://wenku.baidu.com/view/a8a1554e71fe910ef02df893.html，学一下各个参数详解学完之后了解一下怎么调hdmi_mode这个参数https://zhidao.baidu.com/question/519865882625562245.html","categories":[{"name":"物联网","slug":"物联网","permalink":"luoxiao.cf/categories/物联网/"}],"tags":[]},{"title":"Google Colab","slug":"2019–07-23-Google Colab","date":"2019-07-23T23:34:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/23/2019–07-23-Google Colab/","link":"","permalink":"luoxiao.cf/2019/07/23/2019–07-23-Google Colab/","excerpt":"","text":"训练模型太慢怎么办？训练模型的时间，好点的用1小时2小时，遇到稍复杂的得等1天甚至2天。训练后的模型，如果不满足要求，还得再反复调整，再来一遍…这简直是个噩梦有么有。去某宝、JD上搜一搜，一台上万谁买的起。对于我们这些穷得揭不开锅的苦比党来说，Google Colab可以说是我们的福音！ Google GolabColaboratory 是一款研究工具，用于进行机器学习和研究。它是一个 Jupyter 笔记本环境，重点是它不需要进行任何设置就可以跑代码，而且性能方面Google提供了Tesla K80 GPU，很给力了。而且它还是永久免费的，仿佛发现了新大陆ヾ(≧O≦)〃，虽然不知道性能到底怎么样，但是跟我这用了5年的烂本子比，已经不知道好到哪去了。 官网首先这是Google的东西，想用肯定得FQ，这没啥说的，访问下面链接Google Colab。Google Colab 支持Python2和Python3语言;想用R和Scala的小伙伴得忍忍了，Google方面正在研发对他们的支持，后续会开方相关功能。 记事本写代码我们需要先建个“记事本”，可以通过下面两种方法建立:1.第一次进入会弹出一个框，点框下面的 &quot;NEW PYTHON 3 NOTBOOK&quot;新建“记事本”2.这个框关掉以后，左上角找到 File-&gt;New Python 3 Notbook 用例点击“CODE”创建一个代码片段，你可以创建多个代码片段。下面用Python3 测试一下环境是否正常,试着打印tensorflow版本号：12import tensorflow as tfprint(tf.__version__)点击画的红色框框运行代码参考文档汇总:官方问答:https://research.google.com/colaboratory/faq.html#browsers","categories":[{"name":"机器学习","slug":"机器学习","permalink":"luoxiao.cf/categories/机器学习/"}],"tags":[]},{"title":"堆排序的应用-优先队列","slug":"2019–07-18-堆排序的应用-优先队列","date":"2019-07-18T23:52:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/18/2019–07-18-堆排序的应用-优先队列/","link":"","permalink":"luoxiao.cf/2019/07/18/2019–07-18-堆排序的应用-优先队列/","excerpt":"","text":"堆排序在排序复杂性的研究中有着重要的地位，因为它是我们所知的唯一能够同时最优的利用空间和时间的方法-在最坏的情况下它能保证使用～2NlgN次比较和恒定的额外空间。在开始了解优先队列之前我们先了解一下堆的特性：一个大根堆有这么个特性，它的爸爸总是比它的俩孩子的值大;除了这个最基本的以外，你还要知道第k个元素的左孩子是2k，右孩子是2k+1;知道这个以后，下面我们需要实现两个方法，高效的删除最大元素和插入元素。如果新插入一个数，那么根据前面的特性，只需要不断循环的用自身和自己的爸爸（k/2）比较大小，根据比较结果判断是否要交换位置即可;如果要删掉一个最大的数，只需要将根与最后一个数交换位置(因为根是大根堆中最大的数)，将其脱离堆结构，然后将根节点不断和它的孩子（2K、2K+1）比较大小，下沉到合适位置即可；为了满足k,2k,2k+1的这种层级关系，后续将舍弃数组下标为0的位置，因为2*0会影响到这种层级关系的判断。下面我们说一下下沉（sink）和上浮（swim）的实现方法 上浮如果堆的有序状态因为某个节点比它的父节点更大而被打破，那么我们就需要通过交换它和它的父节点来修复堆。12345678private void swim(int k)&#123; while(k &gt; 1 &amp;&amp; less(k/2,k))&#123; exch(k/2,k); k = k/2; &#125;&#125; 下沉与上浮相反，如果堆有序被打破，k节点想要下沉到合适的位置，代码应该这么写。12345678910private void sink(int k)&#123; while(2*k &lt;= N)&#123; int j = 2*k; if(j &lt; N &amp;&amp; less(j,j+1)) j++; if(!less(k,j)) break; exch(k,j); k = j; &#125;&#125; 基于堆的优先队列下面我们实现一下堆的优先队列。优先队列由一个基于堆的完全二叉树表示，存储于数组pq[1…N]中,pq[0]不用，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class MaxPQ&lt;Key extends Comparable&lt;Key&gt;&gt;&#123; private Key[] pq; private int N = 0; public MaxPQ(int maxN)&#123; pq = (Key[]) new Comparable[maxN + 1]; &#125; public boolean isEmpty()&#123; return N == 0; &#125; public int size ()&#123; return N; &#125; public void insert(Key v)&#123; pq[++N] = v; swim(N); &#125; public Key delMax()&#123; Key max = pq[1]; exch(1,N--); pq[N+1] = null; sink(1); return max; &#125; private boolean less(int i,int j)&#123; return pq[i] &lt; pq[j]; &#125; private void exch(i,j)&#123; Key t = pq[i]; pq[i] = pq[j]; pq[j] = t; &#125; private void swim(int k)&#123; ... &#125; private void sink(int k)&#123; ... &#125;&#125;在insert()中，将N加1并把新元素添加到数组最后，然后用swim()恢复秩序。在delMax()中。从pq[1]中得到需要返回的元素，然后将pq[N]移动到pq[1],将N减一并用sink()恢复对的秩序。将pq[N+1]设为null，以便GC回收其所占空间。","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"Hive迁移老数据-动态分区","slug":"2019–07-17-迁移老数据动态分区","date":"2019-07-18T11:18:55.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/18/2019–07-17-迁移老数据动态分区/","link":"","permalink":"luoxiao.cf/2019/07/18/2019–07-17-迁移老数据动态分区/","excerpt":"","text":"现在有个hive表 dw_revisit_user_d ，创建的时候没有加partitioned by,现在想拓展表中的datestr当分区列。要怎么保证原来数据不丢，并且让原来的数据按datestr分区，以后的数据也按datestr分区？我们可以使用select…insert + 动态分区解决问题 步骤123456789101112set hive.exec.dynamic.partition.mode=nonstrict;create table t_1(datestr string,u_id string,acc_cnt bigint);insert into table t_1 values('2019-07-17','1',22);insert into table t_1 values('2019-07-17','2',24);insert into table t_1 values('2019-07-16','3',255);insert overwrite table t_2 partition (datestr)select u_id,acc_cnt,datestr from t_1 where datestr = '2019-07-17';insert overwrite table t_2 partition (datestr)select u_id,acc_cnt,datestr from t_1 where datestr = '2019-07-16';加上下面这句话，不带where直接自动匹配datestr123set hive.exec.dynamici.partition=true;insert overwrite table t_2 partition (datestr)select u_id,acc_cnt,datestr from t_1;之后删掉t_1 修改t_2为t_1就行了","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"hive","slug":"hive","permalink":"luoxiao.cf/tags/hive/"}]},{"title":"ceph集群搭建","slug":"2019-07-17-ceph","date":"2019-07-17T17:43:17.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/07/17/2019-07-17-ceph/","link":"","permalink":"luoxiao.cf/2019/07/17/2019-07-17-ceph/","excerpt":"","text":"作者林文杰；结构图如下图所示 1、Selinux、防火墙和hostname文件将/etc/selinux/config文件下SELINUX=disabled关闭防火墙或者开通6789和6800：7300的端口1vim /etc/hostname 2、配置用户123456useradd -m bdipcephpasswd bdipceph#设置su权限echo \"bdipceph ALL = (root) NOPASSWD:ALL\" | tee /etc/sudoers.d/bdipcephchmod 0440 /etc/sudoers.d/bdipceph 3、将各节点写入hosts123echo 192.168.140.130 ceph130 &gt;&gt; /etc/hostsecho 192.168.140.133 ceph133 &gt;&gt; /etc/hostsecho 192.168.140.134 ceph134 &gt;&gt; /etc/hosts 4、配置免密123456su bdipcephssh-keygen -t rsassh-copy-id ceph130ssh-copy-id ceph133ssh-copy-id ceph134 5、安装ntp服务和配置yum仓库1vim /etc/yum.repo.d/CentOS-ceph.repo下面做参考1234567891011121314151617181920212223[Ceph]name=Ceph packages for $basearchbaseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc 6、安装存储集群12345678910111213yum install ceph-deploysu cephdmkdir ~/my-clusterceph-deploy new ceph133osd pool default size = 2ceph-deploy install ceph130 ceph133 ceph134ceph-deploy mon create-initialceph-deploy osd prepare ceph133:/var/local/osd0 ceph134:/var/local/osd1ceph-deploy osd activate ceph133:/var/local/osd0 ceph134:/var/local/osd1ceph-deploy admin ceph130 ceph133 ceph134chmod +r /etc/ceph/ceph.client.admin.keyringceph health 7、删除osd123456789ceph osd out 1systemctl stop ceph-osd@1ceph osd crush remove osd.1ceph auth del osd.1ceph osd rm osd.1#删除旧文件夹rm -rf /var/local/osd1#调整权重ceph osd crush reweight osd.1 0 8、配置文件修改发送1ceph-deploy --overwrite-conf config push bdipceph104 bdipceph106 bdipceph108 9、配置文件内容123456789#设置日志文件大小osd journal size = 1024#设置节点数量osd pool default size = 3#设置副本数量osd pool default min size = 2#设置ext4能够使用osd max object name len = 256osd max object namespace len = 64 10、创建块设备1234567891011121314151617181920212223#创建映像foo大小4Grbd create --size 4096 foo#查询映像rbd ls#查询单个映像信息rbd info foo#扩大映像大小rbd resize --size 8192 fooresize2fs /dev/rbd0#减小映像大小(会损坏数据)rbd resize --size 4096 foo --allow-shrink#删除映像rbd rm foo#linux环境需要执行下面这条命令rbd feature disable foo exclusive-lock object-map fast-diff deep-flatten#映射块设备rbd map foo --name client.admin #查看已映射块设备rbd showmapped#取消块设备映射rbd unmap /dev/rbd0mkfs.ext4 /dev/rbd0 11、创建块设备快照12345678910#创建快照rbd snap create rbd/foo@snapfoo#罗列快照rbd snap ls rbd/foo#回滚快照rbd snap rollback rbd/foo@snapfoo#删除快照rbd snap rm rbd/foo@snapfoo#清除快照rbd snap purge rbd/foo","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"ceph","slug":"ceph","permalink":"luoxiao.cf/tags/ceph/"}],"author":"张帅"},{"title":"Export requires a --table or a --call argument","slug":"2019–07-16-Export-requires-a--table-or-a--call-argument","date":"2019-07-16T11:03:59.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/16/2019–07-16-Export-requires-a--table-or-a--call-argument/","link":"","permalink":"luoxiao.cf/2019/07/16/2019–07-16-Export-requires-a--table-or-a--call-argument/","excerpt":"","text":"参考官方文档10. sqoop-export使用Sqoop导入数据到RDSMS数据库，结果报错；错误也是让人摸不着头脑，命令中是有这些参数的。 解决办法做下面修改，在值的左右加上双引号即可。1--connect \"jdbc:sqlserver://ip:port;database=database\" \\","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"分布式幂等性设计","slug":"2019–07-15-分布式幂等性设计","date":"2019-07-15T20:57:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/15/2019–07-15-分布式幂等性设计/","link":"","permalink":"luoxiao.cf/2019/07/15/2019–07-15-分布式幂等性设计/","excerpt":"","text":"幂等性幂等性指的是，请求一次或者是多次资源应该具有相同的副作用。 重要性在一些业务场景下，幂等性显得很重要；比如金融系统、电商系统，这些都是很敏感的系统，作为软件开发者，我们有必要了解一下它。订单创建接口，第一次调用超时了，如果再调用一次会不会再创建一个订单？购买商品时，减库存接口超时了，再调一次会不会扣减2个库存？当一笔订单开始支付，支付请求发出后，服务端发生了扣钱操作，接口响应超时了，调用方再重试一次，会不会多扣一笔钱？下面我们介绍一下怎么解决上面所述的问题。 设计通常我们有两个方法来保证幂等性。也就是不管调用接口多少次，都要产生相同的副作用：下游做一个查询接口，上游系统发现请求超时后，调用查询接口。发现成功了，逻辑里面什么都不用做；如果发现失败了，走失败流程进行后续处理保证最终结果正确。把这个查询操作交给下游系统，上游系统只负责重试，下游系统要在代码上保证一次或多次请求结果是一样的.第一种没什么好说的，很好实现；第二种则需要利用一个全局ID来辅助完成。由于我们的系统是一个分布式的，要做到全局唯一貌有点小难度，分布式中，子系统很多。该由谁来维护这个全局ID？这里介绍一下Twitter 的开源项目Snowflake。它是一个分布式ID的生成算法，可以帮我们完成这个工作，你可以通过阅读下面这个文章来交接它https://www.cnblogs.com/haoxinyue/p/5208136.html","categories":[{"name":"分布式","slug":"分布式","permalink":"luoxiao.cf/categories/分布式/"}],"tags":[]},{"title":"排序空间复杂度与稳定性","slug":"2019–07-08-排序空间复杂度与稳定性","date":"2019-07-08T19:34:38.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/08/2019–07-08-排序空间复杂度与稳定性/","link":"","permalink":"luoxiao.cf/2019/07/08/2019–07-08-排序空间复杂度与稳定性/","excerpt":"","text":"8种经典排序算法已经整理完成，下面说一下他们的空间复杂度 O（1）插入排序、冒泡排序、选择排序、希尔排序、堆排序 O（n）～O（logn）快速排序 O（N）归并排序这里有一些网上和书上说可以将归并排序的空间复杂度优化到O（1），这边通过手摇算法确实可以使得空间复杂度达到O（1），但是时间复杂度会上升。 O（M）计数排序、基数排序这个M代表的是桶的数量 稳定性所谓不稳定性，指的是相同元素经过排序后，改变了原数组中数的位置，即为不稳定的排序算法。8种排序算法中，有选择排序、快速排序、希尔排序和堆排序他们是不稳定的排序算法那么我们下面说一下，对于序列（5,5,5,1），为什么他们会是不稳定的排序算法 选择排序选择排序，会将1与第一个5进行交换位置，导致相同元素排序后位置改变。 快速排序对于快排而言，开始任意选择一个数，假如选到了第二个5,那么小于等于它的都将会被放到第二个5的左边，导致相同元素排序后位置改变。 希尔排序假如希尔排序的步长选为2,在1和第二个5比较以后，会和它交换位置，导致相同元素排序后位置改变。 堆排序将上面元素映射为大根堆，堆顶元素会和最后一个元素交换位置，导致相同元素排序后位置改变。 小结在解决工程问题时，通常会使用多个排序算法相结合的套路，来解决问题。面对问题时要活学活用，比如使用计数排序解决按身高排序的问题很高效，但是放在解决按工资排序的问题上就不是那么好了。一般，对于数量不大的情况下，通常选取时间复杂度为O（n^2）的插入排序算法对于数量很大的情况下，通常选取快速排序，或者是其他的时间复杂度为O（nlogn）的排序算法","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"计数和基数排序","slug":"2019–07-08-计数和基数排序","date":"2019-07-08T19:34:38.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/08/2019–07-08-计数和基数排序/","link":"","permalink":"luoxiao.cf/2019/07/08/2019–07-08-计数和基数排序/","excerpt":"","text":"O（N）的排序算法也叫不比较的排序算法，它的思想源于桶排序，其中比较经典的两个例子计数排序和基数排序，它们的时间复杂度趋向于O（n）你可能会纳闷，不比较也能排序？下面我们介绍一下这两种经典排序算法 计数排序假如要给公司员工按身高排序。我们知道员工身高大部分在160，180 之间，建立100～300一共200个桶。遍历所有员工，按身高把员工放到匹配的桶中分别倒出100～300号桶中的员工，这就是一个按身高排好序的序列。 基数排序给下面序列 （124，220，044，120，334，666，001，099）排序，其中都为10进制数；建立 0～9 共10个桶根据上面数的个位，分别放到对应的桶中，比如124，个位是4，就放到4对应的桶中；依次倒出所有数，再根据数的十位，分别放到对应的桶中；依次倒出所有数，再根据数的百位，分别放到对应的桶中；依次倒出所有的数，该序列就是从小到大的有序序列；下面这个网站中可以给你一些好的桶排序的思想，如果有时间，不妨看一下图解Bucket SortCounting SortRadix Sort","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"希尔、归并、堆、快速排序","slug":"2019–07-03-希尔、归并、堆、快速排序","date":"2019-07-08T19:34:38.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/07/08/2019–07-03-希尔、归并、堆、快速排序/","link":"","permalink":"luoxiao.cf/2019/07/08/2019–07-03-希尔、归并、堆、快速排序/","excerpt":"","text":"下面我们介绍一下时间复杂度为O（nlogn）的时间复杂度:希尔排序、归并排序、堆排序、快速排序 堆排序将0～（n-1）的无序列表，映射为大根堆，根据大根堆的特性，堆顶为最大值;将堆顶与堆的最后一个元素交换位置，并将其脱离堆结构，放在数组n-1位置上;重新调整大根堆，重复步骤2，得到第n-2的数直到堆元素个数为1，即整个堆排序完成。图解流程访问下面链接中的 Data Structure Visualizations 希尔排序希尔排序是插入排序的进化版，插入排序的每次迁移步长为1,而希尔排序则通过动态调整步长，进而提高排序效率。假如选定步长为3下面说一下排序过程在0～（N-1）的无序序列中，数组中位置0、1、2三个数将被直接跳过。取3位置的数（a）和0位置上的数（b）比较。如果b&gt;a,则结束比较;如果b&lt;a,则交换b和a的位置，继续使用b和-3位置上的数比较，发现-3数组越界，结束比较;取位置4上的数和位置1上的数比较，重复上面过程;一直到最后一个位置n与n-3比较后结束步长为3的排序过程;让步长减1继续完成上面1、2、3的步骤，知道步长=0,结束整个希尔排序过程;图解流程访问下面链接中的Shell Sort Data Structure Visualizations12345678910111213141516171819202122public void sort(Comparable[] a) &#123; // 希尔排序其实是插入排序的一种优化 int len = a.length; int w = 1; // 自定义w的规则 while(w &lt; len / 3) &#123; w = 3*w + 1; &#125; //开始进行排序 while(w &gt;= 1)&#123; for(int i = w ; i &lt; len ; i ++)&#123; // 将a[i]插入到a[i-w]、a[i-2*w]、a[i-3*w].... for (int j = i ; j &gt;= w &amp;&amp; less(a[j],a[j-w]) ; j -=w )&#123; exch(a,j,j-w); &#125; &#125; w = w / 3; &#125;&#125; 归并排序在0～（n-1）的无序列表中，将大小为1的有序区间，合并为大小为2的有序区间;将大小为2的有序区间合并为大小为4的有序区间;直到有序区间大小容纳所有的数，归并排序完成;图解流程访问下面链接中的Merge Sort Data Structure Visualizations12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static Comparable[] aux;public void sort(Comparable[] a) &#123; aux = new Comparable[a.length]; sort(a,0,a.length-1);&#125;// 自上而下的归并排序public void sort(Comparable[] a,int lo,int hi) &#123; if(hi &lt;= lo) &#123; return; &#125; // a[lo...mid] 和 a[mid+1....hi] int mid = lo + (hi - lo)/2; sort(a,lo,mid); sort(a,mid+1,hi); merge(a,lo,mid,hi);&#125;// 合并 a[lo...mid] 和 a[mid+1....hi]public void merge(Comparable[] a, int lo, int mid, int hi) &#123; // 初始化两个指针 int i = lo; int j = mid + 1; // 将a复制到辅助数组aux for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // 开始拿两个指针对应的数进行比较 for (int k = lo; k &lt;= hi; k++) &#123; // j对应的数 比i小，将比较小的放在第k位置 if(j &gt; hi)&#123; a[k] = aux[i++]; &#125;else if(i &gt; mid)&#123;// 如果左边用完了，直接将右序列中的数放在k位置 a[k] = aux[j++]; &#125;else if(less(aux[j],aux[i]))&#123; // 如果右边用完了，直接将左序列中的数放在k位置 a[k] = aux[j++]; &#125;else&#123; a[k] = aux[i++]; &#125; &#125;&#125; 快速排序利用分治的思想，要对大小为N的无序序列排序，将其分为a[lo]-a[j-1],a[j],a[j+1]-a[hi]三部分，保证a[lo]-a[j-1]中的数永远小于等于a[j];a[j+1]-a[hi]中的数永远大于a[j];在分别递归的对a[lo]-a[j-1]和a[j+1]-a[hi]重复上面的步骤。下面我们说一下什么叫划分过程，划分过程就是怎么将小于等于a[j]的数放到a[j]的左边，大于a[j]的数放在了a[j]的右边：选择一个数a[lo]，初始化左指针 i = lo ，右指针 j = hi；i指针往右移，找到大于等于a[lo]的数(a);j指针往左移，找到小于a[lo]的数b。交换a和b的位置,然后i继续往右移，j继续往左移，直到两者交汇。将a[lo]与a[j]交换位置图解流程访问下面链接中的Quick Sort Data Structure Visualizations123456789101112131415161718192021222324252627282930313233343536public void sort(Comparable[] a) &#123; sort(a,0,a.length-1);&#125;// 快速排序public void sort(Comparable[] a,int lo ,int hi) &#123; if(hi &lt;= lo) &#123; return; &#125; // 找到j,分别对a[lo..j-1]和a[j+1...hi]进行递归快速的排序 int j = partition(a,lo,hi); // sort左序列\\ sort右序列 sort(a,lo,j-1); sort(a,j+1,hi);&#125;// 对a[lo...hi] 进行划分排序public int partition(Comparable[] a, int lo, int hi) &#123; // 定义两个指针 int i = lo; int j = hi + 1; Comparable v = a[lo]; while(true)&#123; // 从左边往右边一直找到第一个比v大的数 while(less(a[++i],v))if(i == hi)break; // 从右边往左边一直找到第一个比v小的数 while(less(v,a[--j]) )if(j == lo)break; if(i &gt;= j) break; exch(a,i,j); &#125; // 比较完成后将j和lo位置上的数互换 exch(a,lo,j); return j;&#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"冒泡、插入、选择排序","slug":"2019-07-03-冒泡、插入、选择排序","date":"2019-07-03T20:54:38.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/07/03/2019-07-03-冒泡、插入、选择排序/","link":"","permalink":"luoxiao.cf/2019/07/03/2019-07-03-冒泡、插入、选择排序/","excerpt":"","text":"下面我们来理一下时间复杂度为O（n^2）的排序算法：冒泡排序、插入排序和选择排序 冒泡排序在0～（N-1）的大小区间中，数组位置0和数组位置1上的进行比较，如果0位置上的大于1位置上的，交换他们的位置，否则不动；紧接着位置1上的和位置2上的进行比较，如果1位置上的大于2位置上的，交换他们的位置，如此反复第一轮下来，数组中最大的那个数会被放到数组的最后面。将0～（N-1）的区间缩小为0～（N-2），反复上述过程。第二轮下来后，最大的会被放到倒数第二个位置。以此类推完成后面各轮过程，直到数组区间大小为1，即整个过程结束。图解流程访问下面链接中的Bubble Sort Data Structure Visualizations1234567891011121314151617181920212223242526272829303132333435363738package mainimport \"fmt\"func less(a int, b int) bool &#123; return a&lt;b&#125;func exch(a *int, b *int)&#123; temp := *a *a = *b *b = temp&#125;func bsort(a *[10]int)&#123; count := len(a) //fmt.Println(count) for i := 0; i &lt; count; i++ &#123; for j := i + 1; j &lt; count; j++ &#123; if less(a[j], a[i]) &#123; exch(&amp;(*a)[j],&amp;(*a)[i]) &#125; &#125; &#125;&#125;func main()&#123; var a = [10]int&#123;3,14,2232,4,54,62,351,422,123,34&#125; fmt.Println(a) //fmt.Println(count) bsort(&amp;a) fmt.Println(a)&#125; 插入排序数组0位置（a）和数组1位置（b）上的数进行比较，如果后者b比前者a小，将b与a交换位置，紧接着b再和数组-1上的位置进行比较，发现数组越越界，结束第一轮的过程。将待处理区间的大小缩小为1～（N-1），数组1位置上的数（a）与数组2位置上的数（b）进行比较，同样的，如果后者b比前者a小，就将两个数进行交换，继续拿数组1位置上的数与数0位置上的数进行比较直到不满足后者小于前者，或者数组越界为止。结束第二轮的过程。反复上面的过程直到待比较区间大小为1停止整个插入排序。图解流程访问下面链接中的Insertion Sort Data Structure Visualizations12345678910public void sort(Comparable[] a)&#123; int len = a.length; for (int i =0 ; i &lt; len ; i++)&#123; // 将a[i] 插入到 a[i - 1] , a[i - 2], a[ i - 3] for (int j = i ; j &gt;= 1 &amp;&amp; less(a[j],a[j-1]) ; j -= 1)&#123; exch(a,j,j-1); &#125; &#125;&#125; 选择排序在0～（N-1）的比较区间中，从头到尾依次比较找出最小的数，放在位置0上，将区间长度变为1～（N-1）在1～（N-1）的比较区间中，从头到尾依次比较找出最小的数，放在位置1上，将区间长度变为2～（N-1）反复上述过程知道比较区间大小为0，结束整个排序过程。图解流程访问下面链接中的Selection Sort Data Structure Visualizations123456789101112public void sort(Comparable[] a)&#123; int len = a.length; for(int i = 0 ; i &lt; len ; i++)&#123; int min = i; for (int j = i + 1 ; j &lt; len ; j++)&#123; if( less(a[j],a[min]) ) min = j; &#125; exch (a,i,min); &#125; &#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"最好、最坏、平均 、均摊时间复杂度","slug":"2019-07-03-最好、最坏、平均 、均摊时间复杂度","date":"2019-07-03T20:42:05.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/07/03/2019-07-03-最好、最坏、平均 、均摊时间复杂度/","link":"","permalink":"luoxiao.cf/2019/07/03/2019-07-03-最好、最坏、平均 、均摊时间复杂度/","excerpt":"","text":"较为复杂的分析方法大致可分为四类、分别为：最好时间复杂度、最坏时间复杂度、平均时间复杂度和均摊时间复杂度。这里有一段代码，针对它下面分别来说一下怎么算着四种时间复杂度。12345678910111213// i的取值范围是 0～nvoid add(int element) &#123; if (i &gt;= len) &#123; int new_array[] = new int[len*2]; for (int j = 0; j &lt; len; ++j) &#123; new_array[j] = array[j]; &#125; array = new_array; len = 2 * len; &#125; array[i] = element; ++i;&#125; 最好、最坏时间复杂度最好时间复杂度，言明直意，就是在最好情况下求得的时间复杂度，对于上面代码，针对长度为N的数组添加一个元素的最好时间复杂度为O（1）在最好情况下，数组空间很充足，可以直接将数组添加到第i位置在最坏情况下，数组空间不够，所以要重新申请一个2倍大小的数组空间，把原来array数组中的数据依次copy到new_array，因此最坏的时间复杂度应该是O（N） 平均时间复杂度最好、坏的时间复杂度局限性很大，有时不能准确说明问题，针对这种情况，我们用代码执行各种情况的加权平均值来说明问题。假设数组的大小为N，i的取值范围为0～N，在0～n-1时间复杂度为O（1），在i等于N的时候时间复杂度为O（N），i的取值有1/（n-1）种可能性，所以有：最终平均时间复杂度是O（1） 均摊时间复杂度网上有好多说平均时间复杂度就是均摊时间复杂度，它们并没有什么区别，不管他们俩是否一样，这边有两个tip来帮助我们算出均摊时间复杂度在N种情况中，如果第被低阶复杂度占去了半壁江山，那么通过均摊更高阶的复杂度到低阶上，最终结果为低阶复杂度。假如你发现低阶复杂度和高阶复杂度出现规律性的交替，那么通常最终结果为低阶复杂度。根据上面的tips。很快就得出它的均摊时间复杂度为O（1） 为什么要引入这4种复杂度？一般我们用不到到这些分析方法，针对某些场景，如果普通的分析方法不能论证我们的论点，那么使用它们往往可以使论点更具有说服力。","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"时间空间复杂度","slug":"2019-07-01-时间空间复杂度","date":"2019-07-01T11:15:05.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/07/01/2019-07-01-时间空间复杂度/","link":"","permalink":"luoxiao.cf/2019/07/01/2019-07-01-时间空间复杂度/","excerpt":"","text":"算法与数据结构相辅相成，谁也离不开谁，学算法和数据结构可以说让我很痛苦，当然对于聪明的你来说，这可能不是什么难事,哈哈哈。这下面的图是算法的整个知识体系图，通过让我们对整个算法体系有个初步了解。我总结了20个最常用的、最基础数据结构与算法,他们分别是10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。如果想学好算法，就得掌握怎么分析代码的性能。业界中常用时间复杂度和空间复杂度来坑量，类似O(1)、O(n)、O(n^2)等。如果说不会求时间和空间复杂度，那只能说你和算法无缘了！下面我们来理理到底应该怎么算，另外算这些东西不需要什么很高深的数学，只需要高中数学知识就足够了。下面会牵扯到一些对数运算，如果你忘了，建议你回去复习一下。 大O复杂度表示法这里有段非常简单的代码，求1,2,3…n的累加和。现在，我就带你一块来估算一下这段代码的执行时间。12345678int cal(int n) &#123; int sum = 0; int i = 1; for (; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum;&#125;假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？第2、3行代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍，所以需要2n * unit_time的执行时间，所以这段代码总的执行时间就是(2n+2) * unit_time。可以看出来，所有代码的执行时间T(n)与每行代码的执行次数成正比按照这个分析思路，我们再来看这段代码。1234567891011int cal(int n) &#123; int sum = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) &#123; j = 1; for (; j &lt;= n; ++j) &#123; sum = sum + i * j; &#125; &#125;&#125;我们依旧假设每个语句的执行时间是unit_time。那这段代码的总执行时间T(n)是多少呢？第2、3、4行代码，每行都需要1个unit_time的执行时间，第5、6行代码循环执行了n遍，需要2n * unit_time的执行时间，第7、8行代码循环执行了n^2 遍，所以需要 2n^2 * unit_time的执行时间。所以，整段代码总的执行时间T(n) = (2n ^ 2 + 2n+3)*unit_time尽管我们不知道unit_time的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间T(n)与每行代码的执行次数n成正比。我们可以把这个规律总结成一个公式。注意，大O就要登场了！我来具体解释一下这个公式。其中，T(n)我们已经讲过了，它表示代码执行的时间；n表示数据规模的大小；f(n)表示每行代码执行的次数总和。因为这是一个公式，所以用f(n)来表示。公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比。所以，第一个例子中的T(n) = O(2n+2)，第二个例子中的T(n) = O(2n^2+2n+3)。这就是大O时间复杂度表示法。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。当n很大时，你可以把它想象成10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大O表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n^2)。 时间复杂度分析前面介绍了大O时间复杂度的由来和表示方法。现在我们来看下，如何分析一段代码的时间复杂度？我这儿有三个比较实用的方法可以分享给你。 1.只关注循环执行次数最多的一段代码我刚才说了，大O这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。为了便于你理解，我还拿前面的例子来说明。12345678int cal(int n) &#123; int sum = 0; int i = 1; for (; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum;&#125;其中第2、3行代码都是常量级的执行时间，与n的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第4、5行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了n次，所以总的时间复杂度就是O(n)。 2.加法法则：总复杂度等于量级最大的那段代码的复杂度我这里还有一段代码。你可以先试着分析一下，然后再往下看跟我的分析思路是否一样12345678910111213141516171819202122232425int cal(int n) &#123; int sum_1 = 0; int p = 1; for (; p &lt; 100; ++p) &#123; sum_1 = sum_1 + p; &#125; int sum_2 = 0; int q = 1; for (; q &lt; n; ++q) &#123; sum_2 = sum_2 + q; &#125; int sum_3 = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) &#123; j = 1; for (; j &lt;= n; ++j) &#123; sum_3 = sum_3 + i * j; &#125; &#125; return sum_1 + sum_2 + sum_3;&#125;这个代码分为三部分，分别是求sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。第一段的时间复杂度是多少呢？这段代码循环执行了100次，所以是一个常量的执行时间，跟n的规模无关。这里我要再强调一下，即便这段代码循环10000次、100000次，只要是一个已知的数，跟n无关，照样也是常量级的执行时间。当n无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。那第二段代码和第三段代码的时间复杂度是多少呢？答案是O(n)和O(n^2)，你应该能容易就分析出来，我就不啰嗦了。综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为O(n^2)。也就是说：总的时间复杂度就等于量级最大的那段代码的时间复杂度。那我们将这个规律抽象成公式就是：如果T1(n)=O(f(n))，T2(n)=O(g(n))；那么T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))). 3.乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积我刚讲了一个复杂度分析中的加法法则，这儿还有一个乘法法则。类比一下，你应该能“猜到”公式是什么样子的吧？如果T1(n)=O(f(n))，T2(n)=O(g(n))；那么T(n)=T1(n) * T2(n) = O(f(n)) * O(g(n)) = O(f(n) * g(n)).也就是说，假设T1(n) = O(n)，T2(n) = O(n^2)，则T1(n) * T2(n) = O(n^3)。落实到具体的代码上，我们可以把乘法法则看成是嵌套循环，我举个例子给你解释一下。12345678910111213141516int cal(int n) &#123; int ret = 0; int i = 1; for (; i &lt; n; ++i) &#123; ret = ret + f(i); &#125; &#125; int f(int n) &#123; int sum = 0; int i = 1; for (; i &lt; n; ++i) &#123; sum = sum + i; &#125; return sum;&#125;我们单独看cal()函数。假设f()只是一个普通的操作，那第4～6行的时间复杂度就是，T1(n) = O(n)。但f()函数本身不是一个简单的操作，它的时间复杂度是T2(n) = O(n)，所以，整个cal()函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n^2)。我刚刚讲了三种复杂度的分析技巧。不过，你并不用刻意去记忆。实际上，复杂度分析这个东西关键在于“熟练”。你只要多看案例，多分析，就能做到“无招胜有招”。 几种常见时间复杂度实例分析虽然代码千差万别，但是常见的复杂度量级并不多。我稍微总结了一下，这些复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级。对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2^n)和O(n!)。当数据规模n越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于NP时间复杂度我就不展开讲了。我们主要来看几种常见的多项式时间复杂度。 1. O(1)首先你必须明确一个概念，O(1)只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有3行，它的时间复杂度也是O(1），而不是O(3)。123int i = 8;int j = 6;int sum = i + j;我稍微总结一下，只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记作O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。 2. O(logn)、O(nlogn)对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我通过一个例子来说明一下。1234i=1;while (i &lt;= n) &#123; i = i * 2;&#125;根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。从代码中可以看出，变量i的值从1开始取，每循环一次就乘以2。当大于n时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量i的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的：所以，我们只要知道x值是多少，就知道这行代码执行的次数了。通过2 ^ x=n求解x这个问题我们想高中应该就学过了，我就不多说了。x=lognx=\\log nx=logn (以2为底)所以，这段代码的时间复杂度就是：O(logn)O(\\log n)O(logn)(以2为底)现在，把代码稍微改下，你再看看，这段代码的时间复杂度是多少？1234i=1;while (i &lt;= n) &#123; i = i * 3;&#125;根据我刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度为O(logn)O(logn)O(logn) (以3为底)。实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为O(logn)O(logn)O(logn)(以2为底)。为什么呢，我知道有些人忘了，下面我帮你论证了一下，用对数的换底公式就可以了！基于我们前面的一个理论：在采用大O标记复杂度的时候，可以忽略系数，即O(Cf(n)) = O(f(n))。所以，O(logn)O(logn)O(logn) (以2为底) 就等于O(logn)O(logn)O(logn) (以3为底)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为O(logn)O(logn)O(logn)。如果你理解了我前面讲的O(logn)O(logn)O(logn)，那O(nlogn)O(nlogn)O(nlogn)就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是O(logn)O(logn)O(logn)，我们循环执行n遍，时间复杂度就是O(nlogn)O(nlogn)O(nlogn)了。而且，O(nlogn)O(nlogn)O(nlogn)也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(nlogn)O(nlogn)O(nlogn)。 3.O(m+n)、O(m*n)我们再来讲一种跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定。老规矩，先看代码！123456789101112131415int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for (; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for (; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125;从代码中可以看出，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是O(m+n)。针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。 空间复杂度分析前面，咱们花了很长时间讲大O表示法和时间复杂度分析，理解了前面讲的内容，空间复杂度分析方法学起来就非常简单了前面我讲过，时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。我还是拿具体的例子来给你说明。（这段代码有点“傻”，一般没人会这么写，我这么写只是为了方便给你解释。）1234567891011void print(int n) &#123; int i = 0; int[] a = new int[n]; for (i; i &lt;n; ++i) &#123; a[i] = i * i; &#125; for (i = n-1; i &gt;= 0; --i) &#123; print out a[i] &#125;&#125;跟时间复杂度分析一样，我们可以看到，第2行代码中，我们申请了一个空间存储变量i，但是它是常量阶的，跟数据规模n没有关系，所以我们可以忽略。第3行申请了一个大小为n的int类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是O(n)。我们常见的空间复杂度就是O(1)、O(n)、O(n2 )，像O(logn)、O(nlogn)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。 小结基础复杂度分析的知识到此就讲完了，我们来总结一下。复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n2 )。等你学完整个专栏之后，你就会发现几乎所有的数据结构和算法的复杂度都跑不出这几个。复杂度分析并不难，关键在于多练。 之后讲后面的内容时，我还会带你详细地分析每一种数据结构和算法的时间、空间复杂度。只要跟着我的思路学习、练习，你很快就能和我一样，每次看到代码的时候，简单的一眼就能看出其复杂度，难的稍微分析一下就能得出答案。转自《数据结构与算法之美》–王争","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"轻办公神器","slug":"2019-06-28-轻办公神器","date":"2019-06-28T14:18:20.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/06/28/2019-06-28-轻办公神器/","link":"","permalink":"luoxiao.cf/2019/06/28/2019-06-28-轻办公神器/","excerpt":"","text":"推荐APP Termiuus、Working copy、Textastic","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[]},{"title":"Hbase读写操作","slug":"2019-06-28-Hbase读写操作","date":"2019-06-28T14:18:20.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/06/28/2019-06-28-Hbase读写操作/","link":"","permalink":"luoxiao.cf/2019/06/28/2019-06-28-Hbase读写操作/","excerpt":"","text":"HDFS、MR解决了分布式存储和分布式计算问题，但是由于HDFS的随机读写能力太差，所以不能直接作为数据库。Hbase是为了应对这点而诞生的，它是一个高性能、高可靠、可伸缩、面向列的分布式存储数据库，结合Zookeeper可以解决HDFS随机读写能力差的问题。那么它到底是怎么解决随机读写能力太差的问题呢？试想一下如果想要1S 往某个文件中插入100条记录，如果没有HBase，用Java代码写会是一种什么样的操作？可能我们需要100次的IO才能搞定。但上面假设一层缓冲层用来缓存一下， 当缓冲池满了以后再往文件中写，会不会好很多？以Mysql为例，我们都知道Mysql的表，库等数据最终都会落在磁盘上，Mysql只不过是架设在OS文件系统上的一款解析软件而已，帮准你用流完成文件的读写。HDFS相当于OS文件系统，HBase和Mysql一样相当于一个解析器。 Hbase架构图被网上的图片坑的很，HLog组件应该归属于HRegionServer管，但是图中却把HLog画到了HRegion中，也有可能是版本问题？下面的图摘自《HBase权威指南》 写操作当Client向HRegionServer发起put请求时，其将会交给对应的HRegion来处理首先HRegion会看是否需要写入HLog（WAL用于做数据恢复和数据回滚）当数据持久化到HLog后，数据会被直接写到MemStore中，并检查MemStore是否满了，如果满了，数据会被刷到HDFS上以HFile文件类型存储，这个操作由另一个HRegionServer的线程处理，同时会保存最后写入序号，系统就知道哪些数据被持久化了。摘自《HBase权威指南》–8.2.2写路径 读操作很纳闷为啥HBase权威指南里面没找到读数据的章节，下面摘自网络流转的说法，具体需要看一下源码验证首先Client向HRegionServer发送Get请求，HRegion将其请求提交给对应的HRegionHRegion会先从MemStore中找，如果找到则返回，如果没有或者数据不全，则去StoreFile中寻找","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"luoxiao.cf/tags/HBase/"}]},{"title":"分布式缓存小结","slug":"2019-05-13-分布式缓存总结","date":"2019-05-13T22:41:00.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/05/13/2019-05-13-分布式缓存总结/","link":"","permalink":"luoxiao.cf/2019/05/13/2019-05-13-分布式缓存总结/","excerpt":"","text":"缓存是提高服务访问速度的最有效的途径之一，下面对缓存的基本原理以及使用做一个小结。 缓存的基本原理缓存指将数据存储在相对较高访问速度的存储介质中，以供系统处理;缓存的本质是一个内存Hash表，数据缓存是以KV形式存储在内存的Hash表中，Hash表数据读写的时间复杂度为O(1)，可以参考下图加深理解，图片摘自-《大型网站技术架构：核心原理与案例分析》下图为是应用到代码中时的逻辑图 合理使用缓存缓存虽然有很多好处，但是不合理的使用缓存反而会帮倒忙，成为系统累赘。作为一个合格的开发者，有必要搞清楚其应用点，以及在应用时的注意事项，下面对其进行简单小结。频繁修改的数据频繁修改的数据不宜存入缓存，如果你这么做了，数据在存入缓存后，应用还来不及访问，就已经再次失效了，途增系统负担。没有热点访问的数据内存往往是有限的，在往内存存储时，若 redis 检测到已经没有足够空间再容纳新增加数据时，会将长期未使用的数据清理出缓存。试想一下缓存被大量非热点数据，会是怎么样的？可能数据还没有再次被访问就已经被挤出缓存。在为数据做缓存时要遵守二八原则，大部分访问的数据没有集中在小部分数据上，那么缓存就没有意义了。数据的不一致性和脏读我们会见到某购物平台店家修改了商品，但前台并未实时更新数据，这种现象称为数据的不一致和脏读。缓存内会给数据设置过­期时间，当数据过期后会重新加载数据库数­据到缓存，所以往往会有一定延时。在互联网行业中，这种延时是可以被接受的。但假如产品人员表示非要优化，那么也有应对方案，就是做实时更新同步缓存，但这种做法会带来更多的系统开销和数据一致性问题。缓存的高可用在公司中，可能会发现对于业务场景，单台 redis 缓存服务即可满足日常需要。但随着业务不断扩展，可能就会带来很多问题。比如当 redis 服务宕机时，整个服务器的业务压力会落在数据­库服务器上。这种压力的突然飙升很有可能造成服务宕机，而且这种宕机并不是简单的直接重起服务就可以解决的。对于这种问题一些人可能会使用热备服务器去解决，当主缓存服务宕掉后，自动切换到备份缓存服务器，但是这样做 违背了缓存设计的初衷，正确的做法应该是使用分布式缓存，数据会被缓存在多台机器上，当某台机器不可用时只是部分数据不可用，重新启动即可。缓存预热缓存中存放的是热点数据，热点数据又是缓存系统利用LRU算法对不断访问的数据筛选淘汰出来的，这个过程需要花费较长的时间。新启动的缓存系统如果没有任何数据，在重建缓存数据过程中，系统的性能和数据库负载都不会太好，那么最好在缓存系统启动时就把数据加载好，这种手段叫缓存的预热。例如淘宝双十一，开发人员会提前一个月预热大量的缓存数据。缓存穿透对于访问数据库没有的数据，可能一些开发人员的做法是直接返回到前台，但是这种做法是不正确的。正确的做法应是将其key缓存起来value 设置为NULL即可。","categories":[{"name":"分布式","slug":"分布式","permalink":"luoxiao.cf/categories/分布式/"}],"tags":[]},{"title":"Hbase ThriftServer访问内网HBase","slug":"2019-05-10-Hbase-ThriftServer访问内网HBase","date":"2019-05-10T17:59:00.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/05/10/2019-05-10-Hbase-ThriftServer访问内网HBase/","link":"","permalink":"luoxiao.cf/2019/05/10/2019-05-10-Hbase-ThriftServer访问内网HBase/","excerpt":"","text":"本地集群环境架构结构如下图所示:实现thriftClient与thriftServer通信，实现访问内网HBase集群118.166.152.33和101.118.124.111 分别为公网IP,192.168.5.2/3/4分别为内网IP 域名映射首先我们要做的是将ThriftServer服务的通信端口9000 映射到内网中，这边映射成了公网的9000端口 thrift下面是Thrift的百度百科Thrift是一种接口描述语言和二进制通讯协议，它被用来定义和创建跨语言的服务。它被当作一个远程过程调用（RPC）框架来使用，是由Facebook为“大规模跨语言服务开发”而开发的。Thrift支持众多通讯协议：TBinaryProtocol – 一种简单的二进制格式，简单，但没有为空间效率而优化。比文本协议处理起来更快，但更难于调试。TCompactProtocol – 更紧凑的二进制格式，处理起来通常同样高效。想了解更多百度百科支持的传输协议有：TFramedTransport – 当使用一个非阻塞服务器时，要求使用这个传输协议。它按帧来发送数据，其中每一帧的开头是长度信息。TSocket – 使用阻塞的套接字I/O来传输。想了解更多百度百科HBase ThriftServer有下面两个参数用来指定是否使用TFramedTransport协议,默认是false这边CDH中不用开启hbase.regionserver.thrift.framedUse Thrift TFramedTransport on the server side. This is the recommended transport for thrift servers and requires a similar setting on the client side. Changing this to false will select the default transport, vulnerable to DoS when malformed requests are issued due to THRIFT-601.hbase.regionserver.thrift.compactUse Thrift TCompactProtocol binary serialization protocol.下面参数用来配置Thrift Gateway的认证，如果你配了这个东西就必须用doAs完成认证才能完成通信12345678&lt;property&gt; &lt;name&gt;hbase.regionserver.thrift.http&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.thrift.support.proxyuser&lt;/name&gt; &lt;value&gt;true/value&gt;&lt;/property&gt;想了解更多Configure the Thrift Gateway to Use the doAs Feature，看59.6章节我这边都没有开启如下图 Client客户端可以用python或者是Java与ThriftServer进行通信。值得一提的是python3 在访问时会抛异常，这边初步查了一下也有解决方案，这边就先用python2.7进行测试，下面是代码示例:python123456789101112131415161718192021222324#!/usr/bin/pythonfrom common import *from thrift.transport import TSocketfrom thrift.protocol import TBinaryProtocolfrom thrift.transport import TTransportfrom hbase import Hbase# Connect to HBase Thrift servertransport = TTransport.TBufferedTransport(TSocket.TSocket(\"101.118.124.111\", \"9090\"))protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)# Create and open the client connectionclient = Hbase.Client(protocol)transport.open()rows = client.getRow(\"cars\", \"row1\")for row in rows: rowKey = row.row print(\"Got row:\" + rowKey);# Close the client connectiontransport.close()java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.bim.hbase;import org.apache.hadoop.hbase.thrift.generated.AlreadyExists;import org.apache.hadoop.hbase.thrift.generated.Hbase;import org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor;import org.apache.thrift.transport.*;import org.apache.thrift.protocol.*;import org.apache.thrift.protocol.TCompactProtocol;import org.apache.thrift.transport.TFramedTransport;import java.io.*;import java.util.ArrayList;import java.util.List;import java.nio.ByteBuffer;import java.nio.charset.Charset;import java.util.Properties;public class HbaseThriftTest &#123; private static String host; private static Integer port; public static void init()&#123; Properties properties = new Properties(); InputStream in = null; try &#123; in = HbaseThriftTest.class.getClassLoader().getResourceAsStream(\"system.properties\"); properties.load(in); host = properties.getProperty(\"hbase.thrift.host\"); port = Integer.parseInt(properties.getProperty(\"hbase.thrift.port\")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if(in != null) &#123; try &#123; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; init(); String Proto = \"binary\"; String TableName = \"t1\"; String ColFamily = \"rowkey002\"; // setup the hbase thrift connection TTransport Transport; Transport = new TSocket(host, port); TCompactProtocol FProtocol = new TCompactProtocol(Transport); Hbase.Client Client = new Hbase.Client(FProtocol); if (Proto.equals(\"binary\")) &#123; TProtocol Protocol = new TBinaryProtocol(Transport, true, true); Client = new Hbase.Client(Protocol); &#125; else if ( Proto.equals(\"framed\")) &#123; Transport = new TFramedTransport(new TSocket(host, port)); TProtocol Protocol = new TBinaryProtocol(Transport, true, true); Client = new Hbase.Client(Protocol); &#125; else if ( ! Proto.equals(\"compact\")) &#123; System.out.println(\"Protocol must be compact or framed or binary\"); &#125; Transport.open(); // prepare the column family List&lt;ColumnDescriptor&gt; Columns = new ArrayList&lt;ColumnDescriptor&gt;(); ColumnDescriptor col = new ColumnDescriptor(); col.name = ByteBuffer.wrap(ColFamily.getBytes()); Columns.add(col); // dump existing tables System.out.println(\"#~ Dumping Existing tables\"); for (ByteBuffer tn : Client.getTableNames()) &#123; System.out.println(\"-- found: \" + new String(tn.array(), Charset.forName(\"UTF-8\"))); &#125; // create the new table System.out.println(\"#~ Creating table: \" + TableName); try &#123; Client.createTable(ByteBuffer.wrap(TableName.getBytes()), Columns); &#125; catch (AlreadyExists ae) &#123; System.out.println(\"WARN: \" + ae.message); &#125; Transport.close(); &#125;&#125;下面是system.properties12hbase.thrift.host=101.118.124.111hbase.thrift.port=9090","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"luoxiao.cf/tags/Hbase/"}]},{"title":"分布式系统的知识结构总结","slug":"2019-05-09-分布式系统的知识结构总结","date":"2019-05-09T12:47:14.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/05/09/2019-05-09-分布式系统的知识结构总结/","link":"","permalink":"luoxiao.cf/2019/05/09/2019-05-09-分布式系统的知识结构总结/","excerpt":"","text":"构件分布式系统的目的是增加系统容量，提高系统可用性。转换成技术方面就是完成下面两件事大流量处理。所谓大流量处理就是利用集群技术将大量的并发请求分发到不同机器上关键业务保护。提高系统可用性，所以需要将故障隔离起来，防止雪崩效应引起的整体服务无法正常服务。说白了就是干两件事，一是提高系统架构的吞吐量，服务更多的并发流量；二是为了提高系统的稳定性，让系统的可用性更高；下面从系统的性能和系统的稳定性来说一下在分布式下需要完成的事情。 系统性能系统性能可以从下面五个方面入手来做整体优化，他们分别是：缓存负载均衡异步数据分区数据镜像 缓存从前台到后台再到数据库，都有缓存。缓存是提高服务响应速度的最直接手段，在分布式环境中，可以使用MemCache、redis来构件分布式缓存。目前从市面上看来大家都更喜欢redis，这其中需要一个Proxy来做缓存的分片和路由。 负载均衡负载均衡是水平拓展的关键技术，它可以是多台机器共同分担一部分流量请求。 异步异步这块主要是通过异步队列对请求做排队处理，这边有很多业务场景，比如可以把前端并发请求的峰值给“削平”了，让后端通过自己能够处理的速度来处理请求，进而来增加系统的吞吐量，但这通常比较适用于实时性不是很高的场景；引入消息队列后，可能会出现消息丢失的问题，这就被迫我们不得不去做消息的持久化，持久化会造成有“状态”的节点，从而增加服务调度的难度。 数据分区/镜像数据分区和数据镜像可以放在一起，数据分区就是将数据按照某种固定的方式分成多个区。比如按照地区来分，这样需要一个数据路由的中间件，不同地区来访问不同区域的数据库，来减少数据库的压力。但是这样会造成跨库的join和跨库的事务异常复杂。而数据镜像是将数据复制成多分，这样就不需要数据中间件了，可以在任意节点上进行读写， 内部会进行自动数据同步，但是数据镜像中最大的问题就是数据一致性问题。对于一般公司来说，在初期会使用读写分离的数据镜像方式，然而后期会采用分库分表的方式。","categories":[{"name":"分布式","slug":"分布式","permalink":"luoxiao.cf/categories/分布式/"}],"tags":[]},{"title":"CDH5.15 权限管理","slug":"2019-04-30-CDH-权限管理","date":"2019-04-30T13:37:30.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/04/30/2019-04-30-CDH-权限管理/","link":"","permalink":"luoxiao.cf/2019/04/30/2019-04-30-CDH-权限管理/","excerpt":"","text":"CDH默认会给我们创建很多角色用户，这边对于开发和管理来说十分不方便。这边将所有的用户修改为ROOT HDFS下面描述一下怎么修改HDFS执行下面指令1chown -R root:root /var/run/hdfs-sockets YARN搜索“用户”、“系统组”、“mapred”、“hadoop”修改为root 其他组件最后修改ZooKeeper，Hive，Impala，Hbase，Spark，Sqoop2，Flume，Kafka等的用户和系统组为root。搜索“用户”、“组”修改为root 将角色加到root组中CDH给我们建的角色在/etc/passwd中可以体现找出所有的相关角色把他们加到root组中12345678910111213141516usermod -a -G datacenter hueusermod -a -G datacenter yarnusermod -a -G datacenter flumeusermod -a -G datacenter impalausermod -a -G datacenter sparkusermod -a -G datacenter zookeeperusermod -a -G datacenter mapredusermod -a -G datacenter sqoopusermod -a -G datacenter hiveusermod -a -G datacenter sqoop2usermod -a -G datacenter oozieusermod -a -G datacenter hbaseusermod -a -G datacenter hdfsusermod -a -G datacenter kuduusermod -a -G datacenter httpfsusermod -a -G datacenter root 修改HDFShdfs dfs -chown -R root:root /重启集群服务即可，后续如果还有权限问题需要按提示进行修改即可","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"luoxiao.cf/tags/CDH/"}]},{"title":"chrome auto proxy","slug":"2019-04-28-chrome-auto-proxy","date":"2019-04-28T15:39:39.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/04/28/2019-04-28-chrome-auto-proxy/","link":"","permalink":"luoxiao.cf/2019/04/28/2019-04-28-chrome-auto-proxy/","excerpt":"","text":"到google应用商店下载Proxy SwitchyOmega 下面进行相关配置点我下载","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[]},{"title":"CDH5.15.0 hbase hue 配置后Error","slug":"2019-04-27-CDH15-5-0-hbase-hue","date":"2019-04-27T17:43:28.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/04/27/2019-04-27-CDH15-5-0-hbase-hue/","link":"","permalink":"luoxiao.cf/2019/04/27/2019-04-27-CDH15-5-0-hbase-hue/","excerpt":"","text":"在CDM hue的配置界面搜索 hue_safety将下面代码加入到“值”123[hbase]hbase_conf_dir=&#123;&#123;HBASE_CONF_DIR&#125;&#125;thrift_transport=buffered在CDM Hbase的配置界面搜索 core-site.xml将下面代码加入到“值”12345678910111213141516&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hbase.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hbase.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;除此之外想看更多配置可以参考 https://blog.csdn.net/zhangshenghang/article/details/85776134","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hue","slug":"Hue","permalink":"luoxiao.cf/tags/Hue/"}]},{"title":"CDH Error: JAVA_HOME is not set and could not be found.","slug":"2019-04-26-CDH-Error-JAVA-HOME-is-not-set-and-could-not-be-found","date":"2019-04-26T13:23:25.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/04/26/2019-04-26-CDH-Error-JAVA-HOME-is-not-set-and-could-not-be-found/","link":"","permalink":"luoxiao.cf/2019/04/26/2019-04-26-CDH-Error-JAVA-HOME-is-not-set-and-could-not-be-found/","excerpt":"","text":"错误一12hdfs dfs -mkdir -p /flume/mysqlPermission denied: user=root, access=WRITE, inode=\"/\":hdfs:supergroup:drwxr-xr-x执行命令的用户没有执行权限。直接给当前用户授权。（这种想法是不正确的，不要为了简化输入命令，就试图修改这些东西）正确的做法应该是。切换指定用户执行命令1[root@cdh1 data]#sudo -u hdfs hadoop fs -mkdir /newFile或者1[root@cdh1 data]#sudo -u hdfs dfhs dfs -mkdir /newFile更简单的是，先进入这个用户，su hdfs 错误二1234sudo -u hdfs hdfs dfs -mkdir -p /flume/mysql Error: JAVA_HOME is not set and could not be found.java -versionjava version \"1.8.0_91\"确实已经设置了JAVA_HOME ，而且在linux shell 执行 echo $JAVA_HOME 也是有输出。123456789find / -name cloudera-config.sh/*/*/*/cloudera-manager/cm-5.10.0/lib64/cmf/service/common/cloudera-config.shlocal JAVA8_HOME_CANDIDATES=( '/usr/java/jdk1.8' '/usr/java/jre1.8' '/usr/lib/jvm/j2sdk1.8-oracle' '/usr/lib/jvm/j2sdk1.8-oracle/jre' '/usr/lib/jvm/java-8-oracle')解决办法:建立一个已经有的JAVA_HOME 链接到 /usr/java/jdk1.8 就好了！目标位置：/usr/java/jdk1.8原文件：///jdk1.8.0_91123ln -s 源文件 目标文件ln -s /*/*/jdk1.8.0_91 /usr/java/jdk1.8sudo -u hdfs hdfs dfs -mkdir -p /flume/mysql","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"luoxiao.cf/tags/CDH/"}]},{"title":"TProtocolException: Bad version in readMessageBegin","slug":"2019-04-26-TProtocolException-Bad-version-in-readMessageBegin","date":"2019-04-26T13:23:25.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/04/26/2019-04-26-TProtocolException-Bad-version-in-readMessageBegin/","link":"","permalink":"luoxiao.cf/2019/04/26/2019-04-26-TProtocolException-Bad-version-in-readMessageBegin/","excerpt":"","text":"链接thrift异常123456789101112131415org.apache.thrift.protocol.TProtocolException: Bad version in readMessageBegin at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:223) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27) at org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:289) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)2019-04-26 11:04:02,472 ERROR org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer: Thrift error occurred during processing of message.org.apache.thrift.protocol.TProtocolException: Bad version in readMessageBegin at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:223) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27) at org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:289) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)去CDH中的Hbase面板中检查hbase.regionserver.thrift.compact和hbase.regionserver.thrift.framed是否启用hbase.regionserver.thrift.framedDescription:Use Thrift TFramedTransport on the server side. This is the recommended transport for thrift servers and requires a similar setting on the client side. Changing this to false will select the default transport, vulnerable to DoS when malformed requests are issued due to THRIFT-601.hbase.regionserver.thrift.compactDescriptionUse Thrift TCompactProtocol binary serialization protocol.增加ThriftServer堆栈大小到2G并保存重启ThriftServer服务即可","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"luoxiao.cf/tags/Hbase/"}]},{"title":"RDD DataSet和DataFrame的区别和应用场景","slug":"2019-04-24-RDD-DataSet和DataFrame的区别和应用场景","date":"2019-04-24T11:47:37.000Z","updated":"2019-11-13T04:56:08.679Z","comments":true,"path":"2019/04/24/2019-04-24-RDD-DataSet和DataFrame的区别和应用场景/","link":"","permalink":"luoxiao.cf/2019/04/24/2019-04-24-RDD-DataSet和DataFrame的区别和应用场景/","excerpt":"","text":"在spark中，RDD、DataFrame、Dataset是最常用的数据类型，本博文给出笔者在使用的过程中体会到的区别和各自的优势。 共性1、 RDD、DataFrame和Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利；2、 三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过，如1234567val sparkconf = new SparkConf().setMaster(\"local\").setAppName(\"test\").set(\"spark.port.maxRetries\",\"1000\")val spark = SparkSession.builder().config(sparkconf).getOrCreate()val rdd=spark.sparkContext.parallelize(Seq((\"a\", 1), (\"b\", 1), (\"a\", 1)))rdd.map&#123;line=&gt; println(\"运行\") line._1&#125;map中的println(“运行”)并不会运行。3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。4、三者都有partition的概念12345678var predata=data.repartition(24).mapPartitions&#123; PartLine =&gt; &#123; PartLine.map&#123; line =&gt; println(“转换操作”) &#125; &#125;&#125;这样对每一个分区进行操作时，就跟在操作数组一样，不但数据量比较小，而且可以方便的将map中的运算结果拿出来，如果直接用map，map中对外面的操作是无效的12345678910val rdd=spark.sparkContext.parallelize(Seq((\"a\", 1), (\"b\", 1), (\"a\", 1)))var flag=0val test=rdd.map&#123;line=&gt; println(\"运行\") flag+=1 println(flag) line._1&#125;println(test.count)println(flag)结果如下:12345678运行1运行2运行330不使用partition时，对map之外的操作无法对map之外的变量造成影响。5、三者有许多共同的函数，如filter，排序等。6、在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。12//这里的spark是SparkSession的变量名import spark.implicits._7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。DataFrame:1234567testDF.map&#123; case Row(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; \"\"&#125;为了提高稳健性，最好后面有一个_通配操作，这里提供了DataFrame一个解析字段的方法。Dataset:12345678case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型testDS.map&#123; case Coltest(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; \"\"&#125; 区别 RDD1、RDD一般和spark mlib同时使用。2、RDD不支持sparkSQL操作。DataFrame:1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如12345testDF.foreach&#123; line =&gt; val col1=line.getAs[String](\"col1\") val col2=line.getAs[String](\"col2\")&#125;每一列的值没法直接访问。2、DataFrame与Dataset一般与spark ml同时使用。3、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如12dataDF.createOrReplaceTempView(\"tmp\")spark.sql(\"select ROW,DATE from tmp where DATE is not null order by DATE\").show(100,false)4、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。123456//保存val saveoptions = Map(\"header\" -&gt; \"true\", \"delimiter\" -&gt; \"t\", \"path\" -&gt; \"hdfs://172.xx.xx.xx:9000/test\")datawDF.write.format(\"com.databricks.spark.csv\").mode(SaveMode.Overwrite).options(saveoptions).save()//读取val options = Map(\"header\" -&gt; \"true\", \"delimiter\" -&gt; \"t\", \"path\" -&gt; \"hdfs://172.xx.xx.xx:9000/test\")val datarDF= spark.read.options(options).format(\"com.databricks.spark.csv\").load()利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。Dataset:这里主要对比Dataset和DataFrame，因为Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。12345678910111213case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型//rdd//(\"a\", 1)//(\"b\", 1)//(\"a\", 1)val test: Dataset[Coltest]=rdd.map&#123;line=&gt; Coltest(line._1,line._2)&#125;.toDStest.map&#123; line=&gt; println(line.col1) println(line.col2)&#125;可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。转化：RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换。DataFrame/Dataset转RDD：这个转换很简单12val rdd1=testDF.rddval rdd2=testDS.rddRDD转DataFrame：1234import spark.implicits._val testDF = rdd.map &#123;line=&gt; (line._1,line._2)&#125;.toDF(\"col1\",\"col2\")一般用元组把一行的数据写在一起，然后在toDF中指定字段名。RDD转Dataset：12345import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = rdd.map &#123;line=&gt; Coltest(line._1,line._2)&#125;.toDS可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。Dataset转DataFrame：这个也很简单，因为只是把case class封装成Row。12import spark.implicits._val testDF = testDS.toDFDataFrame转Dataset：123import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = testDF.as[Coltest]这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。特别注意：在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"luoxiao.cf/tags/Spark/"}]},{"title":"CDH5.15.0升级spark1.6到2.3","slug":"2019-04-23-CDH5-15升级spark1-6到2-3","date":"2019-04-23T20:10:32.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/04/23/2019-04-23-CDH5-15升级spark1-6到2-3/","link":"","permalink":"luoxiao.cf/2019/04/23/2019-04-23-CDH5-15升级spark1-6到2-3/","excerpt":"","text":"CDH5.15.0安装集群以后，默认安装的spark是1.6版本。添加的时候没有spark2,因为spark1.6好多新功能都不能使用，所以这边对其进行升级。 安装包parcel、parcel.sha和manifest.jsoncsd下载parcel等文件点我下载下载csd文件点我下载关于版本，csd和parcel的版本要对应上本例子中都是cloudera1;parcel的版本要选择适合自己操作系统的，本例中使用的是centos7,所以下载el7下载好所有文件1234SPARK2_ON_YARN-2.4.0.cloudera1.jarSPARK2-2.4.0.cloudera1-1.cdh5.13.3.p0.1007356-el7.parcelSPARK2-2.4.0.cloudera1-1.cdh5.13.3.p0.1007356-el7.parcel.sha1manifest.json 上传将SPARK2_ON_YARN-2.4.0.cloudera1.jar上传到主节点的*/opt/cloudera/csd/*没有目录的话创建一个将其余文件上传到主节点的*/opt/cloudera/parcel-repo/*目录下面如果有重名文件必须删掉，如果没有则不用管 重启CSM1234# 在主节点运行/opt/cm-5.15.0/etc/init.d/cloudera-scm-server start# 在所有节点运行/opt/cm-5.15.0/etc/init.d/cloudera-scm-agent start 激活安装到cloudera manager界面 主机-》parcel-》SPARK2 做激活按照正常操作添加SPARK2到集群即可 验证到node1节点下运行spark-shell发现报错首先我们得知道下面这些事情CDH安装目录 /opt/cloudera/parcels/CDH/SPARK2安装目录 /opt/cloudera/parcels/SPARK2所有配置文件目录为 /etc/将CDH中spark配置文件拷贝到SPARK2的配置文件中,并配置spark-env.sh文件1cp /opt/cloudera/parcels/CDH/etc/spark/conf.dist/* /opt/cloudera/parcels/SPARK2/etc/spark2/conf.dist/检查一下配置文件1234vim /opt/cloudera/parcels/SPARK2/etc/spark2/conf.dist/spark-env.sh# 添加下面内容到*spark-env.sh*中export SPARK_DIST_CLASSPATH=$(hadoop classpath) //指定hadoop class文件目录export HADOOP_CONF_DIR=/etc/hadoop/conf //指定hadoop配置文件目录将Spark2加入到环境变量中12345vim /etc/profile#添加如下内容export HADOOP_CONF_DIR=/etc/hadoop/confexport SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2export PATH=$SPARK_HOME/bin:$PATH spark on yarn测试1234567891011cd /opt/cloudera/parcels/SPARK2/lib/spark2/examples/jarsspark-submit --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 4g \\ --executor-memory 2g \\ --executor-cores 1 \\ --queue thequeue \\ ./spark-examples_2.11-2.4.0.cloudera1.jar \\ 10到yarn上查看任务http://zhaoyihao.iok.la:8088/cluster Spark SQL 操作Hive测试这里有一个参数特别重要spark.sql.warehouse.dir下面是官方解释When working with Hive, one must instantiate SparkSession with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. Users who do not have an existing Hive deployment can still enable Hive support. When not configured by the hive-site.xml, the context automatically creates metastore_db in the current directory and creates a directory configured by spark.sql.warehouse.dir, which defaults to the directory spark-warehouse in the current directory that the Spark application is started. Note that the hive.metastore.warehouse.dir property in hive-site.xml is deprecated since Spark 2.0.0. Instead, use spark.sql.warehouse.dir to specify the default location of database in warehouse. You may need to grant write privilege to the user who starts the Spark application.大概的意思是，使用hive需要sparksession设置支持选项，如果用户集群里，没有部署好的hive，sparksession也能够提供hive支持，在这种情况下，如果没有hive-site.xml文件，sparkcontext会自动在当前目录创建元数据db,并且会在spark.sql.warehouse.dir表示的位置创建一个目录，用户存放table数据，所以spark.sql.warehouse.dir是一个用户存放hive table文件的一个目录，因为是一个目录地址，难免会收到操作系统的影响，因为不同的文件系统的前缀是不一样了，为了适配性，spark鼓励在code中设置该选项，而不是在hive-site.xml中设置该选项。1.如果没有部署好的hive，spark确实是会使用内置的hive，但是spark会将所有的元信息都放到spark_home/bin 目录下，也就是为什么配置了spark.sql.warehouse.dir 却不起作用的原因。而且，就算部署了hive，也需要让spark识别hive，否则spark，还是会使用spark默认的hive2.只有在部署好的hive情况下，使用spark.sql.warehouse.dir才会生效，而且spark会默认覆盖hive的配置项。下面摘自官方文档Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/. http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html到cloudera manager 下载hvie的客户端配置，将hive-site.xml，core-site.xml，hdfs-site.xml复制到*/opt/cloudera/parcels/SPARK2-2.4.0.cloudera1-1.cdh5.13.3.p0.1007356/lib/spark2/conf*目录下123cp /etc/hadoop/conf/hdfs-site.xml /opt/cloudera/parcels/SPARK2/lib/spark2/conf/cp /etc/hadoop/conf/core-site.xml /opt/cloudera/parcels/SPARK2/lib/spark2/conf/cp /etc/hive/conf/hive-site.xml /opt/cloudera/parcels/SPARK2/lib/spark2/conf/进入spark-shell1234567891011121314151617val spark = SparkSession .builder() .appName(\"Spark Hive Example\") //在实例化sparkSession时指定hive的warehouse .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") //调用enableHiveSupport开启hive的支持 .enableHiveSupport() .getOrCreate()import spark.implicits._import spark.sqlsql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")// Queries are expressed in HiveQLsql(\"SELECT * FROM src\").show()","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"luoxiao.cf/tags/CDH/"}]},{"title":"CDH5.15.x 启动服务Time out","slug":"2019-04-17-CDH5-15-x-time-out","date":"2019-04-17T19:02:00.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/04/17/2019-04-17-CDH5-15-x-time-out/","link":"","permalink":"luoxiao.cf/2019/04/17/2019-04-17-CDH5-15-x-time-out/","excerpt":"","text":"环境: CDH 5.15.0 + centos71Command aborted because of exception: Command timed-out after 150 seconds这是由于服务端集群未禁用ipv6导致使用ifconfig命令查看网卡信息，如果出现inet6 fe80::20c:29ff:fed0:3514，说明机器开启了ipv6编辑**/etc/sysctl.conf**配置1net.ipv6.conf.all.disable_ipv6=1编辑**/etc/sysconfig/network**配置1NETWORKING_IPV6=no编辑**/etc/sysconfig/network-scripts/ifcfg-eno16777736**1IPV6INIT=no执行sysctl -p或者reboot重启命令","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"luoxiao.cf/tags/CDH/"}]},{"title":"java.lang.NoClassDefFoundError scala/Product$class","slug":"2019-04-16-spark-idea-windows-scala-Product","date":"2019-04-16T13:18:58.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/04/16/2019-04-16-spark-idea-windows-scala-Product/","link":"","permalink":"luoxiao.cf/2019/04/16/2019-04-16-spark-idea-windows-scala-Product/","excerpt":"","text":"环境：windows 7 + idea + scala + spark本地运行以后报下面错误1234567891011121314Exception in thread \"main\" java.lang.NoClassDefFoundError: scala/Product$class at org.apache.spark.SparkConf$DeprecatedConfig.&lt;init&gt;(SparkConf.scala:682) at org.apache.spark.SparkConf$.&lt;init&gt;(SparkConf.scala:539) at org.apache.spark.SparkConf$.&lt;clinit&gt;(SparkConf.scala) at org.apache.spark.SparkConf.set(SparkConf.scala:72) at org.apache.spark.SparkConf.setAppName(SparkConf.scala:87) at com.bim.WordCount$.main(WordCount.scala:9) at com.bim.WordCount.main(WordCount.scala)Caused by: java.lang.ClassNotFoundException: scala.Product$class at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 7 moreSpark和Scala的版本是有对应关系的，下面有个查看关系的小技巧，去https://mvnrepository.com/中搜索spark，进入Spark Project Core查看即可下面分别引入spark-core和spark-sql（不需要的话可以不引）运行即可。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"luoxiao.cf/tags/Spark/"}]},{"title":"java.lang.ArrayIndexOutOfBoundsException 10582","slug":"2019-04-16-spark-idea-windows-array-index-out-of-bounds-exception","date":"2019-04-16T13:11:07.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/04/16/2019-04-16-spark-idea-windows-array-index-out-of-bounds-exception/","link":"","permalink":"luoxiao.cf/2019/04/16/2019-04-16-spark-idea-windows-array-index-out-of-bounds-exception/","excerpt":"","text":"环境：windows 7 + idea + scala 1.12.6 + spark 2.4.0在IDEA中运行报下面错误1234567891011121314Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 10582 at com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.accept(BytecodeReadingParanamer.java:563) at com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.access$200(BytecodeReadingParanamer.java:338) at com.thoughtworks.paranamer.BytecodeReadingParanamer.lookupParameterNames(BytecodeReadingParanamer.java:103) at com.thoughtworks.paranamer.CachingParanamer.lookupParameterNames(CachingParanamer.java:90) at com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.getCtorParams(BeanIntrospector.scala:44) at com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.$anonfun$apply$1(BeanIntrospector.scala:58) at com.fasterxml.jackson.module.scala.introspect.BeanIntrospector$.$anonfun$apply$1$adapted(BeanIntrospector.scala:58) at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241) at scala.collection.Iterator.foreach(Iterator.scala:944) at scala.collection.Iterator.foreach$(Iterator.scala:944) at scala.collection.AbstractIterator.foreach(Iterator.scala:1432) at scala.collection.IterableLike.foreach(IterableLike.scala:71) ...下面是pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;ml.yihao&lt;/groupId&gt; &lt;artifactId&gt;spark&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.12.6&lt;/scala.version&gt; &lt;spark.version&gt;2.4.0&lt;/spark.version&gt; &lt;hadoop.version&gt;2.6.4&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;!--&lt;dependency&gt;--&gt; &lt;!--&lt;groupId&gt;com.thoughtworks.paranamer&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;paranamer&lt;/artifactId&gt;--&gt; &lt;!--&lt;version&gt;2.8&lt;/version&gt;--&gt; &lt;!--&lt;/dependency&gt;--&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;解决：在pom.xml中添加paranamer即可12345&lt;dependency&gt; &lt;groupId&gt;com.thoughtworks.paranamer&lt;/groupId&gt; &lt;artifactId&gt;paranamer&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt;&lt;/dependency&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"luoxiao.cf/tags/Spark/"}]},{"title":"B-树","slug":"2019-03-27-B-树","date":"2019-03-27T21:01:56.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/27/2019-03-27-B-树/","link":"","permalink":"luoxiao.cf/2019/03/27/2019-03-27-B-树/","excerpt":"","text":"B+树，称为B加树；那么对于B-树，谁要是读成B减树，那就太丢人了咯，它虽然带着减号，但是要读成B树。B+树和B-树是一种基础的数据结构，做为开发人员一定要掌握。 什么是B-树首先大家都知道数据库有索引，索引被映射成二叉索引树，被存在于磁盘之上。那么下面我们来看看为啥数据库要使用B-树？换二叉搜索树行不行？从算法逻辑上来讲，二叉搜索树的查找速度和比较次数都是最小的，但是数据库的实现并没有用二叉搜索树，而是用了B-树和B+树，下面来说一下里面的门道。数据库操作数据要进行频繁的“磁盘IO&quot;，因此在设计之初要充分考虑到如何优化磁盘IO造成的读写效率问题。数据库索引存于磁盘之上，当数据量比较大的时候，索引的大小可能有几个G甚至更多。当利用索引查询的时候，肯定不能将全部都加载到内存，能做的只有逐一加载每个磁盘页，这里的磁盘页对应索引树的节点。探究一下如果索引树使用二叉搜索树实现，会是一种什么样的情况，假设树的高度是4，查找的值是10第1次IO第2次IO第3次IO第4次IO查找了4次命中结果，因此磁盘IO的次数是由树的高度决定。为了减少磁盘IO次数，下面使用B-树来将二叉搜索树进行“瘦身”，以此来减少IO次数！下面来具体介绍一下B-树（Balance Tree），一个m阶的B树具有如下几个特征：12345根结点至少有两个子女。每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m所有的叶子结点都位于同一层。每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。以3阶 B-树为例，来认识一下B-树的具体结构。树中的具体元素和上图二叉搜索树节点一样。这棵树中，重点看（2,6）节点，该节点有两个元素2和6，又有三个孩子1，（3,5），8；其中1小于元素2，（3,5）在元素2,6之间，8大于（3,5），符合B-树的几个特征。 B-树的查找假如要查的值为5通过整个流程可以看出 B-树 在查询中比较次数其实不比二叉树少，尤其当单一节点中的元素数量很多时。可是相比磁盘IO的速度，内存中比较耗时几乎可以忽略，所以只要树的高度足够低，IO次数足够少，就可以提升查找性能。相比之下节点内部元素多一些也没有关系，仅仅是多了几次内存交互，只要不超过磁盘页的大小即可，这也是B-树的重要优势之一。 B-树的插入B-树插入新节点过程比较复杂，而且分很多种情况。这边举一个最典型例子，加入我们要插入的值是4自顶向下查找4的节点位置，发现4应当插入到节点元素3，5之间。节点3，5已经是两元素节点，无法再增加。父亲节点 2， 6 也是两元素节点，也无法再增加。根节点9是单元素节点，可以升级为两元素节点。于是拆分节点3，5与节点2，6，让根节点9升级为两元素节点4，9。节点6独立为根节点的第二个孩子。 B-树的删除下面演示一下B-树删除元素11的过程自顶向下查找元素11的节点位置。删除11后，节点12只有一个孩子，不符合B树规范。因此找出12,13,15三个节点的中位数13，取代节点12，而节点12自身下移成为第一个孩子。（这个过程称为左旋） 小结B-树主要应用于文件系统以及部分数据库索引，比如MongoDB。大部分关系型数据库，比如myslq，则使用B+树作为索引。","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"IDEA 无法创建 scala","slug":"2019-03-21-2017IDEA-无法创建-scala","date":"2019-03-21T15:08:38.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/21/2019-03-21-2017IDEA-无法创建-scala/","link":"","permalink":"luoxiao.cf/2019/03/21/2019-03-21-2017IDEA-无法创建-scala/","excerpt":"","text":"IDEA 无法创建Scala class找到根目录下的 spark.iml在里面添加1&lt;orderEntry type=\"library\" name=\"scala-sdk-2.12.6\" level=\"application\" /&gt;","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"ClouderaManager操作指南","slug":"2019-03-19-ClouderaManager手册","date":"2019-03-19T13:58:42.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/19/2019-03-19-ClouderaManager手册/","link":"","permalink":"luoxiao.cf/2019/03/19/2019-03-19-ClouderaManager手册/","excerpt":"","text":"作者 ： 张帅 1 前言构建企业级别的大数据平台不是一件简单的事情，要从多个方面进行考虑，如硬件环境、软件环境。Hadoop生态圈的产品众多，部署、安装、运维、监控等工作异常琐碎，尤其当某个组件出现问题，由于机器的数量、组件的分布情况，往往会使得运维人员无从下手。目前市面上Hadoop的发型版本主要有三种：1. Apache Hadoop 2. CDH(Cloudera Distribution Hadoop) 3. HDP(Hortonworks Data Platform) 首先，Apache Hadoop是最正统的发行版本，版本更新快，新特性增加的多，但相对而言Bug较多，组件之间的兼容性也较差。其次，CDH版本会将Hadoop的各个组件进行打包，形成一个发布版本，针对该版本下的各个组件进行一系列测试，补丁修复，优化策略等，保证了CDH大版本下各个Hadoop组件之间的良好协作性。大部分公司均使用该系列。最后，HDP版本是Hortonworks公司针对Hadoop的发行版本，暂时没有过多的了解。CDH的公司Cloudera推出了Cloudera Manager用于CDH版本集群的管理。Cloudera Manager是一款管理CDH集群的端到端的应用产品，可以通过管理界面可视化的对集群中的一系列组件进行统一的管理：部署、安装、配置、监控等。简单来说，Cloudera Manager有四大功能：1. 管理：对集群进行管理，如添加、删除节点等操作。 2. 监控：监控集群的健康情况，对设置的各种指标和系统运行情况进行全面监控。 3. 诊断：对集群出现的问题进行诊断，对出现的问题给出建议解决方案。 4. 集成：对hadoop的多组件进行整合。 Cloudera Manager的核心是Cloudera Manager Server，简称CMS。CMS提供了管理端Web界面，统一针对其他节点进行控制，如图所示组件介绍1. Agent，Agent安装在集群中的各个节点上，用于启动、结束进程，安装、配置组件，监控节点等 2. Management Service，由一系列的角色构成，角色有监控、预警、报告等 3. Database，存储了配置和监控信息 4. Cloudera Repository，Cloudera Manager用于分发的软件仓库 2 前置准备工作操作系统尽量和要使用Cloudera Manager版本匹配，可以参考官方给出的Cloudera Manager与操作系统的兼容性参照表。Cloudera Manager官方下载Cloudera CDH 各个组件官方下载CDH Requirements for Cloudera Manager 2.1 关于RAID 0的说明尽管建议采用RAID(Redundant Array of Independent Disk,即磁盘阵列)作为NameNode的存储器以保护元数据，但是若将RAID作为datanode的存储设备则不会给HDFS带来益处。HDFS所提供的节点间数据复制技术已可满足数据备份需求，无需使用RAID的冗余机制。此外，尽管RAID条带化技术(RAID 0)被广泛用户提升性能，但是其速度仍然比用在HDFS里的JBOD(Just a Bunch Of Disks)配置慢。JBOD在所有磁盘之间循环调度HDFS块。RAID 0的读写操作受限于磁盘阵列中最慢盘片的速度，而JBOD的磁盘操作均独立，因而平均读写速度高于最慢盘片的读写速度。需要强调的是，各个磁盘的性能在实际使用中总存在相当大的差异，即使对于相同型号的磁盘。针对某一雅虎集群的评测报告点我表明，在一个测试(Gridmix)中，JBOD比RAID 0 快10%；在另一测试(HDFS写吞吐量)中，JBOD比RAID 0 快30%。最后，若JBOD配置的某一磁盘出现故障，HDFS可以忽略该磁盘，继续工作。而RAID的某一盘片故障会导致整个磁盘阵列不可用，进而使相应节点失效。https://zh.hortonworks.com/blog/why-not-raid-0-its-about-time-and-snowflakes/备注：我们实际生成环境使用的是RAID5，12块4T硬盘，可用空间为40T。虽然有可能因为RAID卡的损坏导致节点故障，但是RAID卡极大程度的提高了IO性能，并且在逻辑上将12块硬盘映射为了一块大磁盘。单块磁盘故障不会影响到节点的状态。 2.2 IP地址尽可能的将集群部署在同一网段中，避免夸路由进行数据交互。 2.3 主机名及映射主机名映射为全限定名称和短名称的形式，例如：​​ 192.168.15.193 datacenter01.aisino.com datacenter01 2.4 启动级别启动级别尽可能设置为3，避免其他图形界面消耗机器资源。 2.5 防火墙和selinux关闭防火墙的原因是一些动态任务（YARN、Spark Executor）在运行时动态分配端口号，如果开启防火墙的话会导致任务无法连接，导致任务执行失败。selinux记得要关闭。 2.6 配置系统文件打开数量以及用户最大进程数量vi /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 * soft nproc 16384 * hard nproc 16384 注意，星号表示所有用户 2.7 配置NTP服务Cloudera Manager要求各个节点都启动NTP服务，保证集群内各节点之间的时间同步，由于是分布式架构，节点与节点之间时刻保持通信，如果节点之间时间差别过大，会导致通信故障，从而节点造成宕机。 2.8 配置SSH所有节点都需要配置SSH免密登录（包括自己）。 3 安装Cloudera Manager本次安装环境为CDH-5.7.0版本 3.1 下载安装包1. 下载Cloudera Manager安装包 http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.7.0_x86_64.tar.gz 2. 下载Parcel离线包 http://archive.cloudera.com/cdh5/parcels/5.7.0/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel http://archive.cloudera.com/cdh5/parcels/5.7.0/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1 http://archive.cloudera.com/cdh5/parcels/5.7.0/manifest.json 3.2 安装yum依赖yum install -y chkconfig python bind-utils psmisc libxslt zlib sqlite yum install -y cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs yum install -y redhat-lsb bind-utils libxslt yum install -y protobuf snappy 3.3 安装MySQL数据库Cloudera Manager需要将配置信息以及监控信息存储至数据库中，这里采用MySQL数据。 3.4 MySQL驱动包路径必须将MySQL驱动包复制至/usr/share/java目录下，并且驱动包的名称必须为mysql-connector-java.jar。注意，所有节点都需要MySQL驱动包 3.5 在MySQL中创建数据库create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database cloudera DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 3.6 解压Cloudera Manager安装包Cloudera Manager Server服务安装在Manager1服务器上，因此需要将Cloudera Manager的压缩包以及所有Parcel文件上传至服务器，并将Cloudera Manager压缩包解压至/opt/cloudera-manager目录下 3.7 创建用户在所有节点上创建cloudera-scm用户：​​ useradd --system --home=/opt/cloudera-manager/cm-5.7.0/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment “Cloudera SCM User” cloudera-scm 3.8 创建Cloudera Manager Server元数据目录在主节点上需要创建Cloudera Manager Server的元数据目录：mkdir /var/cloudera-scm-server chown cloudera-scm:cloudera-scm /var/cloudera-scm-server chown cloudera-scm:cloudera-scm /opt/cloudera-manager 3.9 复制cloudera-manager目录到其他节点首先将/opt/cloudera-manager/cm-5.7.0/etc/cloudera-scm-agent/config.ini文件中server_host配置项的地址更改为主节点的地址将主节点上/opt/cloudera-manager目录复制至其他集群节点下，命令如下：scp -r /opt/cloudera-manager 主机:/opt/ 3.10 创建Parcel目录在主节点上创建Parcel包的存储目录，命令如下：mkdir -p /opt/cloudera/parcel-repo chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo 将以下文件复制到该目录下CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcelCDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1manifest.json将CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1文件的名称修改为CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha 3.11 创建Parcel包分发目录Cloudera Manager在安装的过程中，需要将安装包复制到集群的各个节点上，因此需要在集群各个节点上事先创建Parcel包的分发目录。在所有节点上：mkdir -p /opt/cloudera/parcels chown cloudera-scm:cloudera-scm /opt/cloudera/parcels 3.12 初始化CSM数据库脚本在主节点上，执行初始化脚本：/opt/cloudera-manager/cm-5.7.0/share/cmf/schema/scm_prepare_database.sh mysql -hmanager1 -uroot -p123456 --scm-host manager1 scmdbn scmdbu scmdbp 说明：这个脚本就是用来创建和配置CMS需要的数据库的脚本。各参数是指：mysql：数据库用的是mysql，如果安装过程中用的oracle，那么该参数就应该改为oracle。-hmanager1：数据库建立在manager1主机上面。也就是主节点上面。-uroot：root身份运行mysql。-123456：mysql的root密码是123456。–scm-host manager1：CMS的主机，一般是和mysql安装的主机是在同一个主机上。最后三个参数是：数据库名，数据库用户名，数据库密码。我的实际环境中采用如下命令/opt/cloudera-manager/cm-5.7.0/share/cmf/schema/scm_prepare_database.sh mysql -hlocalhost -uroot -p -P13066 --scm-host localhost cloudera root BIM@123%$#qwe 3.13 配置与启动Cloudera Manager Server首先将Cloudera Manager Server的启动脚本复制到/etc/init.d/目录下：​​ cp​ /opt/cloudera-manager/cm-5.7.0/etc/init.d/cloudera-scm-server​ /etc/init.d/cloudera-scm-server配置/etc/init.d/cloudera-scm-server：vi /etc/init.d/cloudera-scm-server 找到CMF_DEFAULTS配置项，进行修改。 CMF_DEFAULTS=${CMF_DEFAULTS:-/opt/cloudera-manager/cm-5.7.0/etc/default} 启动Cloudera Manager Server：service cloudera-scm-server start chkconfig cloudera-scm-server on 3.14 配置与启动Cloudera Manager Agent在所有节点上创建agent的运行时目录：mkdir /opt/cloudera-manager/cm-5.7.0/run/cloudera-scm-agent 将Cloudera Manager Agent的启动脚本复制到/etc/init.d/目录下：cp \\ /opt/cloudera-manager/cm-5.7.0/etc/init.d/cloudera-scm-agent \\ /etc/init.d/cloudera-scm-agent 配置Cloudera Manager Agent：vi /etc/init.d/cloudera-scm-agent 找到CMF_DEFAULTS配置项，进行修改。 CMF_DEFAULTS=${CMF_DEFAULTS:-/opt/cloudera-manager/cm-5.7.0/etc/default} 启动Cloudera Manager Agent：service cloudera-scm-agent start chkconfig cloudera-scm-agent on 3.15 日志文件路径启动Server或Agent由于各种原因可能会导致启动失败，因此需要查看日志文件定位错误信息，进行修复。Server的日志文件位于：/opt/cloudera-manager/cm-5.7.0/log/cloudera-scm-server Agent的日志文件位于：/opt/cloudera-manager/cm-5.7.0/log/cloudera-scm-agent 3.16 进入Cloudera Manager Server管理页面当Server与Agent全部启动完成后，可以访问CMS的WEB管理页面example link，如图所示。账户密码均为admin 4 安装CDH在Cloudera Manager Server的WEB管理页面中，可以批量进行组件的安装、配置等，下面开始安装CDH各种组件。 4.1 接受协议首次登陆时，会自动弹出“接受协议”页面，接受即可。 4.2 选择版本选择版本时，选择免费版本即可。 4.3 查看当前已管理主机如果Agent正产启动，在“当前管理的主机”页面会显示所有管理主机，否则请检查Agent是否正常启动。选择所有主机，点击“继续”按钮。 4.4 等待系统分发Parcel包Cloudera Manager会将Parcel包分发至各个节点，等待几分钟即可。注意，在分发Parcel包的过程中，可以会出现分发失败的问题，查看相应的Agent日志，定位错误，进行修复。比较常见的一个错误是Python脚本出现问题，需要进行一些修改，参见链接：http://www.jianshu.com/p/0d70a67b66b2 4.5 等待检查主机正确性Parcel包分发完成后，Cloudera Manager会检查主机的的正确性，包括一些优化的配置、要关闭的属性等，该步骤非常重要，一定要根据检查结果对主机进行配置修复，否则在以后的过程中会出现各种不可预料问题。 4.6 选择要安装的组件主机正确性检查完成后，就可以进入安装环节了，Cloudera Manager会要求你选择要安装的组件。选择“自定义服务”，选择集群中要安装的组件。 4.7 设置数据库选择完成要安装的组件后，需要为Cloudera Manager配置运行时数据库环境，如图所示。配置的数据库在前面的步骤中已经提前创建完成。 4.8 设置组件的基本运行环境该步骤主要用于设置组件的基本运行环境，例如NameNode的节点、DataNode的节点、数据存放的目录等，该步骤根据实际的物理环境进行设置即可。 4.9 等待启动集群在集群的启动过程中，可能会因为权限或其他问题导致某些服务启动失败，只需要根据错误信息进行修复，即可启动完成。 4.10 启动成功集群启动成功后，可在首页查看集群的总览。 4.11 其他后续可以启动HDFS的HA、YARN的HA以及优化各个组件的配置等。感谢张帅分享。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"luoxiao.cf/tags/CDH/"}],"author":"张帅"},{"title":"用户行为指分析","slug":"2019-03-15-用户行为指分析","date":"2019-03-15T14:24:40.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/15/2019-03-15-用户行为指分析/","link":"","permalink":"luoxiao.cf/2019/03/15/2019-03-15-用户行为指分析/","excerpt":"","text":"1. 概述构件企业级别的大数据平台并不是一件容易的事情，起步阶段要想的全面一些，从多个方面进行考虑。例如关于硬件环境？Hadoop生态圈的产品众多，到底要选择那些组件？另外部署、安装、运维、监控等工作异常繁琐，怎么解决后期管理问题？针对这些问题，下面从软硬件、后期运维和业务的角度来阐述详细规划。 2. 软件支持1） 首先我们说说关于Hadoop的发行版，目前Hadoop的发行版主要有三种，我们怎么选?1231. Apahce 基金会 Hadoop2. CDH (Cloudera Distribution Hadoop)3. HDP （Hortonworks Data Platform）Hadoop属于Apache基金会的顶级项目，Apahce版本迭代速度很快，新特性很多，相对的bug就会很多，组件与组件之间的兼容性也会很差。在实际中大部分它的使用者多为进行学术研究，喜欢爱折腾的那号人，所以这个版本不首选。Cloudera 针对Apahce Hadoop在每个大版本变化后进行一次打包，很好的解决了系统不稳定的问题。与此同时，Cloudera公司推出的Cloudera Manager版本集群管理工具，它可以完成一键部署集群，一键拓展，监控，自动诊断等操作。最重要的是CDH的文档很丰富，解决问题的速度也会很高。目前知晓的使用此发行版的公司有：360、东方航空等，这个版本是目前首选。HDP版本是Hortonworks公司针对Hadoop的发行版本，目前也是比较小众的一个，现阶段还处于探索发展阶段，所以不首选。有了自动化部署Cloudera Manager ,后期运维就能轻松的多，而且里面的自动诊断功能也是相当nice！2） 关于具体应用组件Flume、Hadoop、Kafka、Hive、Spark、Redis、Mysql、Sqlserver、Sqoop、Hue、Oozie 3. 硬件支持配置Hadoop集群至少要求有三台Server，一台为主服务器，三台为从服务器。主服务器上会跑大量后台进程，所以主服务器的配置要远远优于从服务器。**主服务器 ** 最少1台硬件类型要求OSlinux硬盘1~4TBCPU2个频率为2~2.5GHz的四核或六核内存16~32GB**从服务器 **最少两台硬件类型要求OSlinux硬盘1~2TBCPU2个频率为2~2.5GHz的四核或六核内存4~16GB 4. 如何进行网站流量分析下面对平台初期统计指标做一个梳理，另外会介绍一下统计指标的方向。 4.1 指标举例需求：今日，昨天，前天 所有来访者，平均请求的页面数需求：按照来源及时间维度统计PVS，并按照PV大小倒序排序需求：按照时间维度，比如，统计一天内各小时产生最多pvs的来源topN需求：统计每日最热门的功能top10需求：按照时间维度比如小时来统计独立访客及其产生的pv需求：将每天的新访客统计出来需求：查询今日所有回头访客及其访问次数需求：统计出每天所有用户访问网站的平均次数需求：回头/单次访客的访问比重，比如当日回头客占比需求：人均访问频度需求：漏斗模型统计，以模型上传业务来评估模型转化设计的合理性。1234567891011step1: 开启本地化组建step2: 选择模型、选择目录step3: 设置各种参数，勾选模型对比（或者是其他操作）step4: 进行模型转换step5: 进行模型上传step6: 模型浏览六步为一个业务指标，我们可以拓展下面业务指标**1) 查询每一个步骤的总访问人数****2) 查询每一步骤相对于路径起点人数的比例****3) 查询每一步骤相对于上一步骤的漏出率**​除了上面一些指标以外，可以从下面方向入手，发掘一些有价值的数据报表。部分参考**《网站分析实战——如何以数据驱动决策，提升网站价值》** 王彦平，吴盛锋编著 4.2 关于统计统计方向1） 基础分析（PV,IP,UV）趋势分析：根据选定的时段，提供网站流量数据，通过流量趋势变化形态，为您分析网站访客的访问规律、网站发展状况提供参考。对比分析：根据选定的两个对比时段，提供网站流量在时间上的纵向对比报表，帮您发现网站发展状况、发展规律、流量变化率等。当前在线：提供当前时刻站点上的访客量，以及最近15分钟流量、来源、受访、访客变化情况等，方便用户及时了解当前网站流量状况。访问明细：提供最近7日的访客访问记录，可按每个PV或每次访问行为（访客的每次会话）显示，并可按照来源、搜索词等条件进行筛选。 通过访问明细，用户可以详细了解网站流量的累计过程，从而为用户快速找出流量变动原因提供最原始、最准确的依据。2)来源分析来源分类：提供不同来源形式（直接输入、搜索引擎、其他外部链接、站内来源）、不同来源项引入流量的比例情况。通过精确的量化数据，帮助用户分析什么类型的来路产生的流量多、效果好，进而合理优化推广方案。搜索引擎：提供各搜索引擎以及搜索引擎子产品引入流量的比例情况。从搜索引擎引入流量的的角度，帮助用户了解网站的SEO、SEM效果，从而为制定下一步SEO、SEM计划提供依据。搜索词：提供访客通过搜索引擎进入网站所使用的搜索词，以及各搜索词引入流量的特征和分布。帮助用户了解各搜索词引入流量的质量，进而了解访客的兴趣关注点、网站与访客兴趣点的匹配度，为优化SEO方案及SEM提词方案提供详细依据。最近7日的访客搜索记录，可按每个PV或每次访问行为（访客的每次会话）显示，并可按照访客类型、地区等条件进行筛选。为您搜索引擎优化提供最详细的原始数据。来路域名：提供具体来路域名引入流量的分布情况，并可按“社会化媒体”、“搜索引擎”、“邮箱”等网站类型对来源域名进行分类。 帮助用户了解哪类推广渠道产生的流量多、效果好，进而合理优化网站推广方案。来路页面：提供具体来路页面引入流量的分布情况。 尤其对于通过流量置换、包广告位等方式从其他网站引入流量的用户，该功能可以方便、清晰地展现广告引入的流量及效果，为优化推广方案提供依据。来源升降榜：提供开通统计后任意两日的TOP10000搜索词、来路域名引入流量的对比情况，并按照变化的剧烈程度提供排行榜。 用户可通过此功能快速找到哪些来路对网站流量的影响比较大，从而及时排查相应来路问题。3) 受访分析受访域名：提供访客对网站中各个域名的访问情况。 一般情况下，网站不同域名提供的产品、内容各有差异，通过此功能用户可以了解不同内容的受欢迎程度以及网站运营成效。受访页面：提供访客对网站中各个页面的访问情况。 站内入口页面为访客进入网站时浏览的第一个页面，如果入口页面的跳出率较高则需要关注并优化；站内出口页面为访客访问网站的最后一个页面，对于离开率较高的页面需要关注并优化。受访升降榜：提供开通统计后任意两日的TOP10000受访页面的浏览情况对比，并按照变化的剧烈程度提供排行榜。 可通过此功能验证经过改版的页面是否有流量提升或哪些页面有巨大流量波动，从而及时排查相应问题。热点图：记录访客在页面上的鼠标点击行为，通过颜色区分不同区域的点击热度；支持将一组页面设置为&quot;关注范围&quot;，并可按来路细分点击热度。 通过访客在页面上的点击量统计，可以了解页面设计是否合理、广告位的安排能否获取更多佣金等。用户视点：提供受访页面对页面上链接的其他站内页面的输出流量，并通过输出流量的高低绘制热度图，与热点图不同的是，所有记录都是实际打开了下一页面产生了浏览次数（PV）的数据，而不仅仅是拥有鼠标点击行为。访问轨迹：提供观察焦点页面的上下游页面，了解访客从哪些途径进入页面，又流向了哪里。 通过上游页面列表比较出不同流量引入渠道的效果；通过下游页面列表了解用户的浏览习惯，哪些页面元素、内容更吸引访客点击。4) 访客分析地区运营商：提供各地区访客、各网络运营商访客的访问情况分布。 地方网站、下载站等与地域性、网络链路等结合较为紧密的网站，可以参考此功能数据，合理优化推广运营方案。终端详情：提供网站访客所使用的浏览终端的配置情况。 参考此数据进行网页设计、开发，可更好地提高网站兼容性，以达到良好的用户交互体验。新老访客：当日访客中，历史上第一次访问该网站的访客记为当日新访客；历史上已经访问过该网站的访客记为老访客。 新访客与老访客进入网站的途径和浏览行为往往存在差异。该功能可以辅助分析不同访客的行为习惯，针对不同访客优化网站，例如为制作新手导航提供数据支持等。忠诚度：从访客一天内回访网站的次数（日访问频度）与访客上次访问网站的时间两个角度，分析访客对网站的访问粘性、忠诚度、吸引程度。 由于提升网站内容的更新频率、增强用户体验与用户价值可以有更高的忠诚度，因此该功能在网站内容更新及用户体验方面提供了重要参考。活跃度：从访客单次访问浏览网站的时间与网页数两个角度，分析访客在网站上的活跃程度。 由于提升网站内容的质量与数量可以获得更高的活跃度，因此该功能是网站内容分析的关键指标之一。5) 转化路径分析转化定义:访客在您的网站完成了某项您期望的活动，记为一次转化，如注册或下载。目标示例获得用户目标：在线注册、创建账号等。咨询目标：咨询、留言、电话等。互动目标：模型转化、模型分享等。收入目标：购买简约版、付款等。转化数据的应用在报告的自定义指标中勾选转化指标，实时掌握网站的推广及运营情况。结合“全部来源”、“转化路径”、“页面上下游”等报告分析访问漏斗，提高转化率。对“转化目标”设置价值，预估转化收益，衡量ROI。路径分析：根据设置的特定路线，监测某一流程的完成转化情况，算出每步的转换率和流失率数据，如注册流程，购买流程等。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[]},{"title":"二叉数（三）","slug":"2019-03-11-二叉数（三）","date":"2019-03-11T00:04:04.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/11/2019-03-11-二叉数（三）/","link":"","permalink":"luoxiao.cf/2019/03/11/2019-03-11-二叉数（三）/","excerpt":"","text":"下面使用Java实现中序非递归遍历二叉树，开始之前我们先梳理一下它的执行逻辑。申请一个桟记为stack.初始化一个指针cur指向头节点。以头节点为起始，将树的整个左边界压入到桟中。及不断的另cur=cur.left,重复步骤2如果cur.left为空，弹出stack桟顶元素，并打印桟顶元素的值，桟顶元素记为node,另cur=node.right重复步骤2直到cur指向的node为空并且stack为空时，结束该过程。下面是Java代码实现：12345678910111213public static void inOrder1(BiTree tree)&#123; Stack&lt;BiTree&gt; stack = new Stack&lt;&gt;(); while(tree != null || !stack.empty())&#123; if(tree!=null)&#123; stack.push(tree); tree = tree.lChild; &#125;else&#123; BiTree node = stack.pop(); System.out.print(node.data); tree = node.rChild; &#125; &#125;&#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"二叉树（二）","slug":"2019-03-10-二叉树（二）","date":"2019-03-10T00:38:24.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/10/2019-03-10-二叉树（二）/","link":"","permalink":"luoxiao.cf/2019/03/10/2019-03-10-二叉树（二）/","excerpt":"","text":"下面使用Java实现先序非递归遍历二叉树，开始之前我们先梳理一下它的执行逻辑。首先申请一个新的桟，记为stack。每次将头节点head压入stack中每次从stack中弹出桟顶节点，记为cur，然后打印cur节点的值。如果cur右孩子不为空的话，将cur的右孩子先压入stack中。最后如果cur的左孩子不为空的话，将cur的左孩子压入stack中。不断重复步骤3,直到stack为空，全部过程结束。下面是Java代码实现：1234567891011121314151617181920/*** 先序遍历* 非递归*/public static void preOrder1(BiTree tree)&#123; Stack&lt;BiTree&gt; stack = new Stack&lt;&gt;(); if(tree != null)&#123; stack.push(tree); while(!stack.empty())&#123; BiTree node = stack.pop(); while(node != null)&#123; System.out.print(node.data); if(node.rChild != null) stack.push(node.rChild); node = node.lChild; &#125; &#125; &#125;&#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"二叉树（一）","slug":"2019-03-09-二叉树（一）","date":"2019-03-09T13:51:42.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2019/03/09/2019-03-09-二叉树（一）/","link":"","permalink":"luoxiao.cf/2019/03/09/2019-03-09-二叉树（一）/","excerpt":"","text":"树是在实际编程中经常遇到的数据结构，它的逻辑很简单：除了根节点以外，每个节点只有一个父节点，根节点没有父节点;除了叶节点之外所有节点都有一个或多个子节点，叶节点没有子节点。树的前、中、后序遍历是比较基础，同时也是必须要掌握的几个点，它们分别有递归、和非递归的解法。递归相对简单一点，非递归相对复杂一些。下面用Java语言分别实现树的前、中、后序递归遍历，另外这边拓展（递归创建二叉树、求树的深度）两个解法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package BiTree;import java.util.Scanner;/** * @author zyh * @create 19-3-9 下午12:06 */public class BiTree &#123; private Object data; private BiTree lChild; private BiTree rChild; public BiTree(Object data,BiTree lChild,BiTree rChild)&#123; this.data = data; this.lChild = lChild; this.rChild = rChild; &#125; /** * 中序遍历二叉数 左根右 * @param tree */ public static void inOrderTraverse(BiTree tree)&#123; if(tree != null) &#123; inOrderTraverse(tree.lChild); System.out.print(tree.data); inOrderTraverse(tree.rChild); &#125; &#125; /** * 后续遍历二叉数 左右根 * @param tree */ public static void afterOrderTraverse(BiTree tree)&#123; if(tree != null)&#123; afterOrderTraverse(tree.lChild); afterOrderTraverse(tree.rChild); System.out.print(tree.data); &#125; &#125; /** * 先须遍历二叉数 根左右 * @param tree */ public static void preOrderTraverse(BiTree tree)&#123; if(tree != null)&#123; System.out.print(tree.data); preOrderTraverse(tree.lChild); preOrderTraverse(tree.rChild); &#125; &#125; /** * 递归创建二叉数 * @param scanner * @return * @throws Exception */ public static BiTree createBiTree(Scanner scanner) throws Exception&#123; int data = scanner.nextInt(); if(data == 0)&#123; return null; &#125;else&#123; return new BiTree(data,createBiTree(scanner),createBiTree(scanner)); &#125; &#125; /** * 求二叉数的深度 * @param tree * @return */ public static Integer depth(BiTree tree)&#123; if(tree == null)&#123; return 0; &#125;else&#123; Integer m = depth(tree.lChild); Integer n = depth(tree.rChild); if(m &gt; n)&#123; return m+1; &#125;else&#123; return n+1; &#125; &#125; &#125; public static void main(String []args) throws Exception&#123; BiTree tree = createBiTree(new Scanner(System.in)); System.out.println(); System.out.println(\"--------------递归：中序遍历二叉数------------------\"); inOrderTraverse(tree); System.out.println(); System.out.println(\"--------------递归：先序遍历二叉数------------------\"); preOrderTraverse(tree); System.out.println(); System.out.println(\"--------------递归：后序遍历二叉数------------------\"); afterOrderTraverse(tree); System.out.println(); System.out.println(\"--------------递归：求二叉树的深度------------------\"); System.out.println(depth(tree)); &#125;&#125;参考:参考: Data Structure (2nd Edition) 第五章http://book.knowsky.com/book_1030305.htm","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"搭建日志收集服务器","slug":"2019-1-24-搭建日志收集服务器","date":"2019-01-24T00:00:00.000Z","updated":"2019-11-13T04:56:08.680Z","comments":true,"path":"2019/01/24/2019-1-24-搭建日志收集服务器/","link":"","permalink":"luoxiao.cf/2019/01/24/2019-1-24-搭建日志收集服务器/","excerpt":"","text":"业务目标：前台报表需要按项目ID、按同IP下的不同用户进行统计，所以这里就不能再使用nginx日志了。为了解决这一问题，搭建日志收集服务器。1.安装依赖1yum -y install gcc perl pcre-devel openssl openssl-devel2.上传LuaJIT-2.0.4.tar.gz并安装LuaJIT123tar -zxvf LuaJIT-2.0.4.tar.gz -C /usr/local/src/cd /usr/local/src/LuaJIT-2.0.4/make &amp;&amp; make install PREFIX=/usr/local/luajit3.设置环境变量12export LUAJIT_LIB=/usr/local/luajit/libexport LUAJIT_INC=/usr/local/luajit/include/luajit-2.04.创建modules保存nginx的模块1mkdir -p /usr/local/nginx/modules5.上传openresty-1.9.7.3.tar.gz和依赖的模块lua-nginx-module-0.10.0.tar、ngx_devel_kit-0.2.19.tar、ngx_devel_kit-0.2.19.tar、echo-nginx-module-0.58.tar.gz6.将依赖的模块直接解压到/usr/local/nginx/modules目录即可，不需要编译安装1234tar -zxvf lua-nginx-module-0.10.0.tar.gz -C /usr/local/nginx/modules/tar -zxvf set-misc-nginx-module-0.29.tar.gz -C /usr/local/nginx/modules/tar -zxvf ngx_devel_kit-0.2.19.tar.gz -C /usr/local/nginx/modules/tar -zxvf echo-nginx-module-0.58.tar.gz -C /usr/local/nginx/modules/7.解压openresty-1.9.7.3.tar.gz12tar -zxvf openresty-1.9.7.3.tar.gz -C /usr/local/src/cd /usr/local/src/openresty-1.9.7.3/8.编译安装openresty1./configure --prefix=/usr/local/openresty --with-luajit &amp;&amp; make &amp;&amp; make install9.上传nginx12tar -zxvf nginx-1.8.1.tar.gz -C /usr/local/src/cd /usr/local/src/nginx-1.8.1/10.编译nginx并支持其他模块12345678./configure --prefix=/usr/local/nginx \\ --with-ld-opt=\"-Wl,-rpath,/usr/local/luajit/lib\" \\ --add-module=/usr/local/nginx/modules/ngx_devel_kit-0.2.19 \\ --add-module=/usr/local/nginx/modules/lua-nginx-module-0.10.0 \\ --add-module=/usr/local/nginx/modules/set-misc-nginx-module-0.29 \\ --add-module=/usr/local/nginx/modules/echo-nginx-module-0.58 make -j2make install11.修改nginx配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576worker_processes 2;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format tick \"$msec^A$remote_addr^A$u_domain^A$u_url^A$u_title^A$u_referrer^A$u_sh^A$u_sw^A$u_cd^A$u_lang^A$http_user_agent^A$u_utrace^A$u_account\"; access_log logs/access.log tick; sendfile on; keepalive_timeout 65; server &#123; listen 8099; server_name localhost; location /1.gif &#123; #伪装成gif文件 default_type image/gif; #本身关闭access_log，通过subrequest记录log access_log off; access_by_lua \" -- 用户跟踪cookie名为__utrace local uid = ngx.var.cookie___utrace if not uid then -- 如果没有则生成一个跟踪cookie，算法为md5(时间戳+IP+客户端信息) uid = ngx.md5(ngx.now() .. ngx.var.remote_addr .. ngx.var.http_user_agent) end ngx.header['Set-Cookie'] = &#123;'__utrace=' .. uid .. '; path=/'&#125; if ngx.var.arg_domain then -- 通过subrequest到/i-log记录日志，将参数和用户跟踪cookie带过去 ngx.location.capture('/i-log?' .. ngx.var.args .. '&amp;utrace=' .. uid) end \"; #此请求不缓存 add_header Expires \"Fri, 01 Jan 1980 00:00:00 GMT\"; add_header Pragma \"no-cache\"; add_header Cache-Control \"no-cache, max-age=0, must-revalidate\"; #返回一个1×1的空gif图片 empty_gif; &#125; location /i-log &#123; #内部location，不允许外部直接访问 internal; #设置变量，注意需要unescape set_unescape_uri $u_domain $arg_domain; set_unescape_uri $u_url $arg_url; set_unescape_uri $u_title $arg_title; set_unescape_uri $u_referrer $arg_referrer; set_unescape_uri $u_sh $arg_sh; set_unescape_uri $u_sw $arg_sw; set_unescape_uri $u_cd $arg_cd; set_unescape_uri $u_lang $arg_lang; set_unescape_uri $u_utrace $arg_utrace; set_unescape_uri $u_account $arg_account; #打开日志 log_subrequest on; #记录日志到ma.log，实际应用中最好加buffer，格式为tick access_log /var/nginx_logs/ma.log tick; #输出空字符串 echo ''; &#125; &#125;&#125;12.在nginx/html目录下添加一个ma.js (118.89.108.139:8099是nginx访问地址)1234567891011121314151617181920212223242526272829303132333435363738394041424344(function () &#123; var params = &#123;&#125;; //Document对象数据 if(document) &#123; params.domain = document.domain || ''; params.url = document.URL || ''; params.title = document.title || ''; params.referrer = document.referrer || ''; &#125; //Window对象数据 if(window &amp;&amp; window.screen) &#123; params.sh = window.screen.height || 0; params.sw = window.screen.width || 0; params.cd = window.screen.colorDepth || 0; &#125; //navigator对象数据 if(navigator) &#123; params.lang = navigator.language || ''; &#125; //解析_maq配置 if(_maq) &#123; for(var i in _maq) &#123; switch(_maq[i][0]) &#123; case '_setAccount': params.account = _maq[i][1]; break; default: break; &#125; &#125; &#125; //拼接参数串 var args = ''; for(var i in params) &#123; if(args != '') &#123; args += '&amp;'; &#125; args += i + '=' + encodeURIComponent(params[i]); &#125; //通过Image对象请求后端脚本 var img = new Image(1, 1); img.src = 'http://118.89.108.139:8099/log.gif?' + args;&#125;)();13.在要统计的页面添加js12345678910111213&lt;script type=\"text/javascript\"&gt; var _maq = _maq || []; _maq.push(['_setAccount', 'zyh']); (function() &#123; var ma = document.createElement('script'); ma.type = 'text/javascript'; ma.async = true; ma.src = 'http://118.89.108.139:8099/ma.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ma, s); &#125;)();&lt;/script&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"luoxiao.cf/tags/Flume/"}]},{"title":"实时告警系统（二）","slug":"2018-12-06-实时告警系统（二）","date":"2018-12-06T00:00:00.000Z","updated":"2019-11-13T04:56:08.678Z","comments":true,"path":"2018/12/06/2018-12-06-实时告警系统（二）/","link":"","permalink":"luoxiao.cf/2018/12/06/2018-12-06-实时告警系统（二）/","excerpt":"","text":"下面记录环境搭建过程，以及其中遇到的困难，方便后续查询。 概述准备3台centos6.7 :storm1（192.168.1.19）、storm2（192.168.1.20）、storm3（192.168.1.21）所需要的组件：jdk8、zookeeper、kafka、redis、storm 1.创建虚拟机这一步创建三台虚拟机,我这边使用的是virtualbox，你也可以使用VM虚拟机。安装CentOS虚拟机http://www.zonegood.com/2018-12-02-VirtualBox安装CentOS6.7/ 2.部署zookeeper集群安装zookeeper需要jdk8的支持，所以提前安装好jdk8部署zookeeper集群http://www.zonegood.com/2018-04-15-hadoop-zookeeper集群搭建及其使用/ 3.部署storm集群部署storm集群http://www.zonegood.com/2018-12-02-部署storm集群/ 4.部署kafka集群部署kafka集群http://www.zonegood.com/2018-09-29-kafka集群部署/ 5.部署redis集群部署redis集群http://www.zonegood.com/2018-09-15-Redis集群部署/","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"luoxiao.cf/tags/Storm/"}]},{"title":"实时告警系统（一）","slug":"2018-12-03-实时告警系统（一）","date":"2018-12-03T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/12/03/2018-12-03-实时告警系统（一）/","link":"","permalink":"luoxiao.cf/2018/12/03/2018-12-03-实时告警系统（一）/","excerpt":"","text":"今天经理告诉小明（程序员），你做的功能为啥不能用了？于是小明，联系小红童鞋（运维）说你帮我看看线上日志呗，小红童鞋用SSH链接到生产环境，打开后台日志，一看发现确实抛了一个异常，原因是数据库字段太短，数据存不进去，之后事情就这么愉快的解决了！一次两次还好，如果时间久了，你就得一直麻烦小红童鞋帮这看看日志。而且经理他们只看结果，根本不管你是什么问题导致的，长期下来就会对你的能力有质疑;针对这个问题，小明决定做一个日志监控告警系统，在经理发现问题之前就提前发现问题并解决掉 架构设计1.基础产生日志的项目，最少也得有一个。2.flume 是一个分布式日志收集工具，因其简单易用赢得人们青睐，系统使用FLUME收集来自各个子系统中产生的日志，这个日志可以是log4J产生的，具体情况视公司而定；3.kafka 是一个分布式的消息队列，底层用scala编写，其中提供生产者和消费者的功能，他提供类似JMS的特性，我们创建一个Topic,Flume sink到kafka 的这个topic 中4.storm 是一个分布式的流式计算框架，每个topology中并行运行着很多个bolt，整个工作流程似流水一样，从源头源源不断的就像终点。我们使用kafkaSpout获取kafka中的数据，对数据进行过滤、根据匹配规则完成日志匹配，完成发送邮件或者手机短信，来通知模块负责人。5.数据库方面我们可以选用mysql，项目启动后需要把规则加载到storm内存，与此同时做一个定时任务完成更新storm内存中的规则,如果公司项目很大很大，规则很多的话可以考虑用redis，如果很少的话考虑使用mysql即可。6.zookeeper 是一个分布式协调工具 数据库设计1.用户表，用来记录用户基本信息，比如姓名，邮箱，手机号等基本信息2.应用表，用来记录公司的所有子系统3.规则表，关键字段所属应用、负责人、匹配字符4.异常记录表：发现异常信息后触发存储异常记录操作。 逻辑梳理下面主要阐述项目中核心部分逻辑以及注意点：1.项目启动后首先需要加载 APP对应的负责人列表、用户列表、APP的异常匹配规则、APP列表2.数据进入第一层的Bolt后，对数据进行规整和过滤，常规的操作是将数据转成一个JavaBean。3.匹配JavaBean是否满足异常匹配规则4.满足规则，根据appId找到对应负责人，发送邮件和手机短信通知5.将异常记录信息存放到mysql数据库下面阐述定时任务的核心逻辑以及注意点：1.什么时候同步数据比较好？ 如果同步任务写成定时任务，加入kafka中根本就没有数据过来，就算再怎么同步其实也没什么用，所以我们规定只有kafka中有数据过来，这边才跑定时任务。2.检测kafka有没有数据其实很简单，第一次FirstBolt的 execute方法只要触发，就证明有数据，那么我们又不想让他每次都触发更新操作，需要怎么办？- 定义reload boolean全局字段，在非load时间一直修改reload字段为true,在load时间修改reload为false即可； - load规则的时候，需要加上同步操作。 load时间:比如定义只要当前时间能被10整除就是load时间,非load时间比如定义只要当前时间不能被10整除就是非load时间 项目的意义辅助增强系统稳定性，如果内部人员总能在第一时间内发现问题，在其他人发现问题之前就能把问题解决了，长期如此用户对公司产品的认可度也会提升。将系统日志信息记录到数据库，并且设置触发时间以及解决时间，并生成报表。为将来绩效考核做一个数据支撑。用告警系统推动整个部门的积极性，使公司能够更加平稳的发展。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"luoxiao.cf/tags/Storm/"}]},{"title":"VirtualBox安装CentOS6.7","slug":"2018-12-02-VirtualBox安装CentOS6.7","date":"2018-12-02T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/12/02/2018-12-02-VirtualBox安装CentOS6.7/","link":"","permalink":"luoxiao.cf/2018/12/02/2018-12-02-VirtualBox安装CentOS6.7/","excerpt":"","text":"1.安装新建虚拟机环境，由于VBox环境没有CentOS的选项，所以可以选择Red Hat Linux（64Bit）动态分配硬盘大小所以你可以选择大一点的硬盘空间（不用的话不会占用的，但是C盘性能上没固定的好）在VBOX主界面，选择刚刚新建的CentOS64，点击启动。选择第二项，安装操作系统选择Skip选择服务器类型为Basic Server安装完后reboot 2.配置网络参考:https://www.cnblogs.com/zyh1994/p/10040640.html","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[]},{"title":"部署storm集群","slug":"2018-12-02-部署storm集群","date":"2018-12-02T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/12/02/2018-12-02-部署storm集群/","link":"","permalink":"luoxiao.cf/2018/12/02/2018-12-02-部署storm集群/","excerpt":"","text":"1.准备三个虚拟机搭建Storm集群安装步骤参考2018-12-02-VirtualBox安装CentOS6.7 2.配置集群Hosts所有的虚拟机上都需要配置hosts12345vi /etc/hosts#192.168.239.128 storm01 zk01 hadoop01#192.168.239.129 storm02 zk02 hadoop02#192.168.239.130 storm03 zk03 hadoop03 3.服务器间免密码登录[可选]配置免密码登陆，在所有机器上执行以下命令1ssh-keygen -t rsa （四个回车）生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）,使用下面命令将公钥拷贝到要免密登陆的目标机器上1ssh-copy-id &#123;其他机器的hostname&#125; 4.关闭服务器的防火墙12service iptables stopchkconfig iptables off 5.创建服务器上的统一工作目录123mkdir /exportmkdir /export/serverschmod 755 -R /export 6.配置JDK安装jdk8具体步骤不做详述 7.下载storm安装包1wget http://124.202.164.6/files/1139000006794ECA/apache.fayea.com/storm/apache-storm-0.9.5/apache-storm-0.9.5.tar.gz 8.解压安装包123tar -zxvf apache-storm-0.9.5.tar.gz -C /export/servers/cd /export/servers/ln -s apache-storm-0.9.5 storm 9.修改配置文件12mv /export/servers/storm/conf/storm.yaml /export/servers/storm/conf/storm.yaml.bakvi /export/servers/storm/conf/storm.yaml参考12345678910111213141516171819202122storm.zookeeper.servers: - \"storm1\" - \"storm2\" - \"storm3\"#指定storm本地状态保存地址storm.local.dir: \"/export/servers/storm/workdir\"#指定storm集群中的nimbus节点所在的服务器nimbus.host: \"storm1\"#指定nimbus启动JVM最大可用内存大小nimbus.childopts: \"-Xmx1024m\"#指定supervisor启动JVM最大可用内存大小supervisor.childopts: \"-Xmx1024m\"#指定supervisor节点上，每个worker启动JVM最大可用内存大小worker.childopts: \"-Xmx768m\"#指定ui启动JVM最大可用内存大小，ui服务一般与nimbus同在一个节点上。ui.childopts: \"-Xmx768m\"#指定supervisor节点上，启动worker时对应的端口号，每个端口对应槽，每个槽位对应一个workersupervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 10.分发1234scp -r /export/servers/apache-storm-0.9.5 storm02:/export/servers# 然后分别在各机器上创建软连接cd /export/servers/ln -s apache-storm-0.9.5 storm 11.启动在nimbus.host所属的机器上启动 nimbus服务12cd /export/servers/storm/bin/nohup ./storm nimbus &amp;在nimbus.host所属的机器上启动ui服务12cd /export/servers/storm/bin/nohup ./storm ui &amp;在其它个点击上启动supervisor服务12cd /export/servers/storm/bin/nohup ./storm supervisor &amp; 12.查看集群访问nimbus.host:/8080，即可看到storm的ui界面 13.Storm常用操作命令有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑。提交任务命令格式：storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】1bin/storm jar examples/storm-starter/storm-starter-topologies-0.10.0.jar storm.starter.WordCountTopology wordcount杀死任务命令格式：storm kill 【拓扑名称】 -w 10（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间）1storm kill topology-name -w 10停用任务命令格式：storm deactivte 【拓扑名称】1storm deactivte topology-name我们能够挂起或停用运行中的拓扑。当停用拓扑时，所有已分发的元组都会得到处理，但是spouts的nextTuple方法不会被调用。销毁一个拓扑，可以使用kill命令。它会以一种安全的方式销毁一个拓扑，首先停用拓扑，在等待拓扑消息的时间段内允许拓扑完成当前的数据流。启用任务命令格式：storm activate【拓扑名称】1storm activate topology-name重新部署任务命令格式：storm rebalance 【拓扑名称】1storm rebalance topology-name再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"luoxiao.cf/tags/Storm/"}]},{"title":"离线日志分析系统(三)","slug":"2018-11-27-离线日志分析系统(三)","date":"2018-11-27T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/11/27/2018-11-27-离线日志分析系统(三)/","link":"","permalink":"luoxiao.cf/2018/11/27/2018-11-27-离线日志分析系统(三)/","excerpt":"","text":"前一阶段把集群搭建好，编写好FlumeNG采集数据到HDFS上以后，开始使用MapReduce对数据进行初步处理，处理分三个阶段1.过滤掉无用的数据,像访问的静态资源、访问状态码非200的等。2.基于第一步的结果进行日志增强，给每条记录添加SessionId,按访问时间排序后加上递增标号3.初步统计访问起始时间、访问结束时间、进入页面、离开页面、一共访问了多少页面等数据代码的整体结构 第一步过滤规整1.实现第一步的代码WebLogPreProcess.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.zonegood.hive.mr.pre;import com.zonegood.hive.mrbean.WebLog;import com.zonegood.hive.util.ParseUtil;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.HashSet;import java.util.Set;public class WebLogPreProcess &#123; static class WebLogPreProcessMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; Set&lt;String&gt; filter = new HashSet&lt;String&gt;(); Text k = new Text(); NullWritable v = NullWritable.get(); // 过滤静态资源 @Override protected void setup(Context context) throws IOException, InterruptedException &#123; filter.add(\"/about\"); filter.add(\"/black-ip-list/\"); ... &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); WebLog bean = ParseUtil.parse(line); // 过滤掉静态资源 ParseUtil.filter(bean,filter); if(bean.getInvalid())&#123; k.set(bean.toString()); context.write(k,v); &#125; &#125; &#125;&#125;下面看一下ParseUtil类:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.zonegood.hive.util;import com.zonegood.hive.mrbean.WebLog;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Locale;import java.util.Set;public class ParseUtil &#123; public static SimpleDateFormat df1 = new SimpleDateFormat(\"dd/MMM/yyyy:HH:mm:ss\", Locale.US); public static SimpleDateFormat df2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US); public static WebLog parse(String line)&#123; WebLog webLogBean = new WebLog(); String[] arr = line.split(\" \"); if(arr.length &gt; 11)&#123; webLogBean.setIp(arr[0]); webLogBean.setU_info(arr[1]); String time_local = formatDate(arr[3].substring(1)); if(null==time_local) time_local=\"-invalid_time-\"; webLogBean.setTime(time_local); webLogBean.setRequest_method(arr[5]); webLogBean.setRequest_url(arr[6]); webLogBean.setStatus(arr[8]); webLogBean.setSent_body_bytes(arr[9]); webLogBean.setRequest_referer(arr[10]); //如果useragent元素较多，拼接useragent if (arr.length &gt; 12) &#123; StringBuilder sb = new StringBuilder(); for(int i=11;i&lt;arr.length;i++)&#123; sb.append(arr[i]); &#125; webLogBean.setUser_agent(sb.toString()); &#125; else &#123; webLogBean.setUser_agent(arr[11]); &#125; if (Integer.parseInt(webLogBean.getStatus()) &gt;= 400) &#123;// 大于400，HTTP错误 webLogBean.setInvalid(false); &#125; if(\"-invalid_time-\".equals(webLogBean.getTime()))&#123; webLogBean.setInvalid(false); &#125; &#125;else&#123; webLogBean.setInvalid(false); &#125; return webLogBean; &#125; public static void filter(WebLog bean,Set&lt;String&gt; filter)&#123; if(!filter.contains(bean.getRequest_url()))&#123; bean.setInvalid(false); &#125; &#125; public static String formatDate(String time_local) &#123; try &#123; return df2.format(df1.parse(time_local)); &#125; catch (ParseException e) &#123; return null; &#125; &#125;&#125;使用下面的shell脚本完成自动化部署123456789101112131415161718192021222324252627282930313233343536373839404142#!/bin/bash# 作用:批处理weblog日志脚本# 编写:赵一好# java 环境export JAVE_HOME=/data/jdk1.8.0_161export JRE_HOME=$&#123;JAVE_HOME&#125;/jreexport CLASS_PATH=$&#123;JAVE_HOME&#125;/lib/dt.jar:$&#123;JAVE_HOME&#125;/lib/tools.jarexport PATH=$PATH:$&#123;JAVE_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin# hadoop 环境export HADDOP_HOME=/data/hadoop-2.7.3export PATH=$PATH:$&#123;HADDOP_HOME&#125;/bin:$&#123;HADDOP_HOME&#125;/sbin# 时间yesterday=`date -d'-1 day' +%Y-%m-%d`s_year=`date -d'-1 day' +%Y`s_month=`date -d'-1 day' +%m`s_day=`date -d'-1 day' +%d`# inpathin_path=/syslog/preprocess/inpath# outpathout_path=/syslog/preprocess/outpath# jar namejar_name=weblog_pre_process.jar# 判断制定目录是否存在数据flies=`hdfs dfs -ls $in_path | grep $yesterday | wc -l`if [ $flies -gt 0 ];then echo \"开始运行preprocess批处理命令,上传目录为:$out_path/$yesterday\" hadoop jar $jar_name $in_path/$yesterday $out_path/$yesterdayfi# 如果报错发送邮件if [ $? -gt 0 ];then echo \"运行失败,发送邮件...\"fi 第二步日志增强日志增强由ClickStreamPageView.java完成,下面是核心代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233package com.zonegood.hive.mr;import com.zonegood.hive.mrbean.WebLog;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.*;/** * * 统计点击流PV * * 给每条数据添加sessionId,默认session过期时间为30分,当前页面停留了多长时间 * 每个session按访问时间排序,并标上序号 * 保留WebLog中部分字段 * * @author zyh * @create 18-11-3 下午3:41 */public class ClickStreamPageView &#123; static class ClickStreamPageViewMapper extends Mapper&lt;LongWritable, Text, Text, WebLog&gt; &#123; WebLog v = new WebLog(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = line.split(\"\\001\"); if(fields.length &gt;= 10 &amp;&amp; \"true\".equals(fields[0]))&#123; // 处理 v.setIp(fields[1]); v.setU_info(fields[2]); v.setTime(fields[3]); v.setRequest_url(fields[5]); v.setStatus(fields[6]); v.setSent_body_bytes(fields[7]); v.setRequest_referer(fields[8]); v.setUser_agent(fields[9]); k.set(v.getIp()); context.write(k,v); &#125; &#125; static class ClickStreamPageViewReducer extends Reducer&lt;Text, WebLog, NullWritable, Text&gt; &#123; public static SimpleDateFormat df2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\", Locale.US); NullWritable k = NullWritable.get(); Text v = new Text(); /** * * @param key ip * @param values &lt;PageView,PageView,PageView,PageView&gt; * @param context * @throws IOException * @throws InterruptedException */ @Override protected void reduce(Text key, Iterable&lt;WebLog&gt; values, Context context) throws IOException, InterruptedException &#123; ArrayList&lt;WebLog&gt; beans = new ArrayList&lt;WebLog&gt;(); try&#123; for (WebLog webLog:values) &#123; WebLog pv = new WebLog(); try &#123; BeanUtils.copyProperties(pv,webLog); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; beans.add(pv); &#125; Collections.sort(beans, new Comparator&lt;WebLog&gt;() &#123; @Override public int compare(WebLog o1, WebLog o2) &#123; try &#123; Date d1 = formatDate(o1.getTime()); Date d2 = formatDate(o2.getTime()); if(d1 == null || d2 == null) return 0; return d1.compareTo(d2); &#125; catch (Exception e) &#123; return 0; &#125; &#125; &#125;); /** * 1. 里面只有一个 * 1.1 设置sessionId、setp、直接输出,重置sessionId * * 2. 里面有多个 * 2.1 是第一个，直接跳过不输出 * 2.2 是最后一个，设置sessionId,设置step,输出 * 2.3 比较第n个与n-1个的时间差 * 2.2.1差 大于30s * 2.2.1.1 设置sessionId,设置step,输出第n-1个,重置sessionId,重置step * 2.2.2 差小于30s * 2.2.2.1 设置sessionId,设置step,输出第n-1个,step ++ * */ int step = 1; String sessionId = UUID.randomUUID().toString(); for (int i=0 ; i&lt;beans.size() ; i++) &#123; if(beans.size() == 1)&#123; v.set(sessionId+\"\\001\"+ beans.get(i).getIp()+\"\\001\"+ step + \"\\001\" + (60) + \"\\001\" + beans.get(i).getU_info() + \"\\001\" + beans.get(i).getTime() + \"\\001\" + beans.get(i).getRequest_url() + \"\\001\" + beans.get(i).getRequest_referer() + \"\\001\"+ beans.get(i).getUser_agent() + \"\\001\" + beans.get(i).getSent_body_bytes() + \"\\001\" + beans.get(i).getStatus()); // 输出当前pv context.write(k,v); break; &#125; if(i == 0)&#123; continue; &#125; long diffTime = 0; try &#123; diffTime = diffTime(beans.get(i).getTime(),beans.get(i-1).getTime()); &#125; catch (Exception e) &#123; &#125; if(diffTime &gt; 30 * 60 * 1000)&#123; v.set(sessionId+\"\\001\"+ beans.get(i -1).getIp()+\"\\001\"+ step + \"\\001\" + (diffTime/1000) + \"\\001\" + beans.get(i-1).getU_info() + \"\\001\" + beans.get(i-1).getTime() + \"\\001\" + beans.get(i-1).getRequest_url() + \"\\001\" + beans.get(i-1).getRequest_referer() + \"\\001\"+ beans.get(i-1).getUser_agent() + \"\\001\" + beans.get(i-1).getSent_body_bytes() + \"\\001\" + beans.get(i-1).getStatus()); // 输出当前pv context.write(k,v); sessionId = UUID.randomUUID().toString(); step = 1; &#125;else&#123; v.set(sessionId+\"\\001\"+ beans.get(i -1).getIp()+\"\\001\"+ step + \"\\001\" + (diffTime/1000) + \"\\001\" + beans.get(i-1).getU_info() + \"\\001\" + beans.get(i-1).getTime() + \"\\001\" + beans.get(i-1).getRequest_url() + \"\\001\" + beans.get(i-1).getRequest_referer() + \"\\001\"+ beans.get(i-1).getUser_agent() + \"\\001\" + beans.get(i-1).getSent_body_bytes() + \"\\001\" + beans.get(i-1).getStatus()); // 输出当前pv context.write(k,v); step ++; &#125; if(i == beans.size()-1)&#123; v.set(sessionId+\"\\001\"+ beans.get(i).getIp()+\"\\001\"+ step + \"\\001\" + (60) + \"\\001\" + beans.get(i).getU_info() + \"\\001\" + beans.get(i).getTime() + \"\\001\" + beans.get(i).getRequest_url() + \"\\001\" + beans.get(i).getRequest_referer() + \"\\001\"+ beans.get(i).getUser_agent() + \"\\001\" + beans.get(i).getSent_body_bytes() + \"\\001\" + beans.get(i).getStatus()); // 输出当前pv context.write(k,v); &#125; &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; private Date formatDate(String timeStr) throws Exception&#123; return df2.parse(timeStr); &#125; private long diffTime(String t1,String t2) throws Exception&#123; return df2.parse(t1).getTime() - df2.parse(t2).getTime(); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(ClickStreamPageView.class); job.setMapperClass(ClickStreamPageViewMapper.class); job.setReducerClass(ClickStreamPageViewMapper.ClickStreamPageViewReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(WebLog.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); &#125;&#125;自动化运行脚本click_stream_page_view.sh1234567891011121314151617181920212223242526272829303132333435363738394041#!/bin/bash# 作用:统计PageView的批处理脚本# 作者:赵一好# java 环境export JAVE_HOME=/data/jdk1.8.0_161export JRE_HOME=$&#123;JAVE_HOME&#125;/jreexport CLASS_PATH=$&#123;JAVE_HOME&#125;/lib/dt.jar:$&#123;JAVE_HOME&#125;/lib/tools.jarexport PATH=$PATH:$&#123;JAVE_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin# hadoop 环境export HADDOP_HOME=/data/hadoop-2.7.3export PATH=$PATH:$&#123;HADDOP_HOME&#125;/bin:$&#123;HADDOP_HOME&#125;/sbin# 日期yesterday=`date -d'-1 day' +%Y-%m-%d`s_year=`date -d'-1 day' +%Y`s_month=`date -d'-1 day' +%m`s_day=`date -d'-1 day' +%d`# inpathin_path=/syslog/preprocess/outpath# outpathout_path=/syslog/pageview/outpath# jar namejar_name=click_stream_page_view.jar# 检测是否存在待处理数据files=`hdfs dfs -ls $in_path | grep $yesterday | wc -l`if [ $files -gt 0 ];then echo \"开始运行PageView批处理命令,上传目录为:$out_path/$yesterday\" hadoop jar $jar_name $in_path/$yesterday $out_path/$yesterdayfi# 如果报错发送邮件if [ $? -gt 0 ];then echo \"运行失败,发送邮件...\"fi 第三步生成部分指标主要由ClickStreamVisit.java完成，下面是核心代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.zonegood.hive.mr;import com.zonegood.hive.mrbean.PageView;import com.zonegood.hive.mrbean.VisitBean;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;/** * 点击流 * 统计起始时间、结束时间、进入页面、离开页面、一共访问了多少页面、sessionId、IP、从哪来的 * @author zyh * @create 18-11-3 下午5:24 */public class ClickStreamVisit &#123; static class ClickStreamVisitMapper extends Mapper&lt;LongWritable, Text, Text, PageView&gt; &#123; Text k = new Text(); PageView v = new PageView(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(\"\\001\"); v.setSessionId(fields[0]); v.setIp(fields[1]); v.setStep(fields[2]); v.setStayTime(fields[3]); v.setU_info(fields[4]); v.setTime(fields[5]); v.setRequest_url(fields[6]); v.setRequest_referer(fields[7]); v.setUser_agent(fields[8]); v.setSent_body_bytes(fields[9]); v.setStatus(fields[10]); k.set(v.getSessionId()); context.write(k,v); &#125; &#125; static class ClickStreamVisitReducer extends Reducer&lt;Text, PageView, NullWritable, VisitBean&gt; &#123; NullWritable k = NullWritable.get(); Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;PageView&gt; values, Context context) throws IOException, InterruptedException &#123; ArrayList&lt;PageView&gt; beans = new ArrayList&lt;PageView&gt;(); // 按照步骤排序 for (PageView pv : values) &#123; PageView bean = new PageView(); try &#123; BeanUtils.copyProperties(bean, pv); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; beans.add(bean); &#125; Collections.sort(beans, new Comparator&lt;PageView&gt;() &#123; @Override public int compare(PageView o1, PageView o2) &#123; return Integer.parseInt(o1.getStep()) &gt; Integer.parseInt(o2.getStep()) ? 1 : -1; &#125; &#125;); VisitBean visitBean = new VisitBean(); visitBean.setInPage(beans.get(0).getRequest_url()); visitBean.setOutPage(beans.get(beans.size()-1).getRequest_url()); visitBean.setInTime(beans.get(0).getTime()); visitBean.setOutTime(beans.get(beans.size()-1).getTime()); visitBean.setPageVisits(beans.size()); visitBean.setRemote_addr(beans.get(0).getIp()); visitBean.setSession(beans.get(0).getSessionId()); visitBean.setReferal(beans.get(0).getRequest_referer()); context.write(k,visitBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(ClickStreamVisit.class); job.setMapperClass(ClickStreamVisitMapper.class); job.setReducerClass(ClickStreamVisitReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(PageView.class); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(VisitBean.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125;下面是配合MR自动脚本click_stream_visit.sh1234567891011121314151617181920212223242526272829303132333435363738394041#!/bin/bash# 作用:统计Visit的批处理脚本# 作者:赵一好# java 环境export JAVE_HOME=/data/jdk1.8.0_161export JRE_HOME=$&#123;JAVE_HOME&#125;/jreexport CLASS_PATH=$&#123;JAVE_HOME&#125;/lib/dt.jar:$&#123;JAVE_HOME&#125;/lib/tools.jarexport PATH=$PATH:$&#123;JAVE_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin# hadoop 环境export HADDOP_HOME=/data/hadoop-2.7.3export PATH=$PATH:$&#123;HADDOP_HOME&#125;/bin:$&#123;HADDOP_HOME&#125;/sbin# 日期yesterday=`date -d'-1 day' +%Y-%m-%d`s_year=`date -d'-1 day' +%Y`s_month=`date -d'-1 day' +%m`s_day=`date -d'-1 day' +%d`# inpathin_path=/syslog/pageview/outpath# outpathout_path=/syslog/visit/outpath# jar namejar_name=click_stream_visit.jar# 检测是否存在待处理数据files=`hdfs dfs -ls $in_path | grep $yesterday | wc -l`if [ $files -gt 0 ];then echo \"开始运行Visit批处理命令,上传目录为:$out_path/$yesterday\" hadoop jar $jar_name $in_path/$yesterday $out_path/$yesterdayfi# 如果报错发送邮件if [ $? -gt 0 ];then echo \"运行失败,发送邮件...\"fi 自动化使用部署好的Azkaban调度系统调度三个shell脚本，完成自动ETL步骤。1.编写auto_run.sh脚本1234567#!/bin/bash# 作用:自动运行preprocess pageview visit 批处理脚本sh weblog_pre_process.shsh click_stream_page_view.shsh click_stream_visit.sh2.将三个MR程序分别打包成可执行ja,可以使用Maven package指令，这边不过多详述click_stream_page_view.jarclick_stream_visit.jarweblog_pre_process.jar3.编写azkaban的job并打包成zip上传到Azkaban平台123# foo.jobtype=commandcommand=sh auto_run.shfoo.zip中一定要包含上述的所有文件,如下图所示运行foo任务，如果是绿条证明程序没问题具体操作有点繁琐，之前blog有详述，这边就略过了","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"luoxiao.cf/tags/Hadoop/"}]},{"title":"离线日志分析系统(二)","slug":"2018-11-26-离线日志分析系统(二)","date":"2018-11-26T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/11/26/2018-11-26-离线日志分析系统(二)/","link":"","permalink":"luoxiao.cf/2018/11/26/2018-11-26-离线日志分析系统(二)/","excerpt":"","text":"系统的方向为收集ngnix访问日志，做离线批处理统计分析，为上层决策提供数据支持 系统设计小型集群(3台 CentOS)，cor1/cor2/cor3 核心组件下面对用到的核心组件做一个概述，心里有一个大概FlumeNG : 主要收集WEB端产生的ngnix日志汇总到HDFS中HDFS : 存放前一天汇总的数据，为后续离线分析做准备MapReduce : 主要进行ETL，清洗，过滤，规整。根据HDFS汇总数据生成贴源表Hive : 加载HDFS上的数据到仓库（加载贴源表），使用hive sql根据贴源表进一步计算出各维度指标Sqoop : 同步Hive数据仓库中各维度指标到mysql数据库，或者redis。为WEB端展示提供基础数据Azkaban : 各个组件的粘合剂，用来调度各自动脚本，实现自动化 搭建集群1.开发阶段使用虚拟机,这边用的是VirtualBox.先准备3台CentOS虚拟机,cor1/cor2/cor3,这边不做详述2.在cor1上部署hadoop,zookeeper,hive,flume,sqoop,mysql,azkaban,这边不做详述3.在cor2上部署hadoop,zookeeper,这边不做详述4.在cor3上部署hadoop,zookeeper,这边不做详述 收集数据收集数据这边使用,flumeNG采集ngnix日志上传到HDFS，安装好flumeNG,在conf目录下创建tail-hdfs.conf1234567891011121314151617181920212223242526272829303132333435# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /home/hadoop/log/test.loga1.sources.r1.channels = c1# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/a1.sinks.k1.hdfs.filePrefix = events-a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.rollInterval = 3a1.sinks.k1.hdfs.rollSize = 20a1.sinks.k1.hdfs.rollCount = 5a1.sinks.k1.hdfs.batchSize = 1a1.sinks.k1.hdfs.useLocalTimeStamp = true#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1启动FlumeNG：1bin/flume-ng agent -c conf -f conf/tail-hdfs.conf -n a1FlumeNG手册:http://flume.apache.org/FlumeUserGuide.html当然你也可以不使用FlumeNG,使用shell脚本定时检测某个目录下所有的文件定时上传到HDFS中。下面提供一个参考脚本的写法123456789101112131415161718192021222324252627282930313233343536373839#!/bin/bash## ===========================================================================# Data Input: /data/weblog/preprocess/input# Data Output: /data/weblog/preprocess/output# Author: zyh# ===========================================================================#set java envexport JAVE_HOME=/data/jdk1.8.0_161export JRE_HOME=$&#123;JAVE_HOME&#125;/jreexport CLASS_PATH=$&#123;JAVE_HOME&#125;/lib/dt.jar:$&#123;JAVE_HOME&#125;/lib/tools.jarexport PATH=$PATH:$&#123;JAVE_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin#set hadoop envexport HADDOP_HOME=/data/hadoop-2.7.3export PATH=$PATH:$&#123;HADDOP_HOME&#125;/bin:$&#123;HADDOP_HOME&#125;/sbin#flume采集生成的日志文件存放的目录log_flume_dir=/data/flumedata/#待上传文件存放的目录log_pre_input=/data/weblog/preprocess/input#获取时间信息day_01=`date -d'-1 day' +%Y-%m-%d`syear=`date --date=$day_01 +%Y`smonth=`date --date=$day_01 +%m`sday=`date --date=$day_01 +%d`#读取日志文件的目录，判断是否有需要上传的文件files=`hadoop fs -ls $log_flume_dir | grep $day_01 | wc -l`if [ $files -gt 0 ]; thenhadoop fs -mv $&#123;log_flume_dir&#125;/$&#123;day_01&#125; $&#123;log_pre_input&#125;echo \"success moved $&#123;log_flume_dir&#125;/$&#123;day_01&#125; to $&#123;log_pre_input&#125; .....\"fi","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"luoxiao.cf/tags/Hadoop/"}]},{"title":"离线日志分析系统（一）","slug":"2018-11-25-离线日志分析系统（一）","date":"2018-11-25T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/11/25/2018-11-25-离线日志分析系统（一）/","link":"","permalink":"luoxiao.cf/2018/11/25/2018-11-25-离线日志分析系统（一）/","excerpt":"","text":"该系统主要对前一天ngnix日志进行离线批处理统计和分析，从大量数据中晒出有价值的数据，为上层决策提供数据支持，主要思想是MapReduce。使用分布式HDFS文件系统作为离线数据存储，使用hive简化MapReduce开发。ngnix 日志的格式很简单，我们要做的就是从中发掘出有价值的数据，ngnix的access.log的格式,摘抄部分日志12127.0.0.1 - - [05/Sep/2018:23:18:22 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/delete\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:40 +0800] \"GET /4DAnalog/clashreport/find HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"下面表格对上面每个字段进行了详细描述位置字段说明值1ip127.0.0.12实际上绝大多数时候这一项都是如此。这个位置用于记录浏览者的标识-3记录浏览者进行身份验证时提供的名字-4请求的时间[05/Sep/2018:23:18:22 +0800]5访问请求方式GET6浏览者请求资源路径/4DAnalog/clashreport/delete7访问协议HTTP/1.1&quot;8状态代码5029发送给客户端的总字节数，它告诉我们传输是否被打断，把日志记录中的这些值加起来就可以得知服务器在某一段时间发送了多少数据。57510请求来访地址referer,可以利用它分析出访问者从哪来的http://localhost:8080/4DAnalog/clashreport/delete11浏览器信息Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36 统计指标汇总下面对统计的指标左简单概述 1.某时间段的PVS从中可以繁衍出各种维度的PVS，比如1天中每个小时的PVS，1天中每分钟的PVS，一个月中每天的PVS 2.浏览器其中可以统计从Firfox跳转过来的PVS、从Chrome跳转过来的PVS、从IE跳转过来的PVS 3.某时间段内的人均浏览页数从中可以繁衍出各种维度的指标，比如一个月内每日人均浏览页，一天中每小时的浏览页等 4.搜索引擎从百度跳转过来的人数、从bing跳转过来的人数、从google跳转过来的人数 5.TopN指标一天内访问页面热度排行，某小时内访问页面热度排行，一个月页面热度排行 6.独立访客一年内每个月的独立访客、一个月内每天的独立访客 7.回头客/单次访客一个月内回头客/单次访客 8.转换率生成一个月内评估业务指标的数据（各个业务的转换率）","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"luoxiao.cf/tags/Hadoop/"}]},{"title":"Ngnix Websocket 400 错误","slug":"2018-11-15-Ngnix Websocket 400 错误","date":"2018-11-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/11/15/2018-11-15-Ngnix Websocket 400 错误/","link":"","permalink":"luoxiao.cf/2018/11/15/2018-11-15-Ngnix Websocket 400 错误/","excerpt":"","text":"Spring WebSocket 结合ngnix 之后400报错！今天消息推送功能上测试服以后发现不能使用，到测试服上发现发送的请求一直返回400。后来定位到是ngnix配置问题。联系运维哥们将下面代码添加上以后成功解决:12345678map $http_upgrade $connection_upgrade &#123; default upgrade; '' close;&#125;proxy_http_version 1.1;proxy_set_header Upgrade $http_upgrade;proxy_set_header Connection $connection_upgrade;添加好之后的样子12345678910111213map $http_upgrade $connection_upgrade &#123; default upgrade; '' close;&#125;server &#123; ... location /chat/ &#123; proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; &#125;&#125;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"luoxiao.cf/tags/WebSocket/"}]},{"title":"Spring WebSocket消息推送","slug":"2018-11-14-Spring WebSocket消息推送","date":"2018-11-14T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/11/14/2018-11-14-Spring WebSocket消息推送/","link":"","permalink":"luoxiao.cf/2018/11/14/2018-11-14-Spring WebSocket消息推送/","excerpt":"","text":"需求:后台编辑推送消息，前台实时接收消息下面是js实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$(function() &#123; bdipOnline();&#125;);var bdipWebSocket;var bdipReconnectTime = 5000;function bdipOnline() &#123; if (window.WebSocket) &#123; var protocol = publicJS.protocol == \"https\"? \"wss\": \"ws\"; try&#123; bdipWebSocket = new WebSocket(encodeURI(protocol + '://' + publicJS.host +'/ws')); //cloud.bimbdip.com &#125;catch (err)&#123; console.log(\"online链接websocket失败\"); &#125; if(bdipWebSocket)&#123; bdipWebSocket.onopen = function() &#123; console.log(\"bdipWebSocket链接成功... bdipReconnectTime :\" + bdipReconnectTime) //连接成功 //bdipReconnectTime = 10000; setTimeout(bdipOnline, bdipReconnectTime); &#125;; bdipWebSocket.onerror = function() &#123; console.log(\"bdipWebSocket发生错误... bdipReconnectTime :\" + bdipReconnectTime) //连接失败 setTimeout(bdipOnline, bdipReconnectTime); bdipReconnectTime += 5000; &#125;; bdipWebSocket.onclose = function() &#123; //连接断开 console.log(\"bdipWebSocket断开链接... bdipReconnectTime :\" + bdipReconnectTime) setTimeout(bdipOnline, bdipReconnectTime); bdipReconnectTime += 5000; &#125;; //消息接收 bdipWebSocket.onmessage = function(message) &#123; console.log(\"接收到消息，消息内容为:\" + message); var data = JSON.parse(message.data); if (data.type == 'logMessage') &#123; notificate(data.data); &#125; &#125;; &#125; &#125;&#125;// 弹出消息框function notificate(_message) &#123; var msgContent = JSON.parse(_message.content) var title = msgContent.title; if(title == null || typeof(title) == 'undefined' || title == '') &#123; title = \"通知中心\"; &#125; var content = msgContent.content; if(content.length &gt; 140) &#123; content = content.substr(0, 140); &#125; $(\"#notification-title\").html(title); $(\"#notification-content\").html(content); //$(\".notification-panel\").show(); $(\".notification-panel\").slideDown(1000); setTimeout(function()&#123; //$(\".notification-panel\").hide(); $(\".notification-panel\").slideUp(1000); &#125;, 5000);&#125;下面是后台代码,先说一下实现思路:1.在后台管理系统中编辑待推送数据并完成推送消息操作;2.spring websocket 检测到用户登录，按某种规则将用户信息保存到redis；3.定时轮训查看待推送信息，当检测到用户在线时进行推送，更新数据库为已推送；其中使用到了使用redis（订阅和发布）功能进行数据推送,下面上代码：spring websocket handler 监听用户登录的代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101package com.bim.bdip.cloud.home.web;import com.alibaba.fastjson.JSONObject;import com.baomidou.mybatisplus.mapper.EntityWrapper;import com.bim.bdip.cloud.home.constant.BizServiceDefine;import com.bim.bdip.cloud.home.constant.ConstantDefine;import com.bim.bdip.cloud.home.core.base.Parameter;import com.bim.bdip.cloud.home.domain.api.ApiResultEntity;import com.bim.bdip.cloud.home.model.BimMessageCenter;import com.bim.bdip.cloud.home.model.BimUser;import com.bim.bdip.cloud.home.provider.IBizProvider;import org.apache.ibatis.session.RowBounds;import org.apache.log4j.LogManager;import org.apache.log4j.Logger;import org.redisson.api.RTopic;import org.redisson.api.RedissonClient;import org.redisson.api.listener.MessageListener;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.socket.CloseStatus;import org.springframework.web.socket.TextMessage;import org.springframework.web.socket.WebSocketMessage;import org.springframework.web.socket.WebSocketSession;import org.springframework.web.socket.handler.TextWebSocketHandler;import org.springframework.web.socket.server.support.HttpSessionHandshakeInterceptor;import java.io.IOException;import java.util.List;import java.util.Map;public class WebSocketHandler extends TextWebSocketHandler &#123; private static final Logger logger = LogManager.getLogger(WebSocketHandler.class); @Autowired private RedissonClient redissonClient; @Autowired private IBizProvider bizProvider; @Override protected void handleTextMessage(WebSocketSession session, TextMessage message) throws Exception &#123; &#125; @Override public void afterConnectionEstablished(WebSocketSession session) throws Exception &#123; super.afterConnectionEstablished(session); Map&lt;String, Object&gt; attributes = session.getAttributes(); BimUser user = (BimUser) attributes.get(ConstantDefine.LOGIN_SESSION_USER); if (user == null) &#123; session.close(); &#125; else &#123; System.out.println(\"检测到用户登录:\" + user.getId() + \"当前线程ID为:\" + Thread.currentThread().getId()); Long userId = user.getId(); String topic = String.format(\"notification:%d\", userId); RTopic&lt;BimMessageCenter&gt; rtopic = redissonClient.getTopic(topic); rtopic.removeAllListeners(); String listenerKey = String.format(\"notification:%d:listeners\", user.getId()); redissonClient.getSet(listenerKey).delete(); Integer listenerId = rtopic.addListener(new MessageListener&lt;BimMessageCenter&gt;() &#123; @Override public void onMessage(String channel, BimMessageCenter msg) &#123; System.out.println(\"向用户推送消息:\" + msg); JSONObject jsonObject = new JSONObject(); jsonObject.put(\"type\", \"logMessage\"); jsonObject.put(\"data\", msg); TextMessage textMessage = new TextMessage(jsonObject.toJSONString()); try &#123; session.sendMessage(textMessage); &#125; catch (IOException e) &#123; logger.error(\"Websocket Error.\", e); &#125; &#125; &#125;); redissonClient.getSet(listenerKey).add(listenerId); attributes.put(ConstantDefine.LISTENER_ID, listenerId); &#125; &#125; @Override public void afterConnectionClosed(WebSocketSession session, CloseStatus status) throws Exception &#123; Map&lt;String, Object&gt; attributes = session.getAttributes(); BimUser user = (BimUser) attributes.get(ConstantDefine.LOGIN_SESSION_USER); Integer listenerId = (Integer) attributes.get(ConstantDefine.LISTENER_ID); if (user != null) &#123; System.out.println(\"用户退出:\" + user.getId()); super.afterConnectionClosed(session, status); String topic = String.format(\"notification:%d\", user.getId()); String listenerKey = String.format(\"notification:%d:listeners\", user.getId()); redissonClient.getSet(listenerKey).remove(listenerId); RTopic&lt;BimMessageCenter&gt; rtopic = redissonClient.getTopic(topic); rtopic.removeListener(listenerId); &#125; &#125;&#125;下面是定时任务,定时任务轮训待推送数据，如果检测到用户登录(通过redis)，就进行数据推送(通过redis topic):1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.bim.bdip.cloud.home.scheduled;import com.baomidou.mybatisplus.mapper.EntityWrapper;import com.bim.bdip.cloud.home.model.BimMessageCenter;import com.bim.bdip.cloud.home.service.IBimMessageCenterService;import org.apache.ibatis.session.RowBounds;import org.redisson.api.RTopic;import org.redisson.api.RedissonClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.annotation.EnableScheduling;import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Service;import java.util.Date;import java.util.List;@Service@EnableSchedulingpublic class NotificationTask &#123; @Autowired private RedissonClient redissonClient; @Autowired private IBimMessageCenterService messageCenterService; @Scheduled(cron=\"1 * * * * ?\") public void scanNotifications() &#123;EntityWrapper&lt;BimMessageCenter&gt; wrapper = new EntityWrapper&lt;&gt;(); RowBounds rowBounds = new RowBounds(0, 10000000); wrapper.where(\"read_status = &#123;0&#125;\", 0).and(\"notification_status = &#123;0&#125;\", 0).and(\"message_type = &#123;0&#125;\", \"logMessage\"); List&lt;BimMessageCenter&gt; messageList = messageCenterService.selectPage(rowBounds, wrapper); System.out.println(\"轮训检测数据库数据\" + messageList.size() + \"当前线程ID为:\" + Thread.currentThread().getId() + \"当前时间为\" + new Date().getTime()); for(BimMessageCenter message : messageList) &#123; String listenerKey = String.format(\"notification:%d:listeners\", message.getReceiveId()); if(redissonClient.getSet(listenerKey).size() &gt; 0) &#123; System.out.println(\"检测到用户\" + message.getReceiveId() + \"在线，推送消息\"); String topic = String.format(\"notification:%d\", message.getReceiveId()); RTopic&lt;BimMessageCenter&gt; rTopic = redissonClient.getTopic(topic); rTopic.publish(message); if(message.getNotificationStatus() != 2)&#123; message.setNotificationStatus(2); // 已推送 this.messageCenterService.update(message); &#125; &#125; &#125; System.out.println(\"轮训结束，当前时间为:\" + new Date().getTime()); &#125;&#125;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"WebSocket","slug":"WebSocket","permalink":"luoxiao.cf/tags/WebSocket/"}]},{"title":"Hive 各种Join总计汇总","slug":"2018-11-06-hive各种join","date":"2018-11-06T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/11/06/2018-11-06-hive各种join/","link":"","permalink":"luoxiao.cf/2018/11/06/2018-11-06-hive各种join/","excerpt":"","text":"关于hive中的各种join，下边做个总结先准备数据12345678910111213# a.txt1,a2,b3,c4,d7,y8,u# b.txt2,bb3,cc7,yy9,pp建表：12345create table a(id int,name string)row format delimited fields terminated by ',';create table b(id int,name string)row format delimited fields terminated by ',';导入数据：123load data local inpath '/home/hadoop/a.txt' into table a;load data local inpath '/home/hadoop/b.txt' into table b;实验：inner joinselect * from a inner join b on a.id=b.id;left joinselect * from a left join b on a.id=b.id;right joinselect * from a right join b on a.id=b.id;outer joinselect * from a full outer join b on a.id=b.id;left semi joinselect * from a left semi join b on a.id = b.id;","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"luoxiao.cf/tags/Hive/"}]},{"title":"Lambda架构","slug":"2018-10-28-Lambda架构设计","date":"2018-10-28T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/10/28/2018-10-28-Lambda架构设计/","link":"","permalink":"luoxiao.cf/2018/10/28/2018-10-28-Lambda架构设计/","excerpt":"","text":"Lambda架构是由Strom的作者提出的一种通用系统架构，可以说它是一种混合式架构，其架构大致分为三层batch layer、speed layer和serving layer。batch layer : 称为批处理层，主要进行离线数据处理和计算。计算数据量大，延时高是其主要特点speed layer : 称为流处理层 ，主要进行实时数据处理和计算。远远不断的处理过来的数据，延时低是其主要特点Serving layer : 主要进行合并Batch View和Real-time view 中的结果到最终数据集 批处理层(batch layer)批量计算:批量获取数据、批量传输数据、周期性批量计算数据并进行数据展示。下面对具体可行方案进行探讨数据收集：FlumeNG 是一个分布式的采集、聚和传输系统,其核心组件是Agent。使用它可以实现从源到目标的海量数据传输。数据同步层：sqoop可以完成关系型数据到分布式文件系统之间的数据同步数据存储：可以使用HDFS+Hbase分布式计算：mapreduce 或者spark。mapreduce中是一个一个的job，spark中则是一个一个RDD。其中他们俩最大的一个差别是HDFS处理的 数据必须被放在磁盘上，而Spark则不是它的数据可以都在内存中完成序列化框架：Thrift、Protocol buffer、Avro视图：由reduce产生，可以存在Hbase、mysql、redis、memcache中都可以 实时处理层实时处理可以实时产生、传输、计算并展示数据。其特点主要是延时低，数据收集 flume(OG/NG) : agent -&gt; storage,其中可以变形 agent -&gt; kafka -&gt; storage数据分析 (storm(毫级)、spark streaming(秒级))存储数据库 : (Hbase、Cassandra、Impala、Redis/memcache、MySql) 服务层特点: 随机读 、非常端的时间内返回结果、读取batch layer 和speed layer结果,并对其归并 Lambda架构具体实现","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[]},{"title":"十二种大数据经典案例你做过几个?","slug":"2018-10-28-12种大数据相关系统你做过几个？","date":"2018-10-28T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/10/28/2018-10-28-12种大数据相关系统你做过几个？/","link":"","permalink":"luoxiao.cf/2018/10/28/2018-10-28-12种大数据相关系统你做过几个？/","excerpt":"","text":"1离线数据处理项目内容为通过对网站访问日志的采集和清洗，结合数据库中的结构化用户数据，统计并展示网站的PV、UV情况，以对网站的运行情况进行监控。通过此项目，回顾并串联前面讲述的离线数据处理相关技术，如：FIune、Sqoop、Hive、Spark等，了解和掌握PB级数据离线处理的一般过程和架构。 2流式数据处理项目内容为通过对数据库交易数据修改的实时同步，监控网站实时交易情况，以提高网站交易情况监控的时效性，降低网站运行的风险。 通过此项目，回顾并串联前面讲述的实时数据处理相关技术，如：kafka、Spark、Streaning和HBase等，了解和掌握实时数据处理的一般过程和架构。 3推荐系统项目内容，基于公开数据库的商品推荐，某大型互金公司产品推荐系统剖析， 通过对公司实际推荐项目的剖析和根据真实数据搭建推荐系统的实操演练，了解推荐系统的一般架构和常用算法。 4搜索系统项目内容，通过网站爬虫爬取网站数据，然后基于KlastlcSeard和Klbana搭建一个完整的搜索系统。 5系统运行情况仪表盘通过对网站访问日志的采集和清洗，结合数据库中的结构化用户数据，统计并展示网站的PV，UV情况，以对网站的运行情况进行监控。通过此项目，回顾并串联前面讲述的离线数据处理相关技术，如Flume，Sqoop，Hive，Spark等，掌握PB级数据离线处理的一般过程和架构。 6实时交易监控系统过对数据库交易数据修改的实时同步，监控网站实时交易情况，以提高网站交易情况监控的时效性，降低网站运行的风险。通过此项目，回顾并串联前面讲述的实时数据处理相关技术，如Kafka，Spark Streaming和HBase等，掌握实时数据处理的一般过程和架构。 7推荐系统理论与实战讲解推荐系统的相关背景，常用算法及通用架构；基于公开数据集从零构建一个电影推荐系统。通过对公司实际推荐项目的剖析和根据真实数据搭建推荐系统的实操演练，了解推荐系统的一般架构和常用算法 8数据仓库搭建理论与实战讲解数据仓库搭建的方法论，常用建模理论；以互金公司数据仓库搭建场景作为切入，实例演示数据仓库搭建过程及技术架构。 9分布式业务监控系统讲解业务监控系统需求背景，基于大数据的技术方案；通过实例代码搭建完整的业务监控系统 10基于ES的日志系统基于Flume，ElasticSearch等技术搭建系统日志收集与查询系统。 11信贷需求预测系统以京东信贷需求预测竞赛为背景，实例讲解数据挖掘项目中如何设计特征，模型基础，建模以及调参等。 12用户画像系统讲解用户画像系统的需求背景，基于大数据技术的解决方案；通过实例代码演示用户画像系统的搭建。转自大数据开发师https://blog.csdn.net/wj1314250/article/details/80679791","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[]},{"title":"人脸识别","slug":"2018-10-13-人脸识别","date":"2018-10-13T00:00:00.000Z","updated":"2019-11-13T04:56:08.677Z","comments":true,"path":"2018/10/13/2018-10-13-人脸识别/","link":"","permalink":"luoxiao.cf/2018/10/13/2018-10-13-人脸识别/","excerpt":"","text":"最近项目需要进行人人脸识别、人脸特征分析相关开发。在Raspberry Pi中安装face_recognition库,github代码库https://github.com/ageitgey/face_recognition Raspberry 操作系统信息: 硬件需要准备Raspberry Pi主板和PiCamera摄像头。 安装dlibface_recognition 依赖dlib库，先安装dlib库,参考 https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65 遇到的问题描述:1.apt-get卡住0 [连接到archive.ubuntu.com],参考https://ubuntuqa.com/article/408.html 中的最佳解决方案 解决问题2.sudo pip3 install face_recognition报错，错误如下123456789101112Exception:Traceback (most recent call last): File \"/usr/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 298, in _error_catcher .......socket.timeout: The read operation timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 215, in main .....requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.piwheels.org', port=443): Read timed out.尝试手动安装，从这里https://download.csdn.net/download/chentianlong150/10139353下载了个face_recognition_models，手动装了一下以后根本不是face_recognition项目也不知道是个啥东西于是卸载了最后没办法了又反复运行了下面的命令（3次，中间也有失败）。但最后成功了。怀疑是墙的原因Orz1$ sudo pip3 install face_recognition3.缺少libatlas.so.3库错误信息:12345678Traceback (most recent call last): File \"facerec_on_raspberry_pi.py\", line 9, in &lt;module&gt; import face_recognition File \"/usr/local/lib/python3.5/dist-packages/face_recognition/__init__.py\", line 7, in &lt;module&gt; from .api import load_image_file, face_locations, batch_face_locations, face_landmarks, face_encodings, compare_faces, face_distance File \"/usr/local/lib/python3.5/dist-packages/face_recognition/api.py\", line 4, in &lt;module&gt; import dlibImportError: libatlas.so.3: cannot open shared object file: No such file or directory提示没有libatlas.so.3 参考https://askubuntu.com/questions/719538/libatlas-package1$ sudo apt-get install -y libatlas-*4.提示没有摄像头1234Traceback (most recent call last): File \"facerec_on_raspberry_pi.py\", line 10, in &lt;module&gt; import picameraImportError: No module named 'picamera'安装摄像头库12$ sudo apt-get install python3-picamera$ sudo pip3 install --upgrade picamera[array]最开始没有摄像头报错，上淘宝买个吧！1picamera.exc.PiCameraError: Camera is not enabled. Try running 'sudo raspi-config' and ensure that the camera has been enabled. picamera 使用picamera API 地址：https://picamera.readthedocs.io/en/release-1.13/quickstart.html 遇到的问题描述:摄像头没插紧报错:123456pi@raspberrypi ~ $ raspistill -o image.jpgmmal: mmal_vc_component_create: failed to create component 'vc.ril.camera' (1:ENOMEM)mmal: mmal_component_create_core: could not create component 'vc.ril.camera' (1)mmal: Failed to create camera componentmmal: main: Failed to create camera componentmmal: Camera is not detected. Please check carefully the camera module is installed correctly参考下面blog都没解决问题12345https://www.raspberrypi.org/forums/viewtopic.php?f=43&amp;t=101291http://raspberrypi.stackexchange.com/questions/10545/noir-camera-not-workinghttp://www.element14.com/community/thread/31835?start=0&amp;tstart=0http://www.raspberrypi.org/forums/viewtopic.php?f=43&amp;t=79890http://www.raspberrypi.org/forums/viewtopic.php?f=43&amp;p=655380最后发现是摄像头上面没装紧(好坑呵…),注意不是主板与picamera接口出没插紧，摄像头是可装可拆的，见下图。刚接触对摄像头构造了解。参考文档汇总:Python PIL库:https://blog.csdn.net/zhangziju/article/details/79123275face_recognition Github库:https://github.com/ageitgey/face_recognitionface_recognition中文文档:https://www.jianshu.com/p/4dd131567015face_recognition API中文:https://www.jianshu.com/p/a0e61f5fd570Raspberry Pi安装dlib文档:https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65macOS或Ubuntu上安装dlib文档:https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeafPiCarema 文档 :https://picamera.readthedocs.io/en/release-1.13/install.html","categories":[{"name":"物联网","slug":"物联网","permalink":"luoxiao.cf/categories/物联网/"}],"tags":[{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"luoxiao.cf/tags/Raspberry-Pi/"}]},{"title":"校对集群服务器时间","slug":"2018-10-12-校对集群服务器时间","date":"2018-10-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/10/12/2018-10-12-校对集群服务器时间/","link":"","permalink":"luoxiao.cf/2018/10/12/2018-10-12-校对集群服务器时间/","excerpt":"","text":"做分布式如果服务器的时间不同步会造成意想不到的问题，再进行之前最好校对服务器时间。 校对输入 date 命令可以查看当前服务器时间安装ntp工具进行时间校对1$ sudo yum install ntp校对时间之前注意先关闭防火墙123$ service iptables stop # 关闭防火墙$ ntpdate cn.pool.ntp.org # 同步时间$ service iptables start # 开启防火墙","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[]},{"title":"Ubuntu16.04TLS安装VirtualBox","slug":"2018-10-09-Ubuntu16.04TLS安装VirtualBox","date":"2018-10-09T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/10/09/2018-10-09-Ubuntu16.04TLS安装VirtualBox/","link":"","permalink":"luoxiao.cf/2018/10/09/2018-10-09-Ubuntu16.04TLS安装VirtualBox/","excerpt":"","text":"最近学习使用VMware虚拟机一直CPU飙升，开两台三台没问题，一开多就卡顿。估计是Ubuntu和VMware虚拟机兼容不是那么好。这边打算换个VirtualBox试试。尝试到oracle的https://www.virtualbox.org/网站下载ubuntu平台最新版的virtualbox软件，是成功安装了，但是一运行就报内核的错。折腾两天，开始以为是必须要降低内核才能解决，最后偶然看到askubuntu一片文章，最后重新卸载成功安装参考下面两篇blog https://askubuntu.com/questions/947189/cannot-run-virtualbox-on-ubuntu-16-04和https://tecadmin.net/install-oracle-virtualbox-on-ubuntu/穿插一个小插曲，source.list修改了以后运行sudo apt-get update一直卡住0 [连接到archive.ubuntu.com]试了很多办法解决不了更新源的问题，最后看到下面的文章，试了里面的最佳解决方案解决卡0的问题https://ubuntuqa.com/article/408.html","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"自制百度网盘下载器","slug":"2018-10-08-自制百度网盘下载器","date":"2018-10-08T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/10/08/2018-10-08-自制百度网盘下载器/","link":"","permalink":"luoxiao.cf/2018/10/08/2018-10-08-自制百度网盘下载器/","excerpt":"","text":"每天手动下载觉得很麻烦，后来想用树梅派+BaiduPCS搞一个自动下载器，定时下载网盘内容到移动硬盘上，实现云盘到移动硬盘的自动同步。BaiduPCS-Go用Go语言编写,操作类似于Linux，如果会linux使用起来非常得心应手，操作基本相仿。下面记录一下遇到的各种坑。 树梅派无法ssh在window 用ssh + wpa_supplicant.conf 配置方式失效，一直扫不到ip。没有显示器的树梅派真的很难用Orz，最后实在没辙，接了根网线插到交换机上，使用 Nmap - Zenmap GUI扫描，最终拿到ip这边附上Nmap软件链接软件扫描指令1nmap -sn 192.168.0.0/24如果扫描到会有类似下面的信息Nmap scan report for 192.168.0.183Host is up (0.0080s latency).MAC Address: B8:27:EB:DB:97:E7 (Raspberry Pi Foundation)这边把参考的博客记录一下https://blog.csdn.net/wongnoubo/article/details/79628313另外除了Nmap扫描意外，在手机端下载Fing也可以完成扫描。后续配合Termius可以实现手机端的远程ssh! Permission deniedSSH连接respberry以后，需要将硬盘挂在到树梅派上，下载的时候直接指定到硬盘目录。按照常规挂载好硬盘，结果在往硬盘写数据时遇到Permission denied的错误,后才发现是因为挂载方式不恰当，使用下面的方法就OK了：1$ sudo mount -o uid=pi,gid=pi /dev/sda1 /mnt/1GB_USB_flash因为没有安装ntfs-3g的驱动也报错了，装上驱动解决问题：1$ sudo apt-get install ntfs-3ghttps://blog.csdn.net/lovelovelovelovelo/article/details/53862819https://unix.stackexchange.com/questions/195828/permission-denied-on-mounted-devices 文件分隔符思路：使用BaiduPCS指令读取百度网盘内容，然后逐一判断内容在磁盘中是否存在，如果不存在下载。但是没有想到的是网盘中的内容可能中间带着空格， 这样我在shell脚本那里面遇到了处理空格的难题，像同济大学 线性代数第一讲.mp4本来是要下载同济大学 线性代数第一讲.mp4，结果下载成了同济大学。在网上也搜了一些资料，有一些网友提供了利用IFS处理文件名中的空格，但是没成功，在linux处理空格不是我的强项，下面贴出来我写的shell，希望各为linux大牛能帮我出个好主意！12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#/bin/bash# 设置并发的进程数thread_num=5a=$(date +%H%M%S)# mkfifotempfifo=\"my_temp_fifo\"mkfifo $&#123;tempfifo&#125;# 使文件描述符为非阻塞式exec 6&lt;&gt;$&#123;tempfifo&#125;rm -f $&#123;tempfifo&#125;# 为文件描述符创建占位信息for ((i=1;i&lt;=$&#123;thread_num&#125;;i++))do &#123; echo &#125;done &gt;&amp;6exclusion_column=\"目录总数: 10\"# 这边的sed 是对文件名中可能是空格的进行处理for line in `BaiduPCS-Go ls| awk '&#123;for(i=1;i&lt;=4;i++)&#123;$i=\"\"&#125;;print $0&#125;' | sed 's/ /_-_/g'`#for line in `seq 1 10`do &#123; # 解决空格问题 original_line=\"$(echo $line | sed s'/_-_/ /g')\" # 去掉目录最后的分隔符 original_line=$&#123;original_line%*/&#125; if [ -n \"$&#123;original_line&#125;\" ];then echo \"$&#123;original_line&#125; 不为空\" if [[ \"$original_line\" =~ ^目录总数.* ]];then echo \"$&#123;original_line&#125; 不是目录总数\" read -u6 &#123; sleep 1 #echo \"$&#123;original_line&#125;\" #BaiduPCS-Go d \"$&#123;line%*/&#125;\" &amp; echo \"\" &gt;&amp;6 &#125; &amp; fi fi &#125;donewait# 关闭fd6管道exec 6&gt;&amp;-b=$(date +%H%M%S)echo -e \"startTime:\\t$a\"echo -e \"endTime:\\t$b\"上面的套路不行，后来我发现用BaiduPSC d *这个命令会下载盘内所有数据，如果检测到本地磁盘已经存在，就会自动跳过。下面提供一下BaiduPSC相关API链接https://github.com/iikira/BaiduPCS-Go 日志管理将日志重定向到myout.file文件中,方便后续查看1nohup command &gt; myout.file 2&gt;&amp;1 &amp;","categories":[{"name":"物联网","slug":"物联网","permalink":"luoxiao.cf/categories/物联网/"}],"tags":[{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"luoxiao.cf/tags/Raspberry-Pi/"}]},{"title":"一元线性回归分析","slug":"2018-10-07-一元线性回归分析","date":"2018-10-07T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/10/07/2018-10-07-一元线性回归分析/","link":"","permalink":"luoxiao.cf/2018/10/07/2018-10-07-一元线性回归分析/","excerpt":"","text":"回归分析就是利用样本(已知数据),产生拟合方程,根据回归结果，得出各个因变量对目标变量产生的影响，还可以(对未知数据)进行预测。 计算实例下面参考薛毅的书例6.5进行阐述一元回归模型的计算过程。 画出散点图我们利用R画出散点图123456789101112131415161718192021222324# 读取数据，生成矩阵X &lt;- matrix(c(194.5, 20.79, 1.3179, 131.79,194.3, 20.79, 1.3179, 131.79,197.9, 22.40, 1.3502, 135.02,198.4, 22.67, 1.3555, 135.55,199.4, 23.15, 1.3646, 136.46,199.9, 23.35, 1.3683, 136.83,200.9, 23.89, 1.3782, 137.82,201.1, 23.99, 1.3800, 138.00,201.4, 24.02, 1.3806, 138.06,201.3, 24.01, 1.3805, 138.05,203.6, 25.14, 1.4004, 140.04,204.6, 26.57, 1.4244, 142.44,209.5, 28.49, 1.4547, 145.47,208.6, 27.76, 1.4434, 144.34,210.7, 29.04, 1.4630, 146.30,211.9, 29.88, 1.4754, 147.54,212.2, 30.06, 1.4780, 147.80),ncol=4, byrow=T,dimnames = list(1:17, c(\"F\", \"h\", \"log\", \"log100\")));forbes&lt;-as.data.frame(X);# 将F列映射在x轴和log100列映射在y轴描绘出散点图plot(forbes$F,forbes$log100);可以大致看出图形乘线性增长趋势，做回归分析，设线性回归方程为:y=ax+by=ax+by=ax+b根据线性回归方程，在R中描绘出log100关于F的函数1234lm.sol&lt;-lm(log100~F,data=forbes);# 画出回归方程abline(lm.sol); R是怎么画出这条线的？首先做样本点到一元回归方程且平行与Y轴的一条线段，并求得每个样本点的误差值,这样问题就可以转换为怎么画这条直线才能使得残差平方和最小什么是残差平方和,对于上图中每个误差值的绝对值的总和称为残差和，由于带着绝对值求解不太方便，所以对误差值的平方后求和，即为残差平方和f(a,b)=∑i=1n(Yi−(aXi+b))2f\\left(a,b\\right)=\\sum_{i=1}^{n}{\\left(Y_i - \\left(\\ aX_i + b \\right)\\right)^2}f(a,b)=​i=1​∑​n​​(Y​i​​−( aX​i​​+b))​2​​对于函数的极值问题，利用高等数学中偏导数定理，即$$\\frac{\\delta f}{\\delta a}=0$$和$$\\frac{\\delta f}{\\delta b}=0$$。下面为证明过程,由于公式太难打这边就插图了：最终求得a和b的值。有了这个公式，对于本例子，我们就可以算出拟合直线具体是什么。分别求出公式中的各种平均数，然后带入即可，最后算出a=0.89546，b=-42.13087最终的回归拟合直线为Y=−42.13087+0.89546XY=-42.13087 + 0.89546XY=−42.13087+0.89546X 评价回归线拟合程度的好坏可以使用R里面的**summary()**函数解决这个问题。12345678910111213141516171819&gt; summary(lm.sol)Call:lm(formula = log100 ~ F, data = forbes)Residuals: Min 1Q Median 3Q Max -0.32261 -0.14530 -0.06750 0.02111 1.35924 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -42.13087 3.33895 -12.62 2.17e-09 ***F 0.89546 0.01645 54.45 &lt; 2e-16 ***---Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.3789 on 15 degrees of freedomMultiple R-squared: 0.995, Adjusted R-squared: 0.9946 F-statistic: 2965 on 1 and 15 DF, p-value: &lt; 2.2e-16参数详述Residuals: 列出了残差的最小值、下四分为数、中位数、上四分为数、最大值Coefficients : -42.13087表示的是截距、0.89546表示的是斜率,推算的系数的标准差,t值,P-值(结合Signif做显著性检验)Signif:显著性标记,三颗表示极度显著,二颗表示高度显著,一颗*表示显著,圆点表示不太显著,没有记号表示不显著Residual standard error : 残差标准查Multiple R-squared : 相关系数平方F-statistic: F检验值推荐一篇不错的blog回归分析法&amp;一元线性回归操作和解释","categories":[{"name":"机器学习","slug":"机器学习","permalink":"luoxiao.cf/categories/机器学习/"}],"tags":[{"name":"R","slug":"R","permalink":"luoxiao.cf/tags/R/"}]},{"title":"kafka集群部署","slug":"2018-09-29-kafka集群部署","date":"2018-09-29T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/29/2018-09-29-kafka集群部署/","link":"","permalink":"luoxiao.cf/2018/09/29/2018-09-29-kafka集群部署/","excerpt":"","text":"下载安装包http://kafka.apache.org/downloads.html我下载的版本是 kafka_2.12-2.0.0.tgz 解压安装包123$ tar -zxvf /export/software/ kafka_2.12-2.0.0.tgz -C /export/servers/$ cd /export/servers/$ ln -s kafka_2.12-2.0.0 kafka 修改配置文件12$ cp /export/servers/kafka/config/server.properties /export/servers/kafka/config/server.properties.bak$ vi /export/servers/kafka/config/server.properties参考的配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// broker的全局唯一标识broker.id=0// 用来监听链接的端口,producer或consumer将在此端口建立链接port=9092// 如果不配 使用终端broker list 发送数据会报错listeners=PLAINTEXT://192.168.1.10:9092// 处理网络请求的线程数量num.network.threads=3// 处理磁盘io的线程数量num.io.threads=8// 发送套接字的缓冲区大小socket.send.buffer.bytes=102400// 接收套接字的缓冲区域大小socket.receive.buffer.bytes=102400// 请求套接字的缓冲区大小socket.request.max.bytes=104857600// 日志存放目录log.dirs=/export/servers/logs/kafka// topic在当前broker上的分片数num.partitions=2// 用来恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1// segment文件保留的最长时间，超时将会被删除offsets.topic.replication.factor=1// 滚动生成新的segment文件的最大时间log.retention.hours=168log.roll.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000// zokeeper的链接端口zookeeper.connect=localhost:2181zookeeper.connection.timeout.ms=6000# false时执行topic delete 时进行逻辑删除delete.topic.enable=true 分发安装包1234567$ scp -r /export/servers/ kafka_2.12-2.0.0 kafka02:/export/servers// 然后分别在各机器上创建软连$ cd /export/servers/$ ln -s kafka_2.12-2.0.0 kafka再次修改配置文件依次修改各服务器上配置文件的的broker.id，分别是0,1,2不得重复。运行下面指令启动kafka集群1$ bin/kafka-server-start.sh config/server.propertiesserver-start.sh config/server.properties1234567### Kafka常用操作命令查看当前服务器中的所有topic```shell$ bin/kafka-topics.sh --list --zookeeper zk01:2181创建topic1$ ./kafka-topics.sh --create --zookeeper mini1:2181 --replication-factor 1 --partitions 3 --topic first删除topic1$ sh bin/kafka-topics.sh --delete --zookeeper zk01:2181 --topic test需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。通过shell命令发送消息1$ kafka-console-producer.sh --broker-list 192.168.1.12:9092 --topic first如果报错参考文章最下面的报错信息通过shell消费消息1$ bin/kafka-console-consumer.sh --bootstrap-server 192.168.1.12:9092 --from-beginning --topic first查看消费位置1$ sh kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk01:2181 --group testGroup查看某个Topic的详情1$ sh kafka-topics.sh --topic test --describe --zookeeper zk01:2181 困难记录运行bin/kafka-console-producer.sh --broker-list cor3:9092 --topic first发送消息时错误:ERROR Error when sending message to topic first with key: null, value: 0 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TimeoutException: Expiring 2 record(s) for first-0: 1625 ms has passed since batch creation plus linger time解决办法:修改配置文件中1listeners=PLAINTEXT://192.168.1.11:9092运行1bin/kafka-console-producer.sh --broker-list 192.168.1.11:9092 --topic first参考blog https://blog.csdn.net/lvbinibnsb/article/details/81542893","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"luoxiao.cf/tags/Kafka/"}]},{"title":"kafka基本概念","slug":"2018-09-29-kafka基本概念","date":"2018-09-29T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/29/2018-09-29-kafka基本概念/","link":"","permalink":"luoxiao.cf/2018/09/29/2018-09-29-kafka基本概念/","excerpt":"","text":"目录大纲什么是kafkaJMS规范JMS消息传输模型JMS核心组件Kafka核心组件 什么是kafka在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。KAFKA + STORM +REDISApache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性 JMS规范JMS是Java提供的一套技术规范,用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活适用场景: 生产消费者模式（生产者、服务器、消费者） JMS消息传输模型1.点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。2.发布/订阅模式（一对多，数据生产后，推送给所有订阅者）发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即时当前订阅者不可用，处于离线状态。123queue.put（object） #数据生产queue.take(object) #数据消费 JMS核心组件Destination：消息发送的目的地，也就是前面说的Queue和Topic。Message：从字面上就可以看出是被发送的消息。Producer： 消息的生产者，要发送一个消息，必须通过这个生产者来发送。MessageConsumer： 与生产者相对应，这是消息的消费者或接收者，通过它来接收一个消息。通过与ConnectionFactory可以获得一个connection,通过connection可以获得一个session会话。 Kafka核心组件Topic ：消息根据Topic进行归类Producer：发送消息者Consumer：消息接受者broker：每个kafka实例(server)Zookeeper：依赖集群保存meta信息","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"luoxiao.cf/tags/Kafka/"}]},{"title":"Matlab和R矩阵运算命令汇总","slug":"2018-09-27-Matlab和R矩阵运算命令汇总","date":"2018-09-27T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/27/2018-09-27-Matlab和R矩阵运算命令汇总/","link":"","permalink":"luoxiao.cf/2018/09/27/2018-09-27-Matlab和R矩阵运算命令汇总/","excerpt":"","text":"matlab 常用命令矩阵创建123a=[1,2,3;4,5,6;7,8,9]b=[1:1:3;4:1:6;7:1:9]c=[linspace(1,3,3); linspace(4,6,3); linspace(7,9,3)]全为1的矩阵12ones(3)ones(3,4)全为0的矩阵12zeros(3)zeros(3,4)在(0,1)区间均匀分布的随机阵12rand(3)rand(3,4)单位阵1eye()均值为0,方差为1的标准正态分布随机矩阵12randn(3)randn(3,4)对角矩阵12diag([1,2,3,4])diag([1,2,3,4],1)利用小矩阵构建大矩阵12e=[a;b]f=[a,b]矩阵的拆分123456a(2,2)a(1,:)a(:,3)a(1:2,1:2)a(1,1:end)a(end,end)矩阵的逆1inv(a)行列式1det(a)对角线元素12diag(a)diag(a,1)矩阵转置1a’矩阵开方12sqrt(a)a.^0.5在MATLAB中,有一种特殊的运算,因为其运算符是在有关算术运算符前面加点,所以叫点运算,如:.*、./、.\\和.^。两矩阵进行点运算是指它们的对应元素进行相关运算,要求两矩阵的维参数相同。12345a.*ba.\\ba./ba.^2a.^0.5&lt;(小于)、&lt;=(小于或等于)、&gt;(大于)、&gt;=(大于或等于)、==(等于)、~=(不等于)123456789a&lt;ba&lt;=ba&gt;ba&gt;=ba==ba~=ba&gt;5a&gt;=5a==5找出a中大于5的元素位置1find(a&gt;5)改变维数12a=[1:12]reshape(a,3,4)旋转90度1rot90(b)左右翻转1fliplr(b) ; flipdim(b,2)上下翻转1flipud(b) ; flipdim(b,1)上三角元素12tril(c)tril(c,1)下三角元素12triu(c)triu(c,-1)确定矩阵的维数1size(b)求矩阵的迹1trace(c) R 常用命令创建矩阵1234a=matrix(c(1,2,3,4),ncol=2,byrow=T)b=matrix(c(5,6,7,8),ncol=2,byrow=T)c=matrix(c(1,2,3,4),ncol=2,byrow=F)d=matrix(c(5,6,7,8),ncol=2)矩阵线性运算12345a+ba-b2*a4*ba*b矩阵乘法123a%*%bcrossprod(a,b)t(a)%*%b矩阵转置12t(a)t(b)t()作用于向量上123x=c(1:12)t(x)t(t(x))取方阵的对角线元素1diag(a)求方阵的迹1sum(diag(a))构造对角矩阵1c=diag(c(1,2,3,4))矩阵求逆1solve(a)矩阵的行列式123det(a)det(b)det(c)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"luoxiao.cf/categories/机器学习/"}],"tags":[{"name":"Matlab","slug":"Matlab","permalink":"luoxiao.cf/tags/Matlab/"}]},{"title":"数据结构分块查找","slug":"2018-09-26-数据结构分块查找","date":"2018-09-26T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/26/2018-09-26-数据结构分块查找/","link":"","permalink":"luoxiao.cf/2018/09/26/2018-09-26-数据结构分块查找/","excerpt":"","text":"分块查找是顺序查找和二分查找的一个折中选择。它由索引表和块*n组成遵循这样的原则：索引表中存放这块内最大key和块的内存起始地址,并且索引表内key由小到大排列;每个块内的key不必有序。块的大小决定这查询的效率。随意取会影响查询速度，后面会介绍块的最优大小如何算得。分块查找的结构图如下图所示：查询的过程分为两步，先通过索引表确定key所在块，由于索引表是有序的并且数据量不会很大、所以使用顺序或者二分查找都可以。确定到块后由于块内无序，所以只能使用顺序查找。那么求得的总的平均查找长度为ASLbs=Lb+LwASL_{bs} = L_b + L_wASL​bs​​=L​b​​+L​w​​其中,$$L_b$$为查找索引表确定所在块的平均查找长度，$$L_w$$为在块中查找元素的平均查找长度一般情况下，为进行分块查找，可以将长度为n的表平均分为b块，每块含s个记录，则 $$b=\\lceil\\frac{n}{s}\\rceil$$,又假定表中每个记录的查找概率相等，则每个块查找的概率为$$\\frac{1}{b}$$,块中每个记录的查找概率为 $$\\frac{1}{s}$$若用顺序查找确认所在块，则分块查找的平均查找长度为：ASLbs=Lb+Lw=1b∑j=1bj+1s∑i=1si=b+12+s+12ASL_{bs} = L_b + L_w = \\frac{1}{b}\\sum_{j=1}^b j + \\frac{1}{s}\\sum_{i=1}^s i = \\frac{b+1}{2} + \\frac{s+1}{2}ASL​bs​​=L​b​​+L​w​​=​b​​1​​​j=1​∑​b​​j+​s​​1​​​i=1​∑​s​​i=​2​​b+1​​+​2​​s+1​​继续推导ASLbs=12(ns+s)+1ASL_{bs} = \\frac{1}{2}\\left(\\frac{n}{s}+s\\right)+1ASL​bs​​=​2​​1​​(​s​​n​​+s)+1此时的平均查找长度不仅和表长n有关，而且和每一块中的记录个数也有关，在给定n的前提下， s是可以选择的，容易证明，当s是$$\\sqrt{ n }$$ 时，$$ASL_{bs}$$取最小值$$\\sqrt{ n }+1$$，这个比值比顺序查找虽然快但不及二分查找。","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"ubuntu 16.04安装matlab","slug":"2018-09-25-ubuntu 16.04安装matlab","date":"2018-09-25T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/25/2018-09-25-ubuntu 16.04安装matlab/","link":"","permalink":"luoxiao.cf/2018/09/25/2018-09-25-ubuntu 16.04安装matlab/","excerpt":"","text":"下载先下载MATLAB2017a,资源在这里 安装首先我们在家目录下面创建一个文件夹，名字随意，你自己定，我这里就叫matlab吧1mkdir matlab随后我们cd到你的matlab的文件目录下，然后先把第一个iso文件挂载到ubuntu上12sudo mount -o loop R2017a_glnxa64_dvd1.iso /home/(你的用户名)/matlab执行matlab目录下的install进行安装1sudo /home/(你的用户名)/matlab/install然后就进入了安装界面,选择用秘钥进行安装，然后点击next选择yse,下一步激活码09806-07443-53955-64350-21751-41297,安放在MATLAB文件下readme.txt这里因为我安装过了，所以显示的是installed,继续点击next然后到了60%-70%,会提示选择卸载当前的挂载点，选择第二个iso镜像，所以1234sudo umount /home/(你的用户名)/matlab sudo mount -o loop R2017a_glnxa64_dvd2.iso /home/(你的用户名)/matlab这里第二个挂载的镜像要跟第一个挂载的位置要相同，我这里就都是matlab,然后点击继续就可以了安装完之后我们来进行激活12345sudo mkdir /usr/local/MATLAB/R2017a/bin/licenses/ cd /your/MATLAB/files/path // 这里自己要根据自己实际的路径来sudo cp license_standalone.lic /usr/local/MATLAB/R2017a/bin/licenses/ sudo cp libmwservices.so /usr/local/MATLAB/R2017a/bin/glnxa64/把挂载给卸下来1sudo umount /home/(你的用户名)/matlab然后启动1sudo /usr/local/MATLAB/R2017a/bin/matlab然后会弹出激活框，然后导入matlab文件目录下选择license_standalone.lic这个文件，即可激活,如果出现liceses的权限不够,就执行下列命令1sudo chmod 777 /usr/local/MATLAB/R2017a/bin/licenses/现在我们来建立一个的是desktop文件，方便启动～～～1234567891011121314151617181.cd ~/.local/share/application2.sudo gedit matlab.desktop------然后粘贴下面字眼，无需更改，直接用[Desktop Entry]Type=ApplicationName=MatlabGenericName=Matlab 2017aComment=Matlab:The Language of Technical ComputingExec=sh /usr/local/MATLAB/R2017a/bin/matlab -desktopIcon=/usr/local/MATLAB/R2017a/toolbox/nnet/nnresource/icons/matlab.pngStartupNotify=trueTerminal=falseCategories=Development;Matlab;然后在Dash里找到Matlab，把他拖拽到启动栏上，拖拽后的样子请看第一幅图点击图标启动发现有报错然后我们根据报错信息提示说在家目录下的.matlab下的文件没有权限,于是12cd ~/.matlabchmod 777 R2017a","categories":[{"name":"机器学习","slug":"机器学习","permalink":"luoxiao.cf/categories/机器学习/"}],"tags":[{"name":"MatLab","slug":"MatLab","permalink":"luoxiao.cf/tags/MatLab/"}]},{"title":"数据结构二分查找","slug":"2018-09-19-数据结构二分查找","date":"2018-09-19T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/19/2018-09-19-数据结构二分查找/","link":"","permalink":"luoxiao.cf/2018/09/19/2018-09-19-数据结构二分查找/","excerpt":"","text":"对顺序查找法、二分查找法和分块查找做系统学习。用的书是严老师(严薇敏)的C语言版，使用scala重新编写了一下。语法稍微不同思路是一样的。 二分查找实现与演算二分查找的一个前提是必须保证序列有序，假如一个无序的序列想要使用二分查找，实际上速度并不一定比顺序查找要快，因为在进行二分查找之前，需要经过排序，而排序往往消耗的时间页是巨大的。123456789101112131415def binary(arr : Array[Int],elem : Int) : Int = &#123; var low = 1; var hight = arr.length; while (low &lt;= hight)&#123; var mid = ( low + hight ) / 2 if(arr(mid) == elem)&#123; return mid &#125;else if(arr(mid) &gt; elem)&#123; hight = mid - 1 &#125;else&#123; low = mid - 1 &#125; &#125; return 0&#125;这个算法需要注意的是，循环执行的条件是low&lt;=hight,而不是low&lt;hight,因为在low&lt;hight时，查找区间还有最后一个结点，还要进行一步比较。算法演算过程如下图:假如key=21可以看到第一次先和56比，发现key&lt;57，接着和19比,发现key&gt;19以此类推最终找到21。 时间复杂度计算其实我们可以把查找的过程映射成决策树，事情就会变得更加有趣。首先我们需要知道什么是满二叉数,像下图中的(a)我们遵循一个规律树的左子树都比根节点小，树的右子树都比根大。遵循这个策略有序序列映射后就会变成下面的样子对照这树，我们可以发现，第一层的我们最多只用比一次就可以找到对应的值，第二层的节点想要找到得比两次，第三层的节点想要找到得比三次,第四层的节点想要找到就得比四次。因为我们遵循着左小右大的原则,假设每个节点找到的概率都想等，那么平均寻找长度我们就可以计算出来ALS=111(1+2∗2+3∗4+4∗4)=3ALS=\\frac{1}{11} \\left( 1+2*2+3*4+4*4 \\right) = 3ALS=​11​​1​​(1+2∗2+3∗4+4∗4)=3好了!11个节点的有序序列利用二分查找法的平均寻找长度我们算出来是3。那么思考一个问题：对于n个结点的有序序列，如何求出它的平均寻找长度？对于这个问题我们需要一些基础知识，满二叉数的结点数总数 $$\\left( s \\right) $$ 与深度$$\\left( h \\right) $$ 存在这这样的公式s=2h−1s=2^h-1s=2​h​​−1满二叉数的第n层的节点总数 $$\\left( n \\right) $$ 与深度$$\\left( h \\right) $$ 存在这这样的公式n=2h−1n=2^{h-1}n=2​h−1​​上面的公式还可以转换成这样的形式h=⌊log2n⌋+1h = \\lfloor\\log_2 n\\rfloor + 1h=⌊log​2​​n⌋+1对于n个节点的有序序列，映射到满二叉树上以后，第一层的我们最多只用比一次就可以找到对应的值，第二层得比两次,第三层得比三次…第n层就得比$$h\\cdot2^{h-1}$$次(提示:等比数列通向式)，假设每个节点找到的概率都想等，则平均寻找长度为ALS=1n(1+2∗2+3∗3+⋯+(h−1)∗2h−2+h∗2h−1)ALS=\\frac{1}{n}\\left(1+2*2+3*3+ \\cdots + \\left( h-1 \\right) * 2^{h-2} + h * 2^{h-1} \\right)ALS=​n​​1​​(1+2∗2+3∗3+⋯+(h−1)∗2​h−2​​+h∗2​h−1​​)继续推导ALS=1n∑j=1hj⋅2j−1ALS=\\frac{1}{n} \\sum_{j=1}^h\\ j\\cdot2^{j-1}ALS=​n​​1​​​j=1​∑​h​​ j⋅2​j−1​​继续推导ALS=n+1nlog2(n+1)−1ALS=\\frac{n+1}{n} \\log_2 \\left(n+1\\right) - 1ALS=​n​​n+1​​log​2​​(n+1)−1当n较大时，有下面近似值ALS=log2(n+1)−1ALS=\\log_2 \\left(n+1\\right) - 1ALS=log​2​​(n+1)−1则二分查找的时间复杂度为$$\\omicron \\left( \\log_2 n \\right)$$借助下面的图形可以看出二分查找的效率要比顺序查找的效率高的多，当n&gt;0并趋向于无穷时，效果会越来越明显。分块查找见下一节","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"数据结构顺序查找","slug":"2018-09-18-数据结构顺序查找","date":"2018-09-18T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/18/2018-09-18-数据结构顺序查找/","link":"","permalink":"luoxiao.cf/2018/09/18/2018-09-18-数据结构顺序查找/","excerpt":"","text":"对顺序查找法、二分查找法和分块查找做系统学习。用的书是严老师(严薇敏)的C语言版，使用scala重新编写了一下。语法稍微不同思路是一样的。 顺序查找说起顺序查找，大家肯定都觉得再熟悉不过了，不过这里面还是有一些需要注意的,假如以前你一直写无哨兵的顺序查找，那你就out了,因为带哨兵的顺序查找能为你省掉将近一半的时间;下面是对两种实现方法以及原理进行说明。 无哨兵123456def search(arr : Array[Int] ,elem : Int):Int = &#123; var index = -1; for ((x,i) &lt;- arr.view.zipWithIndex) if(x == elem) index = i index&#125;思路很清晰，遍历所有数据一个一个比,如果发现匹配就返回对应下标。 有哨兵12345678910def searchSentry(arr : Array[Int] ,elem : Int,n : Int):Int = &#123; var i = 0 if(arr(n) != elem) arr(n) = elem else return n while(arr(i) != elem) i += 1 if(i &lt; n) return i else return -1&#125;带哨兵的乍一看貌似与无哨兵的没什么区别，然而实践证明，这个改进在length大于1000时，进行查询的时间会减少将近一半。原理是通过监视哨，省去了每次遍历都去检测是否查询完毕的时间。 时间复杂度计算平均查找长度的计算公式ASL=∑i=1nPiCiASL=\\sum_{i=1}^n\\ P_iC_iASL=​i=1​∑​n​​ P​i​​C​i​​从顺序表的查找过程可见，$$C_i$$取决于元素在表中的位置，假如查找第一个记录时，只需要查一次，第n个记录时则需要查n次。这边假设每个元素查找概率相等，则Pi=1nP_i=\\frac{1}{n}P​i​​=​n​​1​​继续推导ASL=1n∑i=1ni=1+n2ASL=\\frac{1}{n}\\sum_{i=1}^n\\ i=\\frac{1+n}{2}ASL=​n​​1​​​i=1​∑​n​​ i=​2​​1+n​​则顺序查找的平均算法复杂度为$$\\omicron \\left( n \\right)$$如果有疑惑可以看Data Structure (2nd Edition) 7.2.1http://book.knowsky.com/book_1030305.htm二分查找看下一篇","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"散列表冲突解决策略","slug":"2018-09-16-散列表冲突解决策略","date":"2018-09-16T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/16/2018-09-16-散列表冲突解决策略/","link":"","permalink":"luoxiao.cf/2018/09/16/2018-09-16-散列表冲突解决策略/","excerpt":"","text":"选择一个&quot;好&quot;的散列函数可以在一定程度上减少冲突，但在实际应用中，很难完全避免发生冲突，所以选择一个有效的处理冲突的方法是散列法的另一个关键问题。创建散列表和查找散列表都会遇到冲突。两种情况下处理冲突的方法应该一致。下面以创建散列表为例，来说明处理冲突的方法。处理冲突的方法可分为两类：开放地址法和链地址法 开放地址法核心原理:把记录都存在散列表数组中，当某一记录关键字key的初始散列地址$$H_0=H(key)$$发生冲突时，以$$H_0$$为基础，采取合适方法计算得到另一个地址$$H_1$$,如果$$H_1$$仍然发生冲，以$$H_1$$为基础在求一下地址$$H_2$$，若$$H_2$$仍然冲突，在求$$H_3$$以此类推。直到$$H_k$$不发生冲突为止，则$$H_k$$为该记录在表中的散列地址。这种方法在寻找“下一个”空的散列地址时，原来的数组空间对所有的元素都是开放的。所以称为开放地址法。通常把寻找“下一个”空位的过程叫做探测，上述方法可用如下公式表示：Hi=(H(key)+di)%mi=1,2,⋯,k(k≤m−1)H_i=(H(key)+d_i)\\%m\\quad i=1,2,\\cdots,k(k\\leq m - 1)H​i​​=(H(key)+d​i​​)%mi=1,2,⋯,k(k≤m−1)(1)线性探测法di=1,2,⋯,m−1d_i=1,2,\\cdots,m-1d​i​​=1,2,⋯,m−1这种探测方法可以将散列表假想成一个循环表，发生冲突时，从冲突地址的下一个单元顺序寻找空单元，如果到最后一个位置也没找到空单元，则回到表头开始继续查找。直到找到一个空位。就把此元素放在此空位。如果找不到空位，则说明散列表已满，需要进行溢出处理。(2)二次探测法di=12,−12,22,−22,32,⋯,+k2,−k2(k≤m2)d_i=1^2,-1^2,2^2,-2^2,3^2,\\cdots,+k^2,-k^2(k\\leq\\frac{m}{2})d​i​​=1​2​​,−1​2​​,2​2​​,−2​2​​,3​2​​,⋯,+k​2​​,−k​2​​(k≤​2​​m​​)结合着线性探测法来说，冲突时，先往右边找，如果依然冲突就从左边开始找。(3)伪随机探测法冲突时,$$加一个随机数再进行模运算例如，散列表的长度为11,散列函数$$H(key)=key%11$$,假设表中已填有关键字分别为17、60、29的记录。下图，现有四个记录，其关键字为38，由散列函数得到散列地址为5,产生冲突。若用线性探测法处理时，得到下一个地址6,仍然冲突，在求下一个地址7，还是冲突，知道散列地址为8的位置，完成探测若用二次探测发，散列地址5冲突后，得到下一个地址6,还是冲突，再求的下一个地址是4,完成探测若使用伪随机探测法，假设产生一的伪随机数为9,则计算下一个散列地址为$$(5+9)%11=3$$,由于脸很好，一次就完成探测。从上述线性探测法的处理过程中可以看到一个现象，当表中i,i+1,i+2位置上已填有记录时，下一个散列地址为i,i+1,i+2和i+3的记录都将填入i+3的位置，这种在处理冲突过程中发生的两个第一个散列地址不同的记录争夺同一个后继散列地址的现象称作二次聚集（或称为堆积），即在处理同义词的冲突过程中又添加非同义词冲突。可以看出上述的三种方法各有优缺点，线性探测法的优点是：只要散列表未填满，总能找到一个不发生冲突的地址。缺点是：会产生二次聚集现象。二次探测法的优点：可以避免二次聚集。缺点也很明显：不能保证一定能找到不发生冲突的地址。 链地址法核心原理：把具有相同散列地址的记录放在同一个单链表中，称为同义词链表。有m个散列地址就有m个单链表,同时用数组$$HT[0\\cdots m-1]$$存放各个链表的头指针，凡是散列表地址为i的记录都以结点方式插入到以$$HT[i]$$为头结点的单链表中。例如 一组关键字为(68,1,14,10,23,27,79,19,20,11,55),设散列函数为$$H(key)=key%13$$,用链表处理后的结果如下图所示：参考: Data Structure (2nd Edition)http://book.knowsky.com/book_1030305.htm","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"luoxiao.cf/categories/算法与数据结构/"}],"tags":[]},{"title":"R安装及语言矩阵运算","slug":"2018-09-16-R安装及语言矩阵运算","date":"2018-09-16T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/16/2018-09-16-R安装及语言矩阵运算/","link":"","permalink":"luoxiao.cf/2018/09/16/2018-09-16-R安装及语言矩阵运算/","excerpt":"","text":"Ubuntu16.04上安装R及RStudio,进行矩阵运算 安装R添加镜像源,将源添加到/etc/apt/sources.list，注意Ubuntu版本（本人Ubuntu版本是xenial，即16.04。根据版本改变对应的名字。）12# Ctrl+Alt+T打开终端sudo echo \"deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\" | sudo tee -a /etc/apt/sources.list下载公钥12gpg --keyserver keyserver.ubuntu.com --recv-key 51716619E084DAB9gpg -a --export 51716619E084DAB9 | sudo apt-key add -安装r-base12sudo apt-get updatesudo apt-get install r-base r-base-dev验证安装是否成功：打开终端，输入“R”，出现R的信息则安装成功。如： 安装RStudio直接从RStudio官网下载所需要的版本。（本人下载的RStudio 1.1.456 - Ubuntu 16.04+/Debian 9+ (64-bit)）第一次没翻墙下不了，翻墙后完美解决。123sudo apt-get install gdebi-coresudo gdebi rstudio-xenial-1.1.456-amd64.debrm rstudio-xenial-1.1.456-amd64.deb若安装成功，打开应用程序，会看到R和RStudio的图标。RStudio界面： 简单的矩阵运算对于三个矩阵A={123456},B={578621},C={5724}A=\\left\\{ \\begin{matrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ 5 &amp; 6 \\end{matrix} \\right\\} , B=\\left\\{ \\begin{matrix} 5 &amp; 7 &amp; 8 \\\\ 6 &amp; 2 &amp; 1 \\end{matrix} \\right\\} , C=\\left\\{ \\begin{matrix} 5 &amp; 7 \\\\ 2 &amp; 4 \\end{matrix} \\right\\}A=​⎩​⎨​⎧​​​1​3​5​​​2​4​6​​​⎭​⎬​⎫​​,B={​5​6​​​7​2​​​8​1​​},C={​5​2​​​7​4​​}求 BA和BA+C使用R进行计算12345&gt;a=matrix(c(1,2,3,4,5,6),ncol=2,byrow=T)&gt;b=matrix(c(5,7,8,6,2,1),ncol=3,byrow=T)&gt;c=matrix(c(5,7,2,4),ncol=2,byrow=T)&gt; b%*%a&gt; b%*%a+c","categories":[{"name":"机器学习","slug":"机器学习","permalink":"luoxiao.cf/categories/机器学习/"}],"tags":[{"name":"R","slug":"R","permalink":"luoxiao.cf/tags/R/"}]},{"title":"Redis 持久化策略","slug":"2018-09-15-Redis两种持久化方式","date":"2018-09-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/15/2018-09-15-Redis两种持久化方式/","link":"","permalink":"luoxiao.cf/2018/09/15/2018-09-15-Redis两种持久化方式/","excerpt":"","text":"redis虽然是一个缓存数据库，但是也提供了持久化数据的方法，主要有两种方式，AOF和RDB AOF 和 RDBAOF 记录server收到的写操作到日志文件，在server重启时通过回放这些写操作莱重建数据集。RDB 按照一定时间间隔对数据集创建基于时间点的快照 AOF 配置方法修改redis.config 配置文件,找到appendonly。默认是appendonly no,如果要启动需要改成appendonly yes把appendfsync的值改成everysec参数备注appendfsync always每次收到写命令就立即强制写入磁盘，最慢的。appendfsync everysec每秒中强制写入磁盘一次在性能和持久化方面做的最好appendfsync no性能最好，持久化没保证 RDB 配置方法默认情况下，redis保存数据集快照到磁盘，名字为dump.rdb的二进制文件，可以设置让redis N 秒之内至少 M次数据改动时进行持久化。或者可以手动调用save或者bgsave命令。例如，下面配置会让redis每60s内至少写入1000次时自动转存数据集到磁盘save 60 1000","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"luoxiao.cf/tags/Redis/"}]},{"title":"Redis集群部署","slug":"2018-09-15-Redis集群部署","date":"2018-09-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/15/2018-09-15-Redis集群部署/","link":"","permalink":"luoxiao.cf/2018/09/15/2018-09-15-Redis集群部署/","excerpt":"","text":"redis 是一个高性能的&lt;K,V&gt;分布式缓存数据库。参考博客http://blog.csdn.net/myrainblues/article/details/25881535 安装Redis3集群（3台Linux storm1,storm2,storm3）1.下载redis的稳定版本下载地址http://download.redis.io/releases/需要注意的是redis版本一定要在3.x以上，不然不支持集群2.上传redis到服务器1for n in &#123;1..5&#125;; do scp redis-4.0.11.tar.gz zyh@cor$n:/usr/local/src/; done3.解压12$ tar -zxvf redis-4.0.11.tar.gz -C ./需要注意的是在操作之前一定要保证权限正确4.进入到源码包中，编译并安装redis123cd /usr/local/src/redis-3.0.7/make &amp;&amp; make install进入/usr/local/bin 查看安装后的结果解释一下后续比较关键的几个命令名称作用备注redis-server启动redis服务redis-cli启动客户端redis-trib.rb启动redis-cluster需要ruby的支持才能用，文件存放在/redis/src/目录下make编译需要GCC的支持,redis-trib工具则需要ruby和gem的支持,下面说一下ruby和gem的安装5.安装所有依赖安装gcc1yum -y install gcc安装ruby和gem1yum -y install ruby rubygems使用gem下载redis集群的配置脚本1gem install redisgem install的安装包会放在/usr/lib/ruby/gems/1.8/cache目录下如果遇到redis requires Ruby version &gt;= 2.2.2的问题解决方案:https://www.jianshu.com/p/0a5de7dc0254原因是Ruby的版本太低，只要升级Ruby的版本即可。这边移除ruby 2.0.0版本，重新安装 2.3.3版本即可解决问题如果redis所在服务器不能上网怎么办?比如想要将下载好的redis-3.2.2.gem拷贝到其他服务器，手动安装即可shell cd /usr/lib/ruby/gems/1.8/cache for n in {2..6}; do scp redis-3.2.2.gem 192.168.0.3$n:$PWD; done在需要安装的机器上执行下面命令安装1gem install --local /usr/lib/ruby/gems/1.8/cache/redis-3.2.2.gem7.部署storm2和storm3用同样的方式在其他的机器上编译安装redis8.修改所有机器的配置文件redis.conf1234567port 6379 #端口pidfile /var/run/redis-6379.pid daemonize yescluster-enabled yesappendonly yesappendfsync no #AOF 策略，参考上篇博客bind $&#123;host&#125;9.启动所有机器上的redis节点1redis-server /etc/redis.conf10.使用redis-trib配置redis集群12cd /usr/local/src/redis-4.0.11/src/sudo cp /usr/local/src/redis-4.0.11/src/redis-trib.rb /usr/local/bin/检查防火墙，如果开启需要关闭1service iptables status使用redis-trib.rb集群构建工具启动集群1redis-trib.rb create --replicas 0 192.168.1.19:6379 192.168.1.20:6379 192.168.1.21:6379遇到 Can I set the above configuration? (type ‘yes’ to accept):输入yes回车即可这边运行以后报了个错误ERR Slot 0 is already busy (Redis::CommandError)原因是因为之前我已经配置过一次，再次使用就会出现slot 0被占用的结果，用redis-cli 登录到每个节点执行 flushall 和 cluster reset 就可以了。博客:https://blog.csdn.net/xiaojin21cen/article/details/70445545启动成功后:11.测试在storm1节点上set一个值，在storm2上查看值,storm1的ip 192.168.1.19 ;storm2的 ip 192.168.1.2012redis-cli -h storm1 -p 6379set zyh zyh在storm2上查看12redis-cli -h storm2 -p 6379get zyh报了错误:(error) MOVED 166 192.168.1.19:6379解决办法:在链接客户端时带上-c参数,参考https://www.oschina.net/question/1259683_21339541redis-cli -c -h storm2 -p 6379","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"luoxiao.cf/tags/Redis/"}]},{"title":"ArrayList 源码刨析","slug":"2018-09-14-ArrayList源码刨析","date":"2018-09-14T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/14/2018-09-14-ArrayList源码刨析/","link":"","permalink":"luoxiao.cf/2018/09/14/2018-09-14-ArrayList源码刨析/","excerpt":"","text":"ArrayList是List的是一个实现，是一个顺序容器，添加的时候如果容器不够，会自动扩容，值得注意的是ArrayList是不同不得，如果想同步可以自己手动实现，也可以使用Vector。ArrayList底层使用的是数组。在源码中可以看到transient Object[] elementData;被transient修饰的变量会被禁止序列化 源码追踪 set()简明直意，将element 放到index位置set(),这里需要注意的一点是elementData[index] = element;赋值的是引用,将element的引用给了elemntData[index]123456789public E set(int index, E element) &#123; // 检查索引 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; // 这里赋值的是引用 return oldValue;&#125; get()获取index位置的元素12345public E get(int index) &#123; rangeCheck(index); return (E)elementData(index); &#125; add()添加的方法有两种，add(E e)和add(int index, E element)，两个方法底层都是往数组中添加元素123456789101112131415public boolean add(E e) &#123; ensureCapacityInternal(size + 1); elementData[size++] = e; return true;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125;在添加时如果空间不够会使用调用进行扩容， grow(minCapacity)12345678910private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 拓展空间为原来的1.5倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // elementData = Arrays.copyOf(elementData, newCapacity); // 拓展空间完成复制&#125; remove()1234567891011121314public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // 让GC 进行垃圾回收 return oldValue;&#125;","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Java","slug":"Java","permalink":"luoxiao.cf/tags/Java/"}]},{"title":"springboot 快速应用","slug":"2018-09-9-springboot 快速应用","date":"2018-09-09T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/09/2018-09-9-springboot 快速应用/","link":"","permalink":"luoxiao.cf/2018/09/09/2018-09-9-springboot 快速应用/","excerpt":"","text":"整合springboot Mybatis快速应用，整理出来方便后期快速查询使用。 额外功能PageHelper 分页插件mybatis generator 自动生成代码插件 步骤 1.创建一个springboot项目： 2.创建项目的文件结构以及jdk的版本 3.选择项目所需要的依赖然后点击finish 5.看一下文件的结构： 6.查看一下pom.xml：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.winter&lt;/groupId&gt; &lt;artifactId&gt;springboot-mybatis-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot-mybatis-demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.7&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.35&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt; &lt;artifactId&gt;jackson-datatype-joda&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-parameter-names&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 分页插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- alibaba的druid数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- mybatis generator 自动生成代码插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;$&#123;basedir&#125;/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 7.项目不使用application.properties文件 而使用更加简洁的application.yml文件：将原有的resource文件夹下的application.properties文件删除，创建一个新的application.yml配置文件12345678910111213141516171819202122232425262728293031323334353637server: port: 8080spring: datasource: name: test url: jdbc:mysql://127.0.0.1:3306/depot username: root password: root # 使用druid数据源 type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.jdbc.Driver filters: stat maxActive: 20 initialSize: 1 maxWait: 60000 minIdle: 1 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: select 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true maxOpenPreparedStatements: 20## 该配置节点为独立的节点，有很多同学容易将这个配置放在spring的节点下，导致配置无法被识别mybatis: mapper-locations: classpath:mapping/*.xml #注意：一定要对应mapper映射xml文件的所在路径 type-aliases-package: com.winter.model # 注意：对应实体类的路径#pagehelper分页插件pagehelper: helperDialect: mysql reasonable: true supportMethodsArguments: true params: count=countSql 8.创建数据库：123456789CREATE DATABASE mytest;CREATE TABLE t_user( user_id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, user_name VARCHAR(255) NOT NULL , password VARCHAR(255) NOT NULL , phone VARCHAR(255) NOT NULL) ENGINE=INNODB AUTO_INCREMENT=1000 DEFAULT CHARSET=utf8; 9.使用mybatis generator 自动生成代码：配置pom.xml中generator 插件所对应的配置文件 ${basedir}/src/main/resources/generator/generatorConfig.xml123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;generatorConfiguration&gt; &lt;!-- 数据库驱动:选择你的本地硬盘上面的数据库驱动包--&gt; &lt;classPathEntry location=\"E:\\developer\\mybatis-generator-core-1.3.2\\lib\\mysql-connector-java-5.1.25-bin.jar\"/&gt; &lt;context id=\"DB2Tables\" targetRuntime=\"MyBatis3\"&gt; &lt;commentGenerator&gt; &lt;property name=\"suppressDate\" value=\"true\"/&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name=\"suppressAllComments\" value=\"true\"/&gt; &lt;/commentGenerator&gt; &lt;!--数据库链接URL，用户名、密码 --&gt; &lt;jdbcConnection driverClass=\"com.mysql.jdbc.Driver\" connectionURL=\"jdbc:mysql://127.0.0.1/mytest\" userId=\"root\" password=\"root\"&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver&gt; &lt;property name=\"forceBigDecimals\" value=\"false\"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成模型的包名和位置--&gt; &lt;javaModelGenerator targetPackage=\"com.winter.model\" targetProject=\"src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;property name=\"trimStrings\" value=\"true\"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成映射文件的包名和位置--&gt; &lt;sqlMapGenerator targetPackage=\"mapping\" targetProject=\"src/main/resources\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成DAO的包名和位置--&gt; &lt;javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.winter.mapper\" targetProject=\"src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 要生成的表 tableName是数据库中的表名或视图名 domainObjectName是实体类名--&gt; &lt;table tableName=\"t_user\" domainObjectName=\"User\" enableCountByExample=\"false\" enableUpdateByExample=\"false\" enableDeleteByExample=\"false\" enableSelectByExample=\"false\" selectByExampleQueryId=\"false\"&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt;点击run-Edit Configurations添加配置运行 ,同一张表一定不要运行多次，因为mapper的映射文件中会生成多次的代码，导致报错最后生成的文件以及结构： 10. 生成的文件UserMapper.java12345678910111213141516171819package com.winter.mapper;import com.winter.model.User;public interface UserMapper &#123; int deleteByPrimaryKey(Integer userId); int insert(User record); int insertSelective(User record); User selectByPrimaryKey(Integer userId); int updateByPrimaryKeySelective(User record); int updateByPrimaryKey(User record); //这个方式我自己加的 List&lt;User&gt; selectAllUser();&#125;User.java12345678910111213141516171819202122232425262728293031323334353637383940414243package com.winter.model;public class User &#123; private Integer userId; private String userName; private String password; private String phone; public Integer getUserId() &#123; return userId; &#125; public void setUserId(Integer userId) &#123; this.userId = userId; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName == null ? null : userName.trim(); &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password == null ? null : password.trim(); &#125; public String getPhone() &#123; return phone; &#125; public void setPhone(String phone) &#123; this.phone = phone == null ? null : phone.trim(); &#125;&#125;UserMapper.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.winter.mapper.UserMapper\" &gt; &lt;resultMap id=\"BaseResultMap\" type=\"com.winter.model.User\" &gt; &lt;id column=\"user_id\" property=\"userId\" jdbcType=\"INTEGER\" /&gt; &lt;result column=\"user_name\" property=\"userName\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"password\" property=\"password\" jdbcType=\"VARCHAR\" /&gt; &lt;result column=\"phone\" property=\"phone\" jdbcType=\"VARCHAR\" /&gt; &lt;/resultMap&gt; &lt;sql id=\"Base_Column_List\" &gt; user_id, user_name, password, phone &lt;/sql&gt; &lt;select id=\"selectByPrimaryKey\" resultMap=\"BaseResultMap\" parameterType=\"java.lang.Integer\" &gt; select &lt;include refid=\"Base_Column_List\" /&gt; from t_user where user_id = #&#123;userId,jdbcType=INTEGER&#125; &lt;/select&gt; &lt;!-- 这个方法是我自己加的 --&gt; &lt;select id=\"selectAllUser\" resultMap=\"BaseResultMap\"&gt; select &lt;include refid=\"Base_Column_List\" /&gt; from t_user &lt;/select&gt; &lt;delete id=\"deleteByPrimaryKey\" parameterType=\"java.lang.Integer\" &gt; delete from t_user where user_id = #&#123;userId,jdbcType=INTEGER&#125; &lt;/delete&gt; &lt;insert id=\"insert\" parameterType=\"com.winter.model.User\" &gt; insert into t_user (user_id, user_name, password, phone) values (#&#123;userId,jdbcType=INTEGER&#125;, #&#123;userName,jdbcType=VARCHAR&#125;, #&#123;password,jdbcType=VARCHAR&#125;, #&#123;phone,jdbcType=VARCHAR&#125;) &lt;/insert&gt; &lt;insert id=\"insertSelective\" parameterType=\"com.winter.model.User\" &gt; insert into t_user &lt;trim prefix=\"(\" suffix=\")\" suffixOverrides=\",\" &gt; &lt;if test=\"userId != null\" &gt; user_id, &lt;/if&gt; &lt;if test=\"userName != null\" &gt; user_name, &lt;/if&gt; &lt;if test=\"password != null\" &gt; password, &lt;/if&gt; &lt;if test=\"phone != null\" &gt; phone, &lt;/if&gt; &lt;/trim&gt; &lt;trim prefix=\"values (\" suffix=\")\" suffixOverrides=\",\" &gt; &lt;if test=\"userId != null\" &gt; #&#123;userId,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test=\"userName != null\" &gt; #&#123;userName,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test=\"password != null\" &gt; #&#123;password,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test=\"phone != null\" &gt; #&#123;phone,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/trim&gt; &lt;/insert&gt; &lt;update id=\"updateByPrimaryKeySelective\" parameterType=\"com.winter.model.User\" &gt; update t_user &lt;set &gt; &lt;if test=\"userName != null\" &gt; user_name = #&#123;userName,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test=\"password != null\" &gt; password = #&#123;password,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test=\"phone != null\" &gt; phone = #&#123;phone,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;/set&gt; where user_id = #&#123;userId,jdbcType=INTEGER&#125; &lt;/update&gt; &lt;update id=\"updateByPrimaryKey\" parameterType=\"com.winter.model.User\" &gt; update t_user set user_name = #&#123;userName,jdbcType=VARCHAR&#125;, password = #&#123;password,jdbcType=VARCHAR&#125;, phone = #&#123;phone,jdbcType=VARCHAR&#125; where user_id = #&#123;userId,jdbcType=INTEGER&#125; &lt;/update&gt;&lt;/mapper&gt; 11.打开类SpringbootMybatisDemoApplication.java，这个是springboot的启动类。我们需要添加点东西：123456789101112131415package com.winter;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication@MapperScan(\"com.winter.mapper\")//将项目中对应的mapper类的路径加进来就可以了public class SpringbootMybatisDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootMybatisDemoApplication.class, args); &#125;&#125;注意：一定要记得加@MapperScan 12.到这里所有的搭建工作都完成了，接下来就是测试的工作，没使用junit4进行测试：首先看一下完成之后的文件的结构编写UserController.java123456789101112131415161718192021222324252627282930313233package com.winter.Controller;import com.winter.model.User;import com.winter.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;/** * Created by Administrator on 2017/8/16. */@Controller@RequestMapping(value = \"/user\")public class UserController &#123; @Autowired private UserService userService; @ResponseBody @RequestMapping(value = \"/add\", produces = &#123;\"application/json;charset=UTF-8\"&#125;) public int addUser(User user)&#123; return userService.addUser(user); &#125; @ResponseBody @RequestMapping(value = \"/all/&#123;pageNum&#125;/&#123;pageSize&#125;\", produces = &#123;\"application/json;charset=UTF-8\"&#125;) public Object findAllUser(@PathVariable(\"pageNum\") int pageNum, @PathVariable(\"pageSize\") int pageSize)&#123; return userService.findAllUser(pageNum,pageSize); &#125;&#125;编写UserService.java123456789101112131415package com.winter.service;import com.winter.model.User;import java.util.List;/** * Created by Administrator on 2017/8/16. */public interface UserService &#123; int addUser(User user); List&lt;User&gt; findAllUser(int pageNum, int pageSize);&#125;编写UserServiceImpl.java123456789101112131415161718192021222324252627282930313233343536373839package com.winter.service.impl;import com.github.pagehelper.PageHelper;import com.winter.mapper.UserMapper;import com.winter.model.User;import com.winter.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;/** * Created by Administrator on 2017/8/16. */@Service(value = \"userService\")public class UserServiceImpl implements UserService &#123; @Autowired private UserMapper userMapper;//这里会报错，但是并不会影响 @Override public int addUser(User user) &#123; return userMapper.insertSelective(user); &#125; /* * 这个方法中用到了我们开头配置依赖的分页插件pagehelper * 很简单，只需要在service层传入参数，然后将参数传递给一个插件的一个静态方法即可； * pageNum 开始页数 * pageSize 每页显示的数据条数 * */ @Override public List&lt;User&gt; findAllUser(int pageNum, int pageSize) &#123; //将参数传给这个方法就可以实现物理分页了，非常简单。 PageHelper.startPage(pageNum, pageSize); return userMapper.selectAllUser(); &#125;&#125; 13.运行main方法使用postman测试添加用户查询接口不做演示访问localhost:8080/user/all/1/2至此springboot 整合mybatis就完成了。感谢博主的分享,传送门","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"luoxiao.cf/tags/Spring/"}]},{"title":"springboot集成redis","slug":"2018-09-9-springboot 集成redis","date":"2018-09-09T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/09/2018-09-9-springboot 集成redis/","link":"","permalink":"luoxiao.cf/2018/09/09/2018-09-9-springboot 集成redis/","excerpt":"","text":"整合redis到springboot 1.在pom中配置redis的相关依赖包：12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt;&lt;/dependency&gt; 2.application.yml12345678910spring: redis: host: 127.0.0.1 port: 6379 password: pass1234 pool: max-active: 100 max-idle: 10 max-wait: 100000 timeout: 0 3. 编写RedisConfig.java123456789101112131415161718192021222324252627282930@Configuration@EnableAutoConfigurationpublic class RedisConfig &#123; @Bean @ConfigurationProperties(prefix = \"spring.redis.pool\") public JedisPoolConfig getRedisConfig()&#123; JedisPoolConfig config = new JedisPoolConfig(); return config; &#125; @Bean @ConfigurationProperties(prefix = \"spring.redis\") public JedisConnectionFactory getConnectionFactory() &#123; JedisConnectionFactory factory = new JedisConnectionFactory(); factory.setUsePool(true); JedisPoolConfig config = getRedisConfig(); factory.setPoolConfig(config); return factory; &#125; @Bean public RedisTemplate&lt;?, ?&gt; getRedisTemplate() &#123; JedisConnectionFactory factory = getConnectionFactory(); RedisTemplate&lt;?, ?&gt; template = new StringRedisTemplate(factory); return template; &#125;&#125; 4.RedisService.java12345678910111213141516171819202122232425262728293031323334public interface RedisService &#123; /** * set存数据 * @param key * @param value * @return */ boolean set(String key, String value); /** * get获取数据 * @param key * @return */ String get(String key); /** * 设置有效天数 * @param key * @param expire * @return */ boolean expire(String key, long expire); /** * 移除数据 * @param key * @return */ boolean remove(String key);&#125; 5.RedisServiceImpl.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Service(\"redisService\")public class RedisServiceImpl implements RedisService &#123; @Resource private RedisTemplate&lt;String, ?&gt; redisTemplate; @Override public boolean set(final String key, final String value) &#123; boolean result = redisTemplate.execute(new RedisCallback&lt;Boolean&gt;() &#123; @Override public Boolean doInRedis(RedisConnection connection) throws DataAccessException &#123; RedisSerializer&lt;String&gt; serializer = redisTemplate.getStringSerializer(); connection.set(serializer.serialize(key), serializer.serialize(value)); return true; &#125; &#125;); return result; &#125; @Override public String get(final String key) &#123; String result = redisTemplate.execute(new RedisCallback&lt;String&gt;() &#123; @Override public String doInRedis(RedisConnection connection) throws DataAccessException &#123; RedisSerializer&lt;String&gt; serializer = redisTemplate.getStringSerializer(); byte[] value = connection.get(serializer.serialize(key)); return serializer.deserialize(value); &#125; &#125;); return result; &#125; @Override public boolean expire(final String key, long expire) &#123; return redisTemplate.expire(key, expire, TimeUnit.SECONDS); &#125; @Override public boolean remove(final String key) &#123; boolean result = redisTemplate.execute(new RedisCallback&lt;Boolean&gt;() &#123; @Override public Boolean doInRedis(RedisConnection connection) throws DataAccessException &#123; RedisSerializer&lt;String&gt; serializer = redisTemplate.getStringSerializer(); connection.del(key.getBytes()); return true; &#125; &#125;); return result; &#125;&#125;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"luoxiao.cf/tags/Spring/"}]},{"title":"influxdb使用及总结","slug":"2018-09-06-influxdb小结","date":"2018-09-06T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/06/2018-09-06-influxdb小结/","link":"","permalink":"luoxiao.cf/2018/09/06/2018-09-06-influxdb小结/","excerpt":"","text":"方便以后查询，做一个简单总结 增相关CLI终端下插入数据insert weather,altitude=1000,area=北 temperature=11,humidity=-4以http形式插入数据curl -i -XPOST 'http://localhost:8086/write?db=testDB' --data-binary 'weather,altitude=1000,area=北 temperature=11,humidity=-4' 查CLI终端下查询SELECT * FROM weather ORDER BY time DESC LIMIT 3以http形式查询curl -G 'http://localhost:8086/query?pretty=true' --data-urlencode &quot;db=testDB&quot; --data-urlencode &quot;q=SELECT * FROM weather ORDER BY time DESC LIMIT 3&quot; 库相关创建数据库CREATE DATABASE &quot;db_name&quot;显示所有数据库SHOW DATABASES删除数据库DROP DATABASE &quot;db_name&quot;USE mydb 表相关显示所有表SHOW MEASUREMENTS创建表并插入数据insert weather,altitude=1000,area=北 temperature=11,humidity=-4删除表DROP MEASUREMENT &quot;measurementName&quot; 保存策略相关Points : 指的是由Tags、Fields和timestamp组成的一行数据InfluxDB没有提供直接删除Points的方法，但是它提供了Retention Policies。主要用于指定数据的保留时间：当数据超过了指定的时间之后，就会被删除。显示testDB所有的保存策略SHOW RETENTION POLICIES ON &quot;testDB&quot;创建一个新的策略CREATE RETENTION POLICY &quot;rp_name&quot; ON &quot;db_name&quot; DURATION 30d REPLICATION 1 DEFAULT修改策略ALTER RETENTION POLICY &quot;rp_name&quot; ON db_name&quot; DURATION 3w DEFAULT删除策略DROP RETENTION POLICY &quot;rp_name&quot; ON &quot;db_name&quot; 连续查询当数据超过保存策略里指定的时间之后，就会被删除。如果我们不想完全删除掉，比如做一个数据统计采样：把原先每秒的数据，存为每小时的数据，让数据占用的空间大大减少（以降低精度为代价）。这就需要InfluxDB提供的：连续查询（Continuous Queries）。显示所有的continuous querySHOW CONTINUOUS QUERIES使用下面命令创建一个新的 continuous queries示例在testDB库中新建了一个名为 cq_30m 的连续查询，每三十分钟取一个temperature字段的平均值，加入 weather30m 表中。continuous queryCREATE CONTINUOUS QUERY cq_30m ON testDB BEGIN SELECT mean(temperature) INTO weather30m FROM weather GROUP BY time(30m)字段说明cq_30m连续查询的名字testDB具体的数据库名mean(temperature)算平均温度weather当前表名weather30m存新数据的表名30m时间间隔为30分钟删除DROP CONTINUOUS QUERY &lt;cq_name&gt; ON &lt;database_name&gt; 用户管理相关显示用户SHOW USERS创建用户CREATE USER &quot;username&quot; WITH PASSWORD 'password'创建管理员权限的用户CREATE USER &quot;username&quot; WITH PASSWORD 'password' WITH ALL PRIVILEGES删除用户DROP USER &quot;username&quot;感谢博主的分享，快速入门很不错传送门1传送门2官方文档传送门","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"InfluxDB","slug":"InfluxDB","permalink":"luoxiao.cf/tags/InfluxDB/"}]},{"title":"腾讯TCaptcha滑块验证码集成","slug":"2018-09-06-腾讯TCaptcha滑块验证码集成","date":"2018-09-06T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/09/06/2018-09-06-腾讯TCaptcha滑块验证码集成/","link":"","permalink":"luoxiao.cf/2018/09/06/2018-09-06-腾讯TCaptcha滑块验证码集成/","excerpt":"","text":"腾讯TCaptcha滑块验证码集成，主要是因为免费。参考文档https://007.qq.com/captcha/#/gettingStart 后台集成在web层登录接口中添加校验逻辑,这边关键点是verifyTicket方法是怎么做校验的1234567891011String ip = AddressHelper.getIpAddr(request);if(request.getMethod().toUpperCase().equals(\"POST\")) &#123; // 验证码是否有效 if (!tCaptchaUtil.verifyTicket(ticket, randstr, ip)) &#123; apiResultEntity = PushDataUtil.setData(ConstantDefine.ApiCallResult_Code_17002, ConstantDefine.ApiCallResult_Code_17262, null); modelAndView.setViewName(\"redirect:\" + loginPage); modelAndView.addObject(apiResultEntity); return modelAndView; &#125;&#125;编写tcaptcha.properties123# TCaptchatcaptcha.appId=xxxxxxxxtcaptcha.appSecret=xxxxxxxx编写TCaptchaUtil工具类在web初始化后将appid和appSecret注入到工具类中12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TCaptchaUtil &#123; private static final Logger logger = LogManager.getLogger(TCaptchaUtil.class); private String appId; private String appSecret; private OkHttpClient httpClient; private static final String VERIFY_URL = \"https://ssl.captcha.qq.com/ticket/verify?aid=%s&amp;AppSecretKey=%s&amp;Ticket=%s&amp;Randstr=%s&amp;UserIP=%s\"; public TCaptchaUtil(String appId, String appSecret) &#123; this.appId = appId; this.appSecret = appSecret; this.httpClient = new OkHttpClient(); &#125; // 前台会将ticket和rand传过来，后台根据这个值进行验证,验证通过返回true public boolean verifyTicket(String ticket, String rand, String userIp) &#123; // \"https://ssl.captcha.qq.com/ticket/verify?aid=%s&amp;AppSecretKey=%s&amp;Ticket=%s&amp;Randstr=%s&amp;UserIP=%s\"; HttpUrl url = new HttpUrl.Builder() .scheme(\"https\") .host(\"ssl.captcha.qq.com\") .addPathSegment(\"ticket\") .addPathSegment(\"verify\") .addQueryParameter(\"aid\", this.appId) .addQueryParameter(\"AppSecretKey\", this.appSecret) .addQueryParameter(\"Ticket\", ticket) .addQueryParameter(\"Randstr\", rand)// .addQueryParameter(\"UserIP\", \"116.231.187.102\") .addQueryParameter(\"UserIP\", userIp) .build(); Request request = new Request.Builder().url(url).build(); try &#123; Response response = httpClient.newCall(request).execute(); if (response.code() == 200) &#123; JSONObject jsonResp = JSONObject.parseObject(response.body().string()); if (jsonResp.getInteger(\"response\") == 1) &#123; return true; &#125; else &#123; logger.info(\"腾讯验证码验证失败：\" + response.body().string()); &#125; &#125; else &#123; logger.info(\"腾讯验证码验证失败：\" + response.body().string()); &#125; &#125; catch (IOException e) &#123; logger.catching(e); return false; &#125; return false; &#125;&#125;在application.xml中配置TCaptcha12345&lt;!-- TCaptcha 配置 --&gt;&lt;bean class=\"com.bim.bdip.cloud.home.util.TCaptchaUtil\"&gt; &lt;constructor-arg value=\"$&#123;tcaptcha.appId&#125;\"/&gt; &lt;constructor-arg value=\"$&#123;tcaptcha.appSecret&#125;\"/&gt;&lt;/bean&gt; 前台集成前台添加js &lt;script src=&quot;https://ssl.captcha.qq.com/TCaptcha.js&quot;&gt;&lt;/script&gt;用户记录临时变量数据&lt;input type=&quot;hidden&quot; name=&quot;ticket&quot; value=&quot;&quot;&gt;&lt;input type=&quot;hidden&quot; name=&quot;randstr&quot; value=&quot;&quot;&gt;添加登录按钮&lt;button id=&quot;TencentCaptcha&quot; data-appid=&quot;xxxxxx&quot; data-cbfn=&quot;login&quot; class=&quot;am-btn-secondary&quot; type=&quot;button&quot;&gt;登录&lt;/button&gt;当按下回车键是触发,并生成滑块验证码1234567891011$(document).keyup(function(event)&#123; if(user == \"true\")&#123; if(event.keyCode == 13)&#123; new TencentCaptcha( 'appid', loginCallback, &#123;&#125; ).show(); &#125; &#125;&#125;);当滑块验证码验证成功后会走回掉函数,并触发登录方法12345window.loginCallback = function(res) &#123; if(res.ret === 0) &#123; login(res.ticket, res.randstr); &#125;&#125;;下面是真正的登录方法12345678910111213141516171819function login(ticket, randstr)&#123; $(\"#login_form input[name='ticket']\").val(ticket); $(\"#login_form input[name='randstr']\").val(randstr); var identifier = $(\"#login_form input[name='identifier']\").val().trim(); var credential = $(\"#login_form input[name='credential']\").val().trim(); if(identifier=='')&#123; Dialog.alert(\"账号不能为空!\"); return; &#125; if(credential=='')&#123; Dialog.alert(\"密码不能为空!\"); return; &#125; var loginForm = $(\"#login_form\"); loginForm.attr(\"action\",\"$&#123;pageContext.request.contextPath &#125;/user/login\"); loginForm.submit();&#125;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[]},{"title":"基于Spark的网站浏览数据统计与分析","slug":"2018-09-3-基于Spark的网站浏览数据统计与分析","date":"2018-09-03T00:00:00.000Z","updated":"2019-11-13T04:56:08.676Z","comments":true,"path":"2018/09/03/2018-09-3-基于Spark的网站浏览数据统计与分析/","link":"","permalink":"luoxiao.cf/2018/09/03/2018-09-3-基于Spark的网站浏览数据统计与分析/","excerpt":"","text":"使用spark对网站的浏览情况进行统计分析，生成数据会输出到HDFS上。这边使用的数据源文件是nginx日志。tmp.logngnix的access.log的格式,摘抄部分日志123456789101112131415127.0.0.1 - - [05/Sep/2018:23:18:22 +0800] \"GET /4DAnalog/clashreport/delete HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:22 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/delete\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:40 +0800] \"GET /4DAnalog/clashreport/find HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:40 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/find\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:42 +0800] \"GET /4DAnalog/clashreport/find HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:42 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/find\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:43 +0800] \"GET /4DAnalog/clashreport/find HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:43 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/find\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:43 +0800] \"GET /4DAnalog/clashreport/find HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:44 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/find\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:52 +0800] \"GET /4DAnalog/clashreport/delete HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:53 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/clashreport/delete\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:59 +0800] \"GET /4DAnalog/chat/delete HTTP/1.1\" 502 575 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"127.0.0.1 - - [05/Sep/2018:23:18:59 +0800] \"GET /favicon.ico HTTP/1.1\" 502 575 \"http://localhost:8080/4DAnalog/chat/delete\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\" 前期准备需要提前准备好tmp.log上传到hdfs文件系统上hdfs dfs -put ~/tmp.log /urlcount/ 环境搭建及代码编写1.创建maven项目123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zonegood&lt;/groupId&gt; &lt;artifactId&gt;hellospark&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.10.6&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt; &lt;mainClass&gt;com.zomegood.hellospark.WordCount&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;如果没有src/main/scala目录，需要手动创建2.新建伴生对象com.zomegood.UrlCount.Main.scala1234567891011121314151617181920212223package com.zomegood.UrlCountimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * @author zyh * @create 18-9-5 下午11:32 * 统计URL访问次数 */object Main &#123; def main(args : Array[String]) : Unit = &#123; val conf = new SparkConf().setAppName(\"UrlCount\") val sc = new SparkContext(conf) // 先将nginx日志用空格符分割开，第7个位置的url，后续从新将url组合成新的Tuple(url,1) // Array((/4DAnalog/clashreport/delete,1), (/favicon.ico,1), (/4DAnalog/clashreport/find,1), (/favicon.ico,1), (/4DAnalog/clashreport/find,1), (/favicon.ico,1), (/4DAnalog/clashreport/find,1), (/favicon.ico,1), (/4DAnalog/clashreport/find,1)) var rdd1 = sc.textFile(args(0)).map(_.split(\" \")).map(arr =&gt; (arr(6),1)); // 根据Tuple 的每个key进行分组统计 rdd1.reduceByKey(_+_).saveAsTextFile(args(1)); sc.stop() &#125;&#125;3.使用maven打jar包运行mvn clean package 以集群方式运行bin/spark-submit --class com.zomegood.UrlCount.Main --master spark://cor1:7077 --executor-memory 512m --total-executor-cores 2 …/spark-mvn-1.0-SNAPSHOT.jar hdfs://cor1:9000/urlcount/tmp.log hdfs://cor1:9000/urlcount/out使用saveAsTextFile运行结果存到hdfs上","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"luoxiao.cf/tags/Spark/"}]},{"title":"spark集群搭建","slug":"2018-08-27-spark集群搭建","date":"2018-08-27T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/27/2018-08-27-spark集群搭建/","link":"","permalink":"luoxiao.cf/2018/08/27/2018-08-27-spark集群搭建/","excerpt":"","text":"这边使用VM创建3台cor1、cor2、cor4虚拟机，配置好网络、关闭防火墙之类的就不多详述、到spark官网下载程序包spark-1.6.1-bin-hadoop2.6需要注意的是虚拟机内存不能分配的太小，不然会导致启动失败。(这边也在考虑再拓展个8G内存条了，可惜现在的内存条真的是很贵。。)我打算在cor1上启动Master，在cor2和cor4上启动Worker。分配的内存分别为(1G,2G,2G) 概述开始之前这边在/目录下事先创建一个 /export/servers/目录，该目录用于存放大数据相关的安装包、比如hadoop、hive、storm等等，方便管理。同时创建一个专门的用户hadoop，在使用这方面东西的时候切换到该角色。不要忘记修改 /export/servers/ 目录的权限组 1.解压使用下面命令解压$ tar -zxvf spark-1.6.1-bin-hadoop2.6.tgz -C /export/servers/ 2.配置配置spark-env.sh文件,在尾部追加下面代码123export JAVA_HOME=/usr/local/jdk1.8.0_161/export SPARK_MASTER_IP=spark1export SPARK_MASTER_PORT=7077将worker所在机器的ip写在slaves文件中12cor2cor4 3.分发将配置好的spark分发到cor2、cor4机器上$ scp -r spark-1.6.1/ hadoop@cor2:/export/servers 4.运行$ /sbin/start-all.sh启动后可以使用 127.0.0.1:8080访问 来访问sparkweb端 5.spark-shellspark为编程人员专门提供了spark-shell，该功能可以使开发者以终端形式编写scala，进行代码调试，十分方便运行spark-shell$ bin/spark-shell --master spark://cor1:7077 --executor-memory 512m --total-executor-cores 2参数说明--master spark://cor1:7077指定Master地址--executor-memory 512m指定每个Worker可用内存为512m--total-executor-cores指定整个集群使用cpu核数 6.写个简单的wordcount启动hdfs，上传wordcount.txt到hdfs上$ sc.textFile(“hdfs://cor1:9000/wordcount.txt”).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(“hdfs://cor1:9000/out”)","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"luoxiao.cf/tags/Spark/"}]},{"title":"virtualenv搭建","slug":"2018-08-26-virtualenv搭建","date":"2018-08-26T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/26/2018-08-26-virtualenv搭建/","link":"","permalink":"luoxiao.cf/2018/08/26/2018-08-26-virtualenv搭建/","excerpt":"","text":"python 虚拟环境创建虚拟环境 mkvirtualenv test进入虚拟环境 workon test列出所有虚拟环境 workon离开虚拟环境 deactivate删除虚拟环境 rmvirtualenc test","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"luoxiao.cf/tags/Python3/"}]},{"title":"scala隐式转换应用","slug":"2018-08-26-scala隐式转换应用","date":"2018-08-26T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/26/2018-08-26-scala隐式转换应用/","link":"","permalink":"luoxiao.cf/2018/08/26/2018-08-26-scala隐式转换应用/","excerpt":"","text":"隐式转换是scala的一个难点，下面几个案例用来帮助理解它。案例一：想要增强某一个系统类的方法,比如为scala.reflect.io.File 添加一个read方法,实现类似下面的写法,12val file = File(filePath)print(file.read())具体实现1234567891011class RichFile(file : File) &#123; def read() : String = &#123; Source.formFile(file.path).mkString &#125;&#125;// 隐式转换只能写在伴生对象中object RichFile&#123; implicit def file2RichFile(file : File) : RichFile = new RichFile(file)&#125;在另一个类的main方法中调用12345def main(args : Array[String]) : Unit = &#123; import RichFile._ println(File(\"/home/zyh/1.txt\").read()) &#125;案例二: 比较Person中年龄的大小 ,这里不局限Person，可以用泛型T编写自定义的Person类1234class Person (var name:String , var age : Int)&#123; &#125;我们定义一个比较的方法，如果我们写成这样，编译器是会报错的，因为编译器不知道T是什么，所有在使用&gt;的时候就会报错12def compare[T](a:T,b:T) = if(a&gt;b) a else b我们要想下面这样去写12345678910111213141516object ComparePerson&#123; // #1 比较大小 def compare[T] (a:T,b:T) (implicit order : T =&gt; Ordered[T])= if(a&gt;b) a else b // #2 只定义#1.是不能比较的，因为a,b 系统不知道是什么类型，所以 &gt; 不能使用,这边解决的方法是利用隐式转换告诉系统 implicit val person2Ordered = (person : Person) =&gt; new Ordered[Person] &#123; override def compare(that : Person) = &#123; that.age - person.age &#125; &#125; // #3 还可以用下面的代码替换2,任选其一 implicit def person2Ordered(person : Person) = new Ordered[Person]&#123; override def compare..... &#125;&#125;在主程序中调用ComparePerson.compare(p1,p2)即可,使用之前一定要引入隐式转换的方法，import ComparePerson._","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"luoxiao.cf/tags/Scala/"}]},{"title":"minecraft游戏插件开发","slug":"2018-08-20-minecraft游戏插件开发","date":"2018-08-20T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/20/2018-08-20-minecraft游戏插件开发/","link":"","permalink":"luoxiao.cf/2018/08/20/2018-08-20-minecraft游戏插件开发/","excerpt":"","text":"本机环境：linux下的java开发minecraft游戏插件，用maven编译打jar包。 概述正式开发之前需要准备上面提到的以外，还需要准备服务端下载链接和客户端游戏插件开发好以后，会使用maven打成jar包，放在 服务端的 plugins 目录下即可生效。 环境搭建大致步骤:1.创建maven项目2.修改pom.xml文件3.下载mincecraft服务器4.配置mincecraft服务器5.启动mincraft客户端6.编写第一个命令插件7.maven打包 1.创建maven项目 2.修改pom.xml文件这里用的是Bukkit的服务器，所以需要引入第三方仓库123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spigot-repo&lt;/id&gt; &lt;url&gt;https://hub.spigotmc.org/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;接着引入开发库12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;org.bukkit&lt;/groupId&gt; &lt;artifactId&gt;bukkit&lt;/artifactId&gt; &lt;version&gt;1.12.2-R0.1-SNAPSHOT&lt;/version&gt;&lt;!--change this value depending on the version or use LATEST--&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.spigotmc&lt;/groupId&gt; &lt;artifactId&gt;spigot-api&lt;/artifactId&gt; &lt;version&gt;1.12.2-R0.1-SNAPSHOT&lt;/version&gt;&lt;!--change this value depending on the version--&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;使用maven-assembly-plugin插件打jar包12345678910111213141516171819202122&lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;!--&lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;org.bukkit.Server&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt;--&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 3.下载mincecraft服务器craftbukkit-1.7.2-R0.4-20140316.221310-4.jarhttps://getbukkit.org/ 4.配置服务器1.编写run.sh脚本，启动服务器$ touch run.sh123#!/bin/bashecho \"start bukkit server\"java -Xms1024M -Xmx1024M -jar craftbukkit-1.7.2-R0.4-20140316.221310-4.jar修改run.sh为可执行文件$ chmod 777 run.sh运行run.sh脚本$ ./run.sh启动以后会发现当前目录会生成很多文件2.修改服务器参数server.properties1234567891011121314151617181920212223242526272829303132333435#Minecraft server properties#Thu Aug 16 00:01:18 CST 2018generator-settings=op-permission-level=4allow-nether=trueonline-model=falselevel-name=worldenable-query=falseallow-flight=falseannounce-player-achievements=trueserver-port=25565level-type=DEFAULTenable-rcon=falseforce-gamemode=falselevel-seed=server-ip=max-build-height=256spawn-npcs=truewhite-list=falsespawn-animals=truehardcore=falsesnooper-enabled=trueonline-mode=falseresource-pack=pvp=truedifficulty=1enable-command-block=truegamemode=0player-idle-timeout=0max-players=20spawn-monsters=truegenerate-structures=trueview-distance=10spawn-protection=16motd=A Minecraft Server具体配置可以查看wiki重新启动服务器使其生效。 5.启动mincraft客户端想要启动minecraft客户端，需要提前准备一个启动器。minecraft启动器和minecraft客户端这篇博客https://www.linuxidc.com/Linux/2016-04/129764.htm有下载链接，不过里面的minecraft客户端启动会抛异常，可能是因为是linux的原因，在下载1.7.2的时候某些jar包损坏导致无法正常启动,后续通过排查找到损坏的jar，从新到网上下载完好的jar才得以解决。如果你用的也是linux，可以联系我，为你提供一个完好版本，当然也可以从网上搜一下，很好解决的。下载好启动器和客户端以后，我们先运行启动器，同服务端一样，编写run.sh 脚本运行minecraft启动器,run.sh内容如下:1234#!/bin/bashecho \"run minecraft client on linux os...\"java -jar HMCL-2.1.7.jar使用下面命令运行启动器$ ./run.sh进入游戏，选择Multiplayer开启多人游戏即可 6.编写第一个命令插件这个命令暂时叫做example，当用户在游戏内输入example时，作出相应提示当服务器启动以后，会加载放在plugin目录下实现JavaPlugin的类，触发它的onEnable方法在src/main/java下创建FirstPlugin.java1234567891011121314151617181920212223242526package com.zonegood;import org.bukkit.entity.Player;import org.bukkit.plugin.java.JavaPlugin;/** * @author zyh */public final class FirstPlugin extends JavaPlugin &#123; @Override public void onEnable() &#123; this.getLogger().info(\"FirstPlugin running...\"); // 加载CommandExample指令 this.getCommand(\"example\").setExecutor(new CommondExample()); &#125; @Override public void onDisable() &#123; for (Player player : getServer().getOnlinePlayers())&#123; this.getLogger().info(\"disable method\" + player.getName()); &#125; &#125;&#125;这边对指令类进行封装，指令需要实现CommandExecutor类，实现onCommand方法，当系统检测到有人发送example指令时，onCommand方法就会被触发，当然前提是，它必须已被注册。在src/main/java/目录下创建CommondExample.java1234567891011121314151617181920212223242526package com.zonegood;import org.bukkit.command.Command;import org.bukkit.command.CommandExecutor;import org.bukkit.command.CommandSender;import org.bukkit.entity.Player;/** * @author zyh */public class CommondExample implements CommandExecutor&#123; private static final String commandName = \"example\"; @Override public boolean onCommand(CommandSender commandSender, Command command, String s, String[] strings) &#123; if(commandName.equalsIgnoreCase(command.getName()))&#123; ((Player)commandSender).sendMessage(((Player)commandSender).getName() + \"execute example command\"); return true; &#125; return false; &#125;&#125;在src/resources/目录下创建plugin.yml,在里面我们需要对example指令进行描述aliases 可以理解为example指令的昵称，在游戏内输入a1,a2 等价于输入example除此之外里面还有其他参数，详情参考wiki1234567891011121314151617name: firstpluginversion: 1.0description: This plugin is so 31337. You can set yourself on fire.# We could place every author in the authors list, but chose not to for illustrative purposes# Also, having an author distinguishes that person as the project lead, and ensures their# name is displayed firstauthor: Zyhauthors: [zyh]website: http://www.zonegood.commain: com.zonegood.FirstPlugincommands: example: description: Set yourself on fire. aliases: [a1,a2] usage: Syntax error! Simply type /&amp;lt;command&amp;gt; to ignite yourself.项目的整体目录结构 7.maven打包下面的指令会将编写好的代码编译打包成jar，放在target目录下$ mvn clean package可以看到Building jar: /media/zyh/workspace/workspace/IDEA/bukiitplugin/target/bukiitplugin-1.0-SNAPSHOT.jar将bukiitplugin-1.0-SNAPSHOT.jar拷贝到server/plugins/目录下重新启动服务器即可 资料官方开发文档：https://bukkit.gamepedia.com/Main_PageDevelopers一栏就是了","categories":[{"name":"游戏开发","slug":"游戏开发","permalink":"luoxiao.cf/categories/游戏开发/"}],"tags":[{"name":"Minecraft","slug":"Minecraft","permalink":"luoxiao.cf/tags/Minecraft/"}]},{"title":"maven exclusion","slug":"2018-08-17-maven exclusion","date":"2018-08-17T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/17/2018-08-17-maven exclusion/","link":"","permalink":"luoxiao.cf/2018/08/17/2018-08-17-maven exclusion/","excerpt":"","text":"怎么使用dependency exclusions我们可以在pom.xml中的&lt;dependency&gt; 下添加&lt;exclusions&gt;,像这样1234567891011121314151617&lt;project&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;sample.ProjectA&lt;/groupId&gt; &lt;artifactId&gt;Project-A&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;!-- declare the exclusion here --&gt; &lt;groupId&gt;sample.ProjectB&lt;/groupId&gt; &lt;artifactId&gt;Project-B&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; dependency exclusion 工作原理以及什么时候使用123456Project-A -&gt; Project-B -&gt; Project-D &lt;! -- This dependency should be excluded --&gt; -&gt; Project-E -&gt; Project-F -&gt; Project C这个图展示了A依赖B,C B依赖D D依赖EF,默认的项目A的 classPath 将包含B,C,D,E,F场景：我们不希望项目D被依赖到项目A的classPath中，因为我们在开发时知道项目A中的功能根本不需要项目D，这个时候项目B的开发人员可以在自己的pom.xml中提供项目的依赖性&lt;optional&gt;true&lt;/optional&gt;,像这样:123456&lt;dependency&gt; &lt;groupId&gt;sample.ProjectD&lt;/groupId&gt; &lt;artifactId&gt;ProjectD&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;如果项目B没有这样做，作为最后手段，你可以在自己的项目中使用exclude，像这样123456789101112131415161718192021&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;sample.ProjectA&lt;/groupId&gt; &lt;artifactId&gt;Project-A&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;sample.ProjectB&lt;/groupId&gt; &lt;artifactId&gt;Project-B&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;sample.ProjectD&lt;/groupId&gt; &lt;!-- Exclude Project-D from Project-B --&gt; &lt;artifactId&gt;Project-D&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;这个时候假如我们将项目A安装到本地仓库，project-x依赖了project-A,那么project-D仍然可以在项目X中被排出在外","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"luoxiao.cf/tags/Maven/"}]},{"title":"Git命令汇总","slug":"2018-08-16-Git命令汇总","date":"2018-08-16T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/16/2018-08-16-Git命令汇总/","link":"","permalink":"luoxiao.cf/2018/08/16/2018-08-16-Git命令汇总/","excerpt":"","text":"为了方便查阅，在这边做一个小结 版本回退HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset --hard commit_id。穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本，类似的还有git log --pretty=online、git log --graph要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。 撤销修改场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，就回到了场景1，第二步按场景1操作。场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，执行git reset --hard commit_id，不过前提是没有推送到远程库。 删除文件删除本地rm file场景1：如果删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本 git checkout -- test.txt场景2：真正删除 git rm file删掉，并且git commit 添加远程库要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git关联后，使用命令git push -u origin master第一次推送master分支的所有内容 从远程库克隆git clone 创建与合并分支Git鼓励大量使用分支：查看分支：git branch创建分支：git branch &lt;name&gt;切换分支：git checkout &lt;name&gt;创建+切换分支：git checkout -b &lt;name&gt;合并某分支到当前分支：先切换到&lt;name&gt;分支,然后执行git merge &lt;name&gt;删除分支：git branch -d &lt;name&gt; 分支管理策略在实际开发中，我们应该按照几个基本原则进行分支管理：首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本；你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。所以，团队合作的分支看起来就像这样：Git在做merge时会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息，我们不想丢掉信息就需要这样做，git merge --no-ff -m &quot;merge with no-ff&quot; dev其中--no-ff参数，表示禁用Fast forward 暂存当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场。可以使用git stash list查看已暂存的内容 多人协作提交代码的流程查看远程库信息，使用git remote -v；本地新建的分支如果不推送到远程，对其他人就是不可见的；从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交；在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致；建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name；从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 rebase在多人开发模式下，往往很容易发生冲突，有些小伙伴往远端推送以后发现莫名其妙会多一个commit，导致不能是一条干净的直线,强迫症标识根本受不了。像下图一样，这时可以使用git rebase解决问题我们在pull时也可以加这样的参数 git pull --no-commit --rebase origin master来解决问题解决后的样子如果感到很疑惑这边推荐两个很不错的学习资料:git book、廖雪峰的官方网站 对比git diff commit-id-1 commit-id-2 --stat 忽略某个文件git update-index --assume-unchanged config.xml","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Git","slug":"Git","permalink":"luoxiao.cf/tags/Git/"}]},{"title":"常用的maven插件","slug":"2018-08-15-maven plugin","date":"2018-08-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/08/15/2018-08-15-maven plugin/","link":"","permalink":"luoxiao.cf/2018/08/15/2018-08-15-maven plugin/","excerpt":"","text":"maven-compiler-plugin编译Java源码，一般只需设置编译的jdk版本123456789&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt;&lt;/plugin&gt;或者在properties设置jdk版本12345&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; maven-dependency-plugin用于复制依赖的jar包到指定的文件夹里1234567891011121314151617&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;version&gt;2.10&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; maven-jar-plugin打成jar时，设定manifest的参数，比如指定运行的Main class，还有依赖的jar包，加入classpath中1234567891011121314&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;/data/lib&lt;/classpathPrefix&gt; &lt;mainClass&gt;com.zhang.spring.App&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt;&lt;/plugin&gt; wagon-maven-plugin用于一键部署，把本地打包的jar文件，上传到远程服务器上，并执行服务器上的shell命令12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;wagon-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;configuration&gt; &lt;serverId&gt;crawler&lt;/serverId&gt; &lt;fromDir&gt;target&lt;/fromDir&gt; &lt;includes&gt;*.jar,*.properties,*.sh&lt;/includes&gt; &lt;url&gt;sftp://59.110.162.178/home/zhangxianhe&lt;/url&gt; &lt;commands&gt; &lt;command&gt;chmod 755 /home/zhangxianhe/update.sh&lt;/command&gt; &lt;command&gt;/home/zhangxianhe/update.sh&lt;/command&gt; &lt;/commands&gt; &lt;displayCommandOutputs&gt;true&lt;/displayCommandOutputs&gt; &lt;/configuration&gt;&lt;/plugin&gt; tomcat7-maven-plugin用于远程部署Java Web项目12345678910&lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;url&gt;http://59.110.162.178:8080/manager/text&lt;/url&gt; &lt;username&gt;linjinbin&lt;/username&gt; &lt;password&gt;linjinbin&lt;/password&gt; &lt;/configuration&gt;&lt;/plugin&gt; maven-shade-plugin用于把多个jar包，打成1个jar包一般Java项目都会依赖其他第三方jar包，最终打包时，希望把其他jar包包含在一个jar包里12345678910111213141516171819202122232425&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt; &lt;manifestEntries&gt; &lt;Main-Class&gt;com.meiyou.topword.App&lt;/Main-Class&gt; &lt;X-Compile-Source-JDK&gt;$&#123;maven.compile.source&#125;&lt;/X-Compile-Source-JDK&gt; &lt;X-Compile-Target-JDK&gt;$&#123;maven.compile.target&#125;&lt;/X-Compile-Target-JDK&gt; &lt;/manifestEntries&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"luoxiao.cf/tags/Maven/"}]},{"title":"手写基于Akka的RPC通信框架","slug":"2018-08-15-基于Akka 的Rpc通信框架","date":"2018-08-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.675Z","comments":true,"path":"2018/08/15/2018-08-15-基于Akka 的Rpc通信框架/","link":"","permalink":"luoxiao.cf/2018/08/15/2018-08-15-基于Akka 的Rpc通信框架/","excerpt":"","text":"spark是一个并行分布式的计算框架，其设计阶段肯定要考虑到数据的交互。那么spark是怎么进行数据交互的？Spark是用Scala编写的，今天来看一下如何使用Scala设计一个Rpc通信服务 设计思路akka.actor.Actor 组件它于servlet有点类似，你可以把它想象成一个servlet，它同样也有自己的生命周期,preStart会在构造函数执行以后被调用，receive会在接收到消息以后被调用Actor中我们分出两类，一个叫Master，一个叫WorkerMaster 是头头Worker是小弟，就像Yarn里面的resourceManage和nodeManage、HDFS中的NameNode和dataNode一样,无规矩不成方圆，代码里面也是一样的。1.worker 启动后,在preStart方法中与master建立链接，向Master发送注册消息（将worker的信息通过样例类封装起来发送给master）2.Master接收到Worker的注册消息后将worker的信息保存起来，向worker反馈注册成功3.Worker定期要向master发送心跳(防止worker挂掉)4.如果worker长时间不回复，就把自己内存中的信息清除，防止将任务分配给死掉的worker 创建maven项目创建一个空的maven项目pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;properties&gt; ... &lt;scala.version&gt;2.10.6&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.10&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-remote_2.10&lt;/artifactId&gt; &lt;version&gt;2.3.14&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\"&gt; &lt;resource&gt;reference.conf&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt; &lt;mainClass&gt;com.zonegood.Master&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;如果没有src/main/scala 需要手动创建 编写Master123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.zonegoodimport akka.actor.&#123;Actor, ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryimport com.zonegood.MessageBox._import scala.collection.mutableimport scala.concurrent.duration._class Master extends Actor&#123; val hashMap = new mutable.HashMap[String,WorkerInfo]() val CHECK_INTERVAL = 15000 override def preStart(): Unit = &#123; println(\"master run...\") import context.dispatcher // 注册定时task 如果worker长时间不回复，就把自己内存中的信息清除，防止将任务分配给死掉的worker context.system.scheduler.schedule(0 millis,CHECK_INTERVAL millis, self,CheckTimeOutWorker) &#125; override def receive: Receive = &#123; case WorkerStartedMessage(workerId:String,workerInfo : WorkerInfo) =&gt; &#123; println(\"worker connect\") // 将worker信息 hashMap.put(workerId,workerInfo) // 向worker反馈注册成功 sender ! RegisterSuccessMessage &#125; case HeartBeatMessage(workerId:String) =&gt;&#123; // 刷新本地worker的状态 if(hashMap.contains(workerId))&#123; val worker = hashMap(workerId) worker.flashTime = System.currentTimeMillis() &#125; &#125; case CheckTimeOutWorker =&gt;&#123; // 拿到的是超时的消息，检测worker是否超时 val currentTime = System.currentTimeMillis() val remove = hashMap.filter(t =&gt; (currentTime - t._2.flashTime) &gt; CHECK_INTERVAL) for (e &lt;- remove)&#123; hashMap -= e._1 &#125; println(hashMap.size) &#125; case HeartBeatSendMessage(workerId) =&gt;&#123; // 更新 val workerInfo = hashMap(workerId) workerInfo.flashTime = System.currentTimeMillis() &#125; &#125;&#125;object Master&#123; def main(args : Array[String]) :Unit = &#123; val host = args(0) val port = args(1).toInt val configStr = s\"\"\" |akka.actor.provider = \"akka.remote.RemoteActorRefProvider\" |akka.remote.netty.tcp.hostname = \"$host\" |akka.remote.netty.tcp.port = \"$port\" \"\"\".stripMargin val conf = ConfigFactory.parseString(configStr) val masterSystem = ActorSystem(\"masterSystem\",conf) masterSystem.actorOf(Props[Master],\"masterActor\") masterSystem.awaitTermination() &#125;&#125; 编写Worker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.zonegoodimport java.util.UUIDimport akka.actor.&#123;Actor, ActorSelection, ActorSystem, Props&#125;import com.typesafe.config.ConfigFactoryimport com.zonegood.MessageBox.&#123;HeartBeatMessage, HeartBeatSendMessage, RegisterSuccessMessage, WorkerStartedMessage&#125;import scala.concurrent.duration._class Worker(id:String,masterHost:String,masterPort:Int) extends Actor with Serializable&#123; var master : ActorSelection = _ private[this] val HEART_BEAT_INTERVAL = 10000 val workerInfo = new WorkerInfo() override def preStart(): Unit = &#123; // connect master master = context.actorSelection( s\"akka.tcp://masterSystem@$masterHost:$masterPort/user/masterActor\") // register master ! new WorkerStartedMessage(this.id,workerInfo) &#125; override def receive: Receive = &#123; case RegisterSuccessMessage =&gt; &#123; // 注册监听器 import context.dispatcher context.system.scheduler.schedule(0 millis, HEART_BEAT_INTERVAL millis , self, HeartBeatMessage) &#125; case HeartBeatMessage =&gt; &#123; // resend message to master master ! new HeartBeatSendMessage(this.id) &#125; &#125;&#125;object Worker&#123; def main(args : Array[String]) : Unit = &#123; val host = args(0) val port = args(1).toInt val masterHost = args(2) val masterPort = args(3).toInt val configStr = s\"\"\" |akka.actor.provider = \"akka.remote.RemoteActorRefProvider\" |akka.remote.netty.tcp.hostname = \"$host\" |akka.remote.netty.tcp.port = \"$port\" \"\"\".stripMargin val conf = ConfigFactory.parseString(configStr) val workerSystem = ActorSystem(\"workerSystem\",conf) workerSystem.actorOf(Props(new Worker(UUID.randomUUID().toString,masterHost,masterPort)),\"workerActor\") workerSystem.awaitTermination() &#125;&#125; 样例类123456789package com.zonegoodobject MessageBox &#123; case class WorkerStartedMessage(workerId:String,workerInfo : WorkerInfo) case class RegisterSuccessMessage() case class CheckTimeOutWorker() case class HeartBeatMessage(workerId:String) case class HeartBeatSendMessage(workerId:String)&#125; 封装数据类123456package com.zonegoodclass WorkerInfo extends Serializable&#123; var flashTime = System.currentTimeMillis()&#125; 使用maven-shade-plugin插件打包1.打包Master执行 jar修改pom.xml123&lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt; &lt;mainClass&gt;com.zonegood.Master&lt;/mainClass&gt;&lt;/transformer&gt;运行 mvn clean packagemv target/my-scala-rpc-1.0-SNAPSHOT.jar ~/workspace/master.jar2.打包Worker执行 jar同理 打出worker.jar进入到~/workspace你就可以看到新的两个jar文件 运行$ cd ~/workspace$ java -jar master.jar 127.0.0.1 8888$ java -jar worker.jar 127.0.0.1 7001 127.0.0.1 8888$ java -jar worker.jar 127.0.0.1 7002 127.0.0.1 8888$ java -jar worker.jar 127.0.0.1 7003 127.0.0.1 8888查看运行结果关闭其中一个Worker查看console结果","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"luoxiao.cf/tags/Scala/"}]},{"title":"git -- 忽略某个文件","slug":"2018-08-08-gitignore 配置","date":"2018-08-08T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/08/08/2018-08-08-gitignore 配置/","link":"","permalink":"luoxiao.cf/2018/08/08/2018-08-08-gitignore 配置/","excerpt":"","text":"修改 .gitignore 文件在git中如果想忽略掉某个文件，不让这个文件提交到版本库中，可以使用修改 .gitignore 文件的方法。举例：.gitignore文件内容如下：1234567891011121314151617181920212223242526272829303132# Android generatedbin/gen/classes/gen-external-apklibs/# Antlocal.properties# Maventarget/release.properties# Eclipse.classpath.project.externalToolBuilders/.metadata.settings# IntelliJ*.iml*.ipr*.iws.idea/out/# Mac.DS_Store# gitignore.gitignore 使用命令.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。正确的做法是在每个clone下来的仓库中手动设置不要检查特定文件的更改情况。git update-index --assume-unchanged FILE在FILE处输入要忽略的文件。如果要还原的话，使用命令：git update-index --no-assume-unchanged FILE 使用.git/info/excludegit 还提供了另一种 exclude 的方式来做同样的事情，不同的是 .gitignore 这个文件本身会提交到版本库中去。用来保存的是公共的需要排除的文件。而 .git/info/exclude 这里设置的则是你自己本地需要排除的文件。 他不会影响到其他人。也不会提交到版本库中去。举例：1234567891011121314151617181920212223242526# git ls-files --others --exclude-from=.git/info/exclude# Lines that start with '#' are comments.# For a project mostly in C, the following would be a good set of# exclude patterns (uncomment them if you want to use them):# *.[oa]# *~.gradle/.idea/.settings/appcompat_v7/bin/build/gen/gradle/out/proguard/ship/target/.classpath.gitignore.idea.project.readme.update-config*.imllocal.properties.gitignore 还有个有意思的小功能， 一个空的 .gitignore 文件 可以当作是一个 placeholder 。当你需要为项目创建一个空的 log 目录时， 这就变的很有用。 你可以创建一个 log 目录 在里面放置一个空的 .gitignore 文件。这样当你 clone 这个 repo 的时候 git 会自动的创建好一个空的 log 目录了。","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Git","slug":"Git","permalink":"luoxiao.cf/tags/Git/"}]},{"title":"Raspberry Pi微型计算机的使用","slug":"2018-07-28-Raspberry Pi-Raspberry Pi微型计算机的使用","date":"2018-07-28T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/07/28/2018-07-28-Raspberry Pi-Raspberry Pi微型计算机的使用/","link":"","permalink":"luoxiao.cf/2018/07/28/2018-07-28-Raspberry Pi-Raspberry Pi微型计算机的使用/","excerpt":"","text":"简述Raspberry Pi(中文名为“树莓派”,简写为RPi，(或者RasPi / RPI)，只有信用卡大小的微型电脑，其系统基于Linux。 先今Raspberry Pi已经可以安装并运行Window操作系统。Raspberry Pi是一款基于ARM的微型电脑主板，以SD/MicroSD卡为内存硬盘，卡片主板周围有1/2/4个USB接口和一个10/100 以太网接口（A型没有网口），可连接键盘、鼠标和网线，同时拥有视频模拟信号的电视输出接口和HDMI高清视频输出接口，以上部件全部整合在一张仅比信用卡稍大的主板上，具备所有PC的基本功能只需接通电视机和键盘，就能执行如电子表格、文字处理、玩游戏、播放高清视频等诸多功能。 Raspberry Pi B款只提供电脑板，无内存、电源、键盘、机箱或连线。树莓派的生产是通过有生产许可的三家公司Element 14/Premier Farnell、RS Components及Egoman。这三家公司都在网上出售树莓派。现在，你可以在诸如京东、淘宝等国内网站购买到你所想要的树莓派。树莓派基金会提供了基于ARM的Debian和Arch Linux的发行版供大众下载。还计划提供支持Python作为主要编程语言，支持Java、BBC BASIC (通过 RISC OS 映像或者Linux的&quot;Brandy Basic&quot;克隆)、C 和Perl等编程语言.摘自百度百科 前期准备如果想玩Raspberry Pi,来先从软硬件两方面来列一下前期的准备。 硬件1.Raspberry Pi主板 (238 RMB)在买主板的时候，店家往往会给你推荐很多配件，比如电源、SD卡、散热片、外壳等等，不买也不影响正常使用，我购买的时候什么配件都没要，只要了一个主板关于电源的话，我用的是小米的手机充电器，这里需要注意一点说明书上标明电源要用5V-2A的电源，如果不是5V-2A的最好还是别用。当然如果是个不差钱的当我啥都没说。（土豪一起玩啊！！v）2.SD卡 (59 RMB)SD卡我这边是单独购买的，买的是闪迪32G的，听朋友说8G就已经够用了，如果你觉得太小可以买16G(40+ RMB)。32G 确实有点太大了。3.读卡器(6 RMB)读卡器是在SD卡write OS时用的。我这边从便利店买了一个便宜货6块钱。 软件软件需要准备三个，三个软件都是为了制作SD而准备的1.SDFormatter用这个更是化SD卡2.Win32DiskImager这个是往SD卡中写操作系统用的3.RASPBIAN OSRASPBIAN OS它是树梅派基金会专门依据debain操作系统改出来的一个新的定制化的操作系统。也是官方推荐安装的一款操作系统。这边油两个不同版本RASPBIAN STRETCH WITH DESKTOP : 会带着桌面一起安装RASPBIAN STRETCH LITE ： 只安装命令行，不安装桌面关于选择哪个，全在个人，想要桌面就安第一个。我使用的第第二个不带桌面的。 制作SD卡具体流程也很简单，这边大概说一下，用读卡器将SD卡与电脑连接起来，用SDFormatter格式化SD卡，启动Win32DiskImager，选择下好的镜像，点击write写入SD等待成功即可。 SSH这边没有网线，也没有显示器，那么要怎么ssh？具体操作步骤如下。需要注意的是， 1.方案一前提你需要准备一个linux操作系统。插上SD卡，如果你是linux操作系统，除了第一个boot盘符意外，你就能看到第二个盘符，叫&quot;rootfs&quot;$ sudo vim rootfs/etc/wap_supplicant/wpa_supplicant.conf在其中添加下段代码1234network=&#123;ssid=\"your wifi name\"psk=\"your wifi password\"&#125;进入到与rootfs并列的boot目录,在其中添加一个空的名字为ssh的文件$ touch boot/ssh拔出SD卡，将SD卡插入Raspberry Pi主板卡槽中，插上小米手机充电器，在事先登录好的路由器管理界面查看IP变化。新增的那个IP就是Raspberry Pi的ip了然后就可以愉快的链接ssh了。默认帐号:pi密码:raspberry$ ssh pi@192.168.100.109假如没有linux操作系统，我们就不能修改/etc/wpa_supplicant 目录下的 wpa_supplicant.conf文件，那么你可以尝试方案二 2.方案二插上SD卡,因为是window所以只能看到boot分区，在boot分区下新建 空的ssh文件再创建一个wpa_supplicant.conf将下段代码替换进去1234567ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network=&#123;ssid=\"your wifi name\"psk=\"your wifi password\"&#125;将内存卡拔出重新插入接上电源,操作系统会将boot下的wpa_supplicant.conf,替换到/etc/wpa_supplicant/目录下。最后同上找到ip，进行ssh就好了下面说一下我在探索这块时踩过的坑关于wpa_supplicant.conf里面的写法，网上有很多版本，开始参考了两篇文章，里面的配置我试了试都不行，折腾了一下午，其中的心酸一言难尽，尽管如此这边还是附上链接地址链接一，链接二经过踩坑以后，这边对配置文件进行了调整最后才成功。就是把里面的第一行country=xx 删掉.猜测的原因是，因为xx填写的并不正确导致无法正常链接wifi。这边干脆就把它删掉，因为删掉就是默认的了，事实证明删掉它以后wifi就可以连了，猜测是正确的。","categories":[{"name":"物联网","slug":"物联网","permalink":"luoxiao.cf/categories/物联网/"}],"tags":[{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"luoxiao.cf/tags/Raspberry-Pi/"}]},{"title":"sqlserver中or和and优先级问题","slug":"2018-07-27-sqlserver-sqlserver中or和and优先级问题","date":"2018-07-27T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/07/27/2018-07-27-sqlserver-sqlserver中or和and优先级问题/","link":"","permalink":"luoxiao.cf/2018/07/27/2018-07-27-sqlserver-sqlserver中or和and优先级问题/","excerpt":"","text":"今天发现写的接口有问题,排查到最后发现因为自己忽略了sqlserver中or和and的优先级，导致查询的数据不正确。关于他们的使用见下面的例子比如想要查询高三2班2000年出生或者是2002年出生的所有男生，那么sql应该这么写1select * from tb_class where sex = 'male' and (birth = '2000' or birth = '2002')还有另一种写法，可以用in代替or1select * from tb_class where sex = 'male' and birth in ( '2000' , '2002')","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[]},{"title":"NoClassDefFoundError和ClassNotFoundExceptio的区别","slug":"2018-06-29-Exception-NoClassDefFoundError和ClassNotFoundExceptio的区别","date":"2018-06-29T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/06/29/2018-06-29-Exception-NoClassDefFoundError和ClassNotFoundExceptio的区别/","link":"","permalink":"luoxiao.cf/2018/06/29/2018-06-29-Exception-NoClassDefFoundError和ClassNotFoundExceptio的区别/","excerpt":"","text":"问题描述小伙伴突然说新部署的企业级某功能不能用了，到线上查看，发现访问界面报 NoClassDefFoundError com.xxxx.xxxx.xxx.java 异常。 问题排查乍一看NoClassDefFoundError貌似是类没找到，你会发现tomcat中并没有缺少任何class。那么这个异常到底是什么意思。看下面java代码:12345678910111213141516171819202122public class Constants &#123; static String ABC = \"abc\"; static &#123; if (1 == 1) throw new RuntimeException(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; new Thread() &#123; public void run() &#123; String s2 = Constants.ABC; &#125; &#125;.start(); try &#123; Thread.sleep(1000); &#125; catch (Exception e) &#123; &#125; String s = Constants.ABC; &#125;&#125;在运行上面程序后，会报NoClassDefFoundError异常，原因是：在初始化Constants类时调用了static静态代码块抛了异常。所以再次使用Constants类时就报了这个错误了。 解决那么问题定位到了以后，到线上对com.xxxx.xxxx.xxx.class 进行反编译 发现有这样一段代码123456789101112static&#123; // ... String sql = \"select cversion from license\"; st = rs.executeSql(sql); while(st.next())&#123; if( StringUtil.isNullOrEmpty(st.getString(1)))&#123; throw new Exception(); &#125; // ... &#125; // ....&#125;到数据库查了一下license表,发现cversion 字段的值为null","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"Alipay对接","slug":"2018-07-24-alipay-Alipay支付宝集成","date":"2018-06-24T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/06/24/2018-07-24-alipay-Alipay支付宝集成/","link":"","permalink":"luoxiao.cf/2018/06/24/2018-07-24-alipay-Alipay支付宝集成/","excerpt":"","text":"在开始开发之前，需要通过向阿里申请获取相关信息，主要就是审核，审核通过后会得到相关参数随后与spring做集成,这边的思路是由spring容器去管理DefaultAlipayClient和AlipayPaymentAlipayPayment是我们自己写的接口DefaultAlipayClient 是阿里提供给我们的接口12345678910&lt;bean class=\"com.alipay.api.DefaultAlipayClient\" name=\"alipayClient\" autowire=\"byName\"&gt; &lt;constructor-arg value=\"$&#123;alipay.gateway&#125;\"/&gt; &lt;constructor-arg value=\"$&#123;alipay.appId&#125;\"/&gt; &lt;constructor-arg value=\"$&#123;alipay.appPrivateKey&#125;\"/&gt; &lt;constructor-arg value=\"json\"/&gt; &lt;constructor-arg value=\"utf-8\"/&gt; &lt;constructor-arg value=\"$&#123;alipay.appPublicKey&#125;\"/&gt; &lt;constructor-arg value=\"RSA2\"/&gt;&lt;/bean&gt;&lt;bean class=\"com.bim.bdip.cloud.home.payment.AlipayPayment\" name=\"alipayPayment\" autowire=\"byName\"/&gt; 购买界面入口12345678910111213141516171819// 购买兑换码@RequestMapping(method = RequestMethod.POST, value = \"/buy\", produces = \"text/html\")@ResponseBodypublic String buyRedeemCode(HttpServletRequest request, HttpServletResponse response) throws PaymentException, IOException &#123; HttpSession session = request.getSession(); BimUser bimUser = (BimUser) session.getAttribute(ConstantDefine.LOGIN_SESSION_USER); if (bimUser == null) &#123; String page = \"/page/bdip/baseModel/index\"; response.sendRedirect(page); &#125; long uId = bimUser.getId(); String redeemCodeLevel = request.getParameter(\"redeemCodeLevel\"); String paymentMethod = request.getParameter(\"paymentMethod\"); Object[] param = new Object[]&#123;redeemCodeLevel, uId, paymentMethod&#125;; Parameter parameter = new Parameter(BizServiceDefine.bimOrderService, \"buyRedeemCode\").setParam(param); parameter = bizProvider.execute(parameter); BimOrder order = parameter == null ? null : (BimOrder) parameter.getModel(); return alipayPayment.pay(order).getBody();&#125; 业务层1.锁定商品，禁止他人购买2.生成订单123456789101112131415161718192021222324252627282930@Overridepublic BimOrder buyRedeemCode(String redeemCodeLevel, Long uid, String paymentMethod) &#123; BimOrder order = new BimOrder(); BimRedeemLevel redeemLevel = new BimRedeemLevel(); redeemLevel.setLevel(redeemCodeLevel); redeemLevel = bimRedeemLevelMapper.selectOne(redeemLevel); Wrapper&lt;BimRedeemCode&gt; wrapper = new EntityWrapper&lt;&gt;(); wrapper = wrapper.where(\"redeem_level_id = &#123;0&#125;\", redeemLevel.getId()) .and(\"status = 0\") // 未售出 .and(\"redeem_code_type = 1\"); // 线上购买 List&lt;BimRedeemCode&gt; bimRedeemCodes = bimRedeemCodeMapper.selectList(wrapper); if (bimRedeemCodes.size() == 0) &#123; return null; &#125; String subject = String.format(\"购买 %.2f埃币 兑换码\", redeemLevel.getIcoinAmount()); BimRedeemCode redeemCode = bimRedeemCodes.get(0); redeemCode.setStatus(ConstantDefine.REDEEM_CODE_STATUS_LOCKED); bimRedeemCodeMapper.updateById(redeemCode); order.setAmount(redeemLevel.getAmount()); order.setPayExpireTime(new Date(new Date().getTime() + 1800000l)); // 半小时过期 order.setCurrencyType(ConstantDefine.CURRENCY_CNY); order.setCommodityTypeId(1l); order.setPaymentMethod(paymentMethod); order.setCreatedRole(0); // 用户 order.setUserId(uid); order.setProductId(redeemCode.getId()); order.setSubject(subject); return this.addBimOrder(order);&#125; alipayPayment 是为了对接做一个简单的封装。1.alipayClient2.returnUrl3.notifyUrl12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Servicepublic class AlipayPayment implements IPayment &#123; private Logger logger = LogManager.getLogger(this.getClass()); @Autowired private AlipayClient alipayClient; @Value(\"$&#123;alipay.returnUrl&#125;\") private String returnUrl; @Value(\"$&#123;alipay.notifyUrl&#125;\") private String notifyUrl; @Override public PaymentResult pay(BimOrder order) throws PaymentException &#123; AlipayTradePagePayRequest request = new AlipayTradePagePayRequest(); AlipayTradePagePayModel model = new AlipayTradePagePayModel(); request.setReturnUrl(returnUrl); request.setNotifyUrl(notifyUrl); JSONObject bizContent = new JSONObject(); bizContent.put(\"out_trade_no\", order.getOrderNumber()); bizContent.put(\"product_code\", \"FAST_INSTANT_TRADE_PAY\");// bizContent.put(\"total_amount\", order.getAmount()); bizContent.put(\"total_amount\", 0.01); // 设置消费金额 bizContent.put(\"subject\", order.getSubject()); bizContent.put(\"body\", order.getSubject()); request.setBizContent(bizContent.toJSONString()); PaymentResult paymentResult = new PaymentResult(); try &#123; AlipayResponse response = alipayClient.pageExecute(request); String body = response.getBody(); paymentResult.setResultType(PaymentResultType.BODY); paymentResult.setBody(body); &#125; catch (AlipayApiException e) &#123; logger.catching(e); throw new PaymentException(e); &#125; return paymentResult; &#125; @Override public boolean refund(BimOrder order) &#123; return false; &#125; @Override public boolean cancel(BimOrder order) &#123; return false; &#125;&#125;pay 接口负责将生成好的Order订单信息发送给阿里，然后阿里那边会返回一个结果1234567891011121314public class PaymentResult &#123; private String redirectUrl; private String body; private PaymentResultType resultType; private String orderNo; private Date expireTime; // ====== setter and getter =====&#125;public enum PaymentResultType &#123; REDIRECT, BODY&#125;当用户支付以后，就会走我们写的回调函数,returnUrl 和 notifyUrl俩参数实现已经在Common-config.properties写好alipay.returnUrl=http://cloud.bimbdip.com/callbacks/paymentReturnalipay.notifyUrl=http://cloud.bimbdip.com/callbacks/paymentNotify1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Controller@Api(value = \"回调\")@RequestMapping(\"/callbacks\")@PropertySources(value = &#123;@PropertySource(\"classpath:Common-config.properties\"), @PropertySource(\"classpath:user_space_size.properties\")&#125;)public class CallbacksController &#123; @Autowired private IBizProvider bizProvider; @Autowired private AlipayClient alipayClient; @Value(\"$&#123;alipay.publicKey&#125;\") private String alipayPublicKey; @ApiOperation(value = \"支付宝notify\") @RequestMapping(method = RequestMethod.POST, value = \"/alipayNotify\", produces = \"text/plain;charset=UTF-8\") public @ResponseBody String alipayNotify(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); for (String key : request.getParameterMap().keySet()) &#123; map.put(key, request.getParameter(key)); &#125; boolean signatureVerified = AlipaySignature.rsaCheckV1(map, alipayPublicKey, \"utf-8\", \"RSA2\"); if (signatureVerified) &#123; String orderNumber = request.getParameter(\"out_trade_no\"); String paymentOrderNumber = request.getParameter(\"trade_no\"); this.alipayOrderPaid(orderNumber, paymentOrderNumber); &#125; return \"success\"; &#125; /* * 支付宝Return */ @ApiOperation(value = \"支付宝return\") @RequestMapping(method = RequestMethod.GET, value = \"/alipayReturn\") public String alipayReturn(HttpServletRequest request, HttpServletResponse response) throws AlipayApiException &#123; Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); Map&lt;String, String[]&gt; requestParams = request.getParameterMap(); for (Iterator&lt;String&gt; iter = requestParams.keySet().iterator(); iter.hasNext(); ) &#123; String name = (String) iter.next(); String[] values = (String[]) requestParams.get(name); String valueStr = \"\"; for (int i = 0; i &lt; values.length; i++) &#123; valueStr = (i == values.length - 1) ? valueStr + values[i] : valueStr + values[i] + \",\"; &#125; //乱码解决，这段代码在出现乱码时使用// valueStr = new String(valueStr.getBytes(\"ISO-8859-1\"), \"utf-8\"); params.put(name, valueStr); &#125; boolean signatureVerified = AlipaySignature.rsaCheckV1(params, alipayPublicKey, \"utf-8\", \"RSA2\"); if (signatureVerified) &#123; String orderNumber = request.getParameter(\"out_trade_no\"); String paymentOrderNumber = request.getParameter(\"trade_no\"); this.alipayOrderPaid(orderNumber, paymentOrderNumber); &#125; else &#123; return \"forward:/page/bdip/user/index\"; &#125; return \"forward:/page/bdip/redeemcode/record\"; &#125; private void alipayOrderPaid(String orderNumber, String paymentOrderNumber) &#123; Object[] param = new Object[]&#123;orderNumber, \"ALIPAY\", paymentOrderNumber&#125;; Parameter parameter = new Parameter(BizServiceDefine.bimOrderService, \"orderPaid\").setParam(param); bizProvider.execute(parameter); &#125;&#125; 关于returnUrl和notifyUrl买家付款成功以后,支付宝那边会调用returnUrl接口，跳转到相应界面展示给用户看。只有当用户付款成功以后才有效notifyUrl，是用户后台通信存在的，当用户付款成功以后，支付宝会调用这个接口，我们要做的就是在这个接口中修改订单状态以及商品状态，成功以后返回一个&quot;success&quot;即可考虑到安全性问题代码中使用了RSA非对称加密协议对通信的信息进行加解密。(AlipaySignature.rsaCheckV1)至此基础开发应该就已经结束了。根据实景情况做具体调整","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[]},{"title":"hbase集群搭建","slug":"2018-06-21-hbase-hbase集群搭建","date":"2018-06-21T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/06/21/2018-06-21-hbase-hbase集群搭建/","link":"","permalink":"luoxiao.cf/2018/06/21/2018-06-21-hbase-hbase集群搭建/","excerpt":"","text":"本机环境Ubuntu 16.0.4 TLSVmware Workstation 14.1.1hadoop 2.7.3 （集群）zookeeper 3.4.10 (集群)前提已经在VM中部署好Hadoop 和Zookeeper集群。本次使用3台主机分别为cor1、cor2、cor3，在部署Hbase之前首先确保zookeeper和hadoop完好且可用。 下载apache官方下载,选取的版本是hbase-2.0.1-bin.tar.gz。关于hbase和hadoop版本的对应关系，这边给一个blog做参考https://blog.csdn.net/vtopqx/article/details/77882491，小伙伴也可以去看官方文档，会更详细一些。 安装解压到当前目录$ tar -zxvf hbase-2.0.1-bin.tar.gz -C ./进去conf目录编辑hbase-env.sh文件$ vim hbase-env.sh12345678910111213# the java implementation to use. 1.7+ requiredexport JAVA_HOME=/opt/jdk//jdk1.8.0_66# export JAVA_HOME=/usr/java/jdk1.8.0_66# 设置日志目录和PID目录export HBASE_LOG_DIR=/data/bigdata/logs/hbaseexport HBASE_PID_DIR=/data/bigdata/data/hbase# 使用外部zookeeperexport HBASE_MANAGES_ZK=false编辑hbase-site.xml,关于每个参数的详细描述可以在官方文档的第7章节7. Default Configuration中查到这边就不做详述,附上链接http://hbase.apache.org/book.html#config.files$ vi hbase-site.xml12345678910111213141516171819202122232425&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hbase-2.0.1/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://cor1:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cor1:2181,cor2:2181,cor3:2181&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/hadoop/zookeeper/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;配置conf/regionservers，写在该文件中的将被认为是从节点，在主节点上运行bin/start-hbase.sh以后，会自动启动从节点。 分发使用scp命令，将配置好的hbase分发给其他cor2和cor3节点$ scp -r hbase-2.0.1 hadoop@cor2:/home/hadoop$ scp -r hbase-2.0.1 hadoop@cor2:/home/hadoop 启动进入到hbase根目录执行下面命令就可以运行hbase,需要提前确保zookeeper和hadoop集群正常运行。$ bin/start-hbase.sh 测试与Hadoop一样，hbase同样为我们提供了好看的WEB UI界面masterregionServerhttp://cor1:16010http://cor1:16030我们也可以在本地CLI中使用命令进入hbase shell$ bin/hbase shell 案例1.查看有哪些表1hbase(main)&gt; list2.创建表12345# 语法：create &lt;table&gt;, &#123;NAME =&gt; &lt;family&gt;, VERSIONS =&gt; &lt;VERSIONS&gt;&#125;# 例如：创建表t1，有两个family name：f1，f2，且版本数均为2hbase(main)&gt; create 't1',&#123;NAME =&gt; 'f1', VERSIONS =&gt; 2&#125;,&#123;NAME =&gt; 'f2', VERSIONS =&gt; 2&#125;3.删除表12hbase(main)&gt; disable 't1'hbase(main)&gt; drop 't1'4.查看表的结构1234# 语法：describe &lt;table&gt;# 例如：查看表t1的结构hbase(main)&gt; describe 't1'5.修改表结构修改表结构必须先disable123456# 语法：alter 't1', &#123;NAME =&gt; 'f1'&#125;, &#123;NAME =&gt; 'f2', METHOD =&gt; 'delete'&#125;# 例如：修改表test1的cf的TTL为180天hbase(main)&gt; disable 'test1'hbase(main)&gt; alter 'test1',&#123;NAME=&gt;'body',TTL=&gt;'15552000'&#125;,&#123;NAME=&gt;'meta', TTL=&gt;'15552000'&#125;hbase(main)&gt; enable 'test1'6.添加数据1234# 语法：put &lt;table&gt;,&lt;rowkey&gt;,&lt;family:column&gt;,&lt;value&gt;,&lt;timestamp&gt;# 例如：给表t1的添加一行记录：rowkey是rowkey001，family name：f1，column name：col1，value：value01，timestamp：系统默认hbase(main)&gt; put 't1','rowkey001','f1:col1','value01'7.查询数据12345678910111213141516171819202122232425# 语法：get &lt;table&gt;,&lt;rowkey&gt;,[&lt;family:column&gt;,....]# 例如：查询表t1，rowkey001中的f1下的col1的值hbase(main)&gt; get 't1','rowkey001', 'f1:col1'# 或者：hbase(main)&gt; get 't1','rowkey001', &#123;COLUMN=&gt;'f1:col1'&#125;# 查询表t1，rowke002中的f1下的所有列值hbase(main)&gt; get 't1','rowkey001'# 语法：scan &lt;table&gt;, &#123;COLUMNS =&gt; [ &lt;family:column&gt;,.... ], LIMIT =&gt; num&#125;# 另外，还可以添加STARTROW、TIMERANGE和FITLER等高级功能# 例如：扫描表t1的前5条数据hbase(main)&gt; scan 't1',&#123;LIMIT=&gt;5&#125;# 语法：count &lt;table&gt;, &#123;INTERVAL =&gt; intervalNum, CACHE =&gt; cacheNum&#125;# INTERVAL设置多少行显示一次及对应的rowkey，默认1000；CACHE每次去取的缓存区大小，默认是10，调整该参数可提高查询速度# 例如，查询表t1中的行数，每100条显示一次，缓存区为500hbase(main)&gt; count 't1', &#123;INTERVAL =&gt; 100, CACHE =&gt; 500&#125;8.删除数据12345678910111213141516# 语法：delete &lt;table&gt;, &lt;rowkey&gt;, &lt;family:column&gt; , &lt;timestamp&gt;,必须指定列名# 例如：删除表t1，rowkey001中的f1:col1的数据hbase(main)&gt; delete 't1','rowkey001','f1:col1'# 注：将删除改行f1:col1列所有版本的数据# 语法：deleteall &lt;table&gt;, &lt;rowkey&gt;, &lt;family:column&gt; , &lt;timestamp&gt;，可以不指定列名，删除整行数据# 例如：删除表t1，rowk001的数据hbase(main)&gt; deleteall 't1','rowkey001'# 语法： truncate &lt;table&gt;# 其具体过程是：disable table -&gt; drop table -&gt; create table# 例如：删除表t1的所有数据hbase(main)&gt; truncate 't1'前些日子去日本旅游，blog停更几天。后面需要继续加油了！！","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"luoxiao.cf/tags/Hbase/"}]},{"title":"为什么使用scala","slug":"2018-05-22-scala-为什么使用scala","date":"2018-05-22T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/22/2018-05-22-scala-为什么使用scala/","link":"","permalink":"luoxiao.cf/2018/05/22/2018-05-22-scala-为什么使用scala/","excerpt":"","text":"近期Scala十分火，很大一部分是spark的原因;当然另一个很重要的原因，Scala本身确实也是一个十分不错的语言,Scala是一门多范式编程语言，以JVM为目标环境，将面向对象和函数式编程有机地结合在一起，带来独特的编程体验。虽然JDK 8以后也推出了lambda,但是Scala与其相比还是略胜一筹。Scala的作者Martin Odersky早期曾是JVM的核心代码提供者，所以他在编写Scala时也尽量弥补Java中存在的不足。Martin Odersky曾说“没有一门预言能像Scala这样,让我产生持续的兴趣和热情，让我重新感受到学习、思考和解决问题的乐趣”。正如你所看到的 Scala底层也是跑在JVM上的，所以它与Java集成度非常高，我们可以直接拿Scala调用已经写好的Java接口实现无缝对接。目前已经有很多公司和个人采用Scala来构建他们的平台和应用，作为JVM上第一个获得广泛成功的非Java语言，Scala正以它独特的魅力吸引着越来越多人的热情投入。Scala语言表达能力十分强，一行代码抵得上Java多行，开发速度快；Scala是静态编译的，所以和JRuby,Groovy比起来速度会快很多。可以看到上图中Sacala程序员工作状态略显轻松。 安装到Scala官网https://www.scala-lang.org/download/下载对应操作系统安装程序并安装,后续配置好环境变量 Helloword创建HelloScala.scala文件12345678object HelloScala&#123; def main(args:Array[String]) &#123; println(&quot;hello scala!&quot;); &#125;&#125;编译$ scalac HelloScala.scala运行$ scala HelloScala","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"luoxiao.cf/tags/Scala/"}]},{"title":"vue环境搭建","slug":"2018-05-12-vue-vue环境搭建","date":"2018-05-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/12/2018-05-12-vue-vue环境搭建/","link":"","permalink":"luoxiao.cf/2018/05/12/2018-05-12-vue-vue环境搭建/","excerpt":"","text":"目录安装vue脚手架初始化项目运行项目 安装vue脚手架sudo npm install -g vue-cli#初始化项目使用list可以查看已存在的模板。vue list用init命令执行默认模板是webpack来初始化项目sellvue init webpack sell 运行项目使用npm 构建项目(在init时指定的是yarn，这里就使用yarn install)，install会下载项目具备的依赖包npm install然后就可以运行项目了npm run dev访问localhost:8080","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"luoxiao.cf/tags/Vue/"}]},{"title":"try-catch影响性能？","slug":"2018-05-12-java-try-catch性能问题","date":"2018-05-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/12/2018-05-12-java-try-catch性能问题/","link":"","permalink":"luoxiao.cf/2018/05/12/2018-05-12-java-try-catch性能问题/","excerpt":"","text":"今天在审查代码时，发现自己的查询接口并没有try-catch。开始这样写是因为觉得写try-catch没必要，查询不可能会发生异常，但后来想了想又不太对，假如以后拓展接口，在查询后又用了查询后信息呢？就会抛空指针异常了！为了保证接口的健壮性，最好还是要加上try-catch。那么到底try-catch影不影响程序执行的速度？网上也有好多类似的博客，去解释这个问题。结果是：try-catch不影响性能，严格意义上说，如果不是百万级别的数据并发，try-catch对程序的影响是微乎其微的。有人做过例子，可以参考一下参考blog在stackoverflow上老外也帮着讲了一下try-catch的运行机制参考链接1参考链接2总结:如果在运行时，程序抛了异常就会去查异常表,这是影响性能的主要原因。至于在哪里写try-catch，其实只是影响了异常表里的两个变量而已（起始地址、结束地址），写在哪里跟應不影响性能是没有关系的。","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"forge开发环境Setup","slug":"2018-05-12-forge-forge开发环境Setup","date":"2018-05-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/12/2018-05-12-forge-forge开发环境Setup/","link":"","permalink":"luoxiao.cf/2018/05/12/2018-05-12-forge-forge开发环境Setup/","excerpt":"","text":"windows下搭建forge官网提供的案例链接,编译器使用vscode。 目录安装 node安装 git安装 MongoDB注册Autodesk Developer帐号配置项目启动项目 安装 node推荐使用nvm安装,github 下载并安装最新版的nvm。使用管理员权限打开命令提示符输入如下命令：123nvm install 8.11.1npm install -g --production windows-build-toolsnpm install -g cross-env配置npm以及yarn打开powershell或命令行输入如下命令，将npm使用淘宝的镜像进行加速并安装yarn：123npm config set registry=https://registry.npm.taobao.orgnpm install -g yarnyarn config set registry https://registry.npm.taobao.org 安装 git安装git命令行客户端或SourceTree等GUI客户端。命令行下输入$ git clone https://github.com/Autodesk-Forge/forge-rcdb.nodejs.gitGUI客户端用界面操作将https://github.com/Autodesk-Forge/forge-rcdb.nodejs.git clone到本地。 安装 MongoDB下载Robo 3t客户端 （ https://robomongo.org/download ）。连接本地mongo（默认不用认证直接连接）创建database: forge-rcdb给该database创建用户，并授予readWrite权限。将forge-rcdb项目中 resources\\db\\dev 目录下的数据导入mongodb。mongodb相关一些指令:1234sudo service mongod startmongo -uroot -pmongoimport -h localhost:27017 -d forge-rcdb /home/zyh/workspace/nodejs/forge-rcdb.nodejs/resources/db/devless /etc/mongod.conf 注册Autodesk Developer帐号访问 https://developer.autodesk.com/myapps/create 创建 App。Callback URL设为 http://localhost:3000/api/forge/callback/oauth 。记录下Client ID以及Client Secret 配置项目编辑forge-rcdb项目中的config\\development.config.js 将database部分的设置为刚安装的mongodb的信息。可选：将Client Id以及Client Secret替换掉Client Id以及Client Secret部分。在forge-rcdb项目目录下执行yarn install 启动项目执行如下命令可以启动项目：cross-env NODE_ENV=development HOT_RELOADING=true FORGE_DEV_CLIENT_SECRET=FORGE_DEV_CLIENT_ID=npm start若上一步中将Client Id以及Secret写入到配置文件，运行如下命令启动项目cross-env NODE_ENV=development HOT_RELOADING=true npm start","categories":[{"name":"Autodesk","slug":"Autodesk","permalink":"luoxiao.cf/categories/Autodesk/"}],"tags":[{"name":"Forge","slug":"Forge","permalink":"luoxiao.cf/tags/Forge/"}]},{"title":"数据样本的离散程度计算","slug":"2018-05-10-ubuntu-数据样本的离散程度计算","date":"2018-05-10T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/10/2018-05-10-ubuntu-数据样本的离散程度计算/","link":"","permalink":"luoxiao.cf/2018/05/10/2018-05-10-ubuntu-数据样本的离散程度计算/","excerpt":"","text":"统计数据样本的离散程度方法大致可分为三种：极差、方差、标准差，样本数据极差只能简单计算数据存在的区间分布范围情况，它在计算离散程度时存在着很大的问题。计算方式如下：极差 = 样本数据最大值 - 样本数据最小值方差计算公式如下：σ2=1N∑i=1N(xi−μ)2\\sigma^2=\\frac{1}{N}\\sum_{i=1}^N{\\left(x_i-\\mu\\right)^2}σ​2​​=​N​​1​​​i=1​∑​N​​(x​i​​−μ)​2​​如果觉得计算比较麻烦可以用推导后的公式计算方差相比极差而言，可以准确描述数据样本的离散程度,但是它也存在这问题。就是在描述数据样本的离散度时会丢失单位精度，这是因为在原样本的基础上进行了平方操作。假如原样本的每个数都有单位米(m),那么我们在求方差后，米就变成了平方米。为了解决这个问题，引入了标准差,计算公式只需要对方差开方就可以了。σ=σ22\\sigma=\\sqrt[2]{\\sigma^2}σ=​2​​√​σ​2​​​​​","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[]},{"title":"ubuntu16.04 Teamviewer不能远程控制","slug":"2018-05-08-ubuntu-ubuntu16.04不能远程控制.1","date":"2018-05-08T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/08/2018-05-08-ubuntu-ubuntu16.04不能远程控制.1/","link":"","permalink":"luoxiao.cf/2018/05/08/2018-05-08-ubuntu-ubuntu16.04不能远程控制.1/","excerpt":"","text":"问题描述通过windows 7旗舰版 可以连接ubuntu的TeamViewer，但是鼠标不能控制电脑。ubuntu的版本是16.04TLS,TeamViewer 的版本是 13 解决开始以为是Ubuntu中TeamViewer的权限没有开，但是后来看了看权限是开着的在网上搜索了好久，各种说法都有，试了好多都没用。最后通过在askubuntu搜索TeamViewer关键字，一页一页看看了10几页把问题解决了，还是国外网站靠谱啊。不能访问的原因是因为ubuntu本机某些依赖包没有装导致的。尝试运行下面指令:$ sudo apt-get install libjpeg62:i386 libxinerama1:i386 libxrandr2:i386 libxtst6:i386 ca-certificates可以看到本机运行以后又安装了两个包 libjpeg62:i386 和 libxtst6:i386，应该就是它俩的原因了。另外附上解决问题的帖子https://askubuntu.com/questions/764228/teamviewer-11-wont-run-on-ubuntu-16-04-64-bit","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"Azkaban任务调度系统部署","slug":"2018-05-05-hadoop-Azkaban部署","date":"2018-05-05T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/05/2018-05-05-hadoop-Azkaban部署/","link":"","permalink":"luoxiao.cf/2018/05/05/2018-05-05-hadoop-Azkaban部署/","excerpt":"","text":"概述 为什么需要工作流调度系统一个完整的数据分析系统通常都是由大量任务单元组成：shell脚本程序，java程序，mapreduce程序、hive脚本等各任务单元之间存在时间先后及前后依赖关系为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示：1、 通过Hadoop先将原始数据同步到HDFS上；2、 借助MapReduce计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张Hive表中；3、 需要对Hive中多个表的数据进行JOIN处理，得到一个明细数据Hive大表；4、 将明细数据进行复杂的统计分析，得到结果报表信息；5、 需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 工作流调度实现方式简单的任务调度：直接使用linux的crontab来定义；复杂的任务调度：开发调度平台或使用现成的开源调度系统，比如ooize、azkaban等 常见工作流调度系统市面上目前有许多工作流调度器在hadoop领域，常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等 各种调度工具特性对比下面的表格对上述四种hadoop工作流调度器的关键特性进行了比较，尽管这些工作流调度器能够解决的需求场景基本一致，但在设计理念，目标用户，应用场景等方面还是存在显著的区别，在做技术选型的时候，可以提供参考特性HamakeOozieAzkabanCascading工作流描述语言XMLXML (xPDL based)text file with key/value pairsJava API依赖机制data-drivenexplicitexplicitexplicit是否要web容器NoYesyesyes进度跟踪console/log messagesweb pageweb pageJava APIHadoop job调度支持NoYesyesyes运行模式command line utilitydaemondaemonAPIPig支持YESYesyesyes事件通知nononoyes需要安装NoYesyesno支持的hadoop版本0.18+0.20+currently unknown0.18+重试支持noworkflownode evelyesyes运行任意命令yesyesyesyesAmazon EMR支持yesnocurrently unknownyes Azkaban与Oozie对比对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。详情如下:功能两者均可以调度mapreduce,pig,java,脚本工作流任务两者均可以定时执行工作流任务工作流定义Azkaban使用Properties文件定义工作流Oozie使用XML文件定义工作流工作流传参Azkaban支持直接传参，例如${input}Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}定时执行Azkaban的定时执行任务是基于时间的Oozie的定时执行任务基于时间和输入数据资源管理Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作Oozie暂无严格的权限控制工作流执行Azkaban有两种运行模式，分别是soloserver mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)Oozie作为工作流服务器运行，支持多用户和多工作流工作流管理Azkaban支持浏览器以及ajax方式操作工作流Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流 Azkaban介绍Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。它有如下功能特点：Web用户界面方便上传工作流方便设置任务之间的关系调度工作流认证/授权(权限的工作)能够杀死并重新启动工作流模块化和可插拔的插件机制项目工作区工作流和任务的日志记录和审计 Azkaban安装部署 准备工作Azkaban Web服务器azkaban-web-server-2.5.0.tar.gzAzkaban执行服务器azkaban-executor-server-2.5.0.tar.gzMySQL目前azkaban只支持 mysql,需安装mysql服务器,本文档中默认已安装好mysql服务器,并建立了 root用户,密码 root.下载地址:http://azkaban.github.io/downloads.html 安装将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序 1.azkaban web服务器安装解压azkaban-web-server-2.5.0.tar.gz命令: tar –zxvf azkaban-web-server-2.5.0.tar.gz将解压后的azkaban-web-server-2.5.0 移动到 azkaban目录中,并重新命名 webserver123mv azkaban-web-server-2.5.0 ../azkabancd ../azkabanmv azkaban-web-server-2.5.0 server 2.azkaban 执行服器安装解压azkaban-executor-server-2.5.0.tar.gz命令:tar –zxvf azkaban-executor-server-2.5.0.tar.gz将解压后的azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor123mv azkaban-executor-server-2.5.0 ../azkabancd ../azkabanmv azkaban-executor-server-2.5.0 executorazkaban脚本导入解压: azkaban-sql-script-2.5.0.tar.gz命令:tar –zxvf azkaban-sql-script-2.5.0.tar.gz将解压后的mysql 脚本,导入到mysql中:进入mysql1234mysql&gt; create database azkaban;mysql&gt; use azkaban;Database changedmysql&gt; source/home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql; 创建SSL配置参考地址: http://docs.codehaus.org/display/JETTY/How+to+configure+SSL命令: keytool -keystore keystore -alias jetty -genkey -keyalg RSA运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下:输入keystore密码：再次输入新密码:您的名字与姓氏是什么？[Unknown]：您的组织单位名称是什么？[Unknown]：您的组织名称是什么？[Unknown]：您所在的城市或区域名称是什么？[Unknown]：您所在的州或省份名称是什么？[Unknown]：该单位的两字母国家代码是什么[Unknown]： CNCN=Unknown, OU=Unknown, O=Unknown,L=Unknown, ST=Unknown, C=CN 正确吗？[否]： y输入的主密码 （如果和 keystore 密码相同，按回车）： 再次输入新密码:完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.如:cp keystore azkaban/server 配置文件注：先配置好服务器节点上的时区1、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可2、拷贝该时区文件，覆盖系统本地时区配置cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeazkaban web服务器配置进入azkaban web服务器安装目录 conf目录v 修改azkaban.properties文件命令vi azkaban.properties 内容说明如下:#Azkaban Personalization Settingsazkaban.name=Test #服务器UI名称,用于服务器上方显示的名字azkaban.label=My Local Azkaban #描述azkaban.color=#FF3601 #UI颜色azkaban.default.servlet.path=/index #web.resource.dir=web/ #默认根web目录default.timezone.id=Asia/Shanghai #默认时区,已改为亚洲/上海 默认为美国#Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManager #用户权限管理默认类user.manager.xml.file=conf/azkaban-users.xml #用户配置,具体配置参加下文#Loader for projectsexecutor.global.properties=conf/global.properties # global配置文件所在位置azkaban.project.dir=projects #database.type=mysql #数据库类型mysql.port=3306 #端口号mysql.host=localhost #数据库连接IPmysql.database=azkaban #数据库实例名mysql.user=root #数据库用户名mysql.password=root #数据库密码mysql.numconnections=100 #最大连接数# Velocity dev modevelocity.dev.mode=false# Jetty服务器属性.jetty.maxThreads=25 #最大线程数jetty.ssl.port=8443 #Jetty SSL端口jetty.port=8081 #Jetty端口jetty.keystore=keystore #SSL文件名jetty.password=123456 #SSL文件密码jetty.keypassword=123456 #Jetty主密码 与 keystore文件相同jetty.truststore=keystore #SSL文件名jetty.trustpassword=123456 # SSL文件密码# 执行服务器属性executor.port=12321 #执行服务器端口# 邮件设置mail.sender=xxxxxxxx@163.com #发送邮箱mail.host=smtp.163.com #发送邮箱smtp地址mail.user=xxxxxxxx #发送邮件时显示的名称mail.password=********** #邮箱密码job.failure.email=xxxxxxxx@163.com #任务失败时发送邮件的地址job.success.email=xxxxxxxx@163.com #任务成功时发送邮件的地址lockdown.create.projects=false #cache.directory=cache #缓存目录v azkaban 执行服务器executor配置进入执行服务器安装目录conf,修改azkaban.propertiesvi azkaban.properties#Azkabandefault.timezone.id=Asia/Shanghai #时区# Azkaban JobTypes 插件配置azkaban.jobtype.plugin.dir=plugins/jobtypes #jobtype 插件所在位置#Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects#数据库设置database.type=mysql #数据库类型(目前只支持mysql)mysql.port=3306 #数据库端口号mysql.host=192.168.20.200 #数据库IP地址mysql.database=azkaban #数据库实例名mysql.user=root #数据库用户名mysql.password=root #数据库密码mysql.numconnections=100 #最大连接数# 执行服务器配置executor.maxThreads=50 #最大线程数executor.port=12321 #端口号(如修改,请与web服务中一致) executor.flow.threads=30 #线程数v 用户配置进入azkaban web服务器conf目录,修改azkaban-users.xmlvi azkaban-users.xml 增加 管理员用户 &lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&gt; &lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&gt; `&lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot; /&gt;` &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&gt; &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt; 启动 web服务器在azkaban web服务器目录下执行启动命令bin/azkaban-web-start.sh注:在web服务器根目录运行或者启动到后台nohup bin/azkaban-web-start.sh 1&gt;/tmp/azstd.out 2&gt;/tmp/azerr.out &amp; 执行服务器在执行服务器目录下执行启动命令bin/azkaban-executor-start.sh注:只能要执行服务器根目录运行启动完成后,在浏览器(建议使用谷歌浏览器)中输入https://服务器IP地址:8443 ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login. Azkaban实战Azkaba内置的任务类型支持command、java Command类型单一job示例1、创建job描述文件vi command.job123#command.jobtype=commandcommand=echo 'hello'2、将job资源文件打包成zip文件zip -r command.zip command.job3、通过azkaban的web管理平台创建project并上传job压缩包首先创建project上传zip包4、启动执行该job Command类型多job工作流flow1、创建有依赖关系的多个job描述第一个job：foo.job123# foo.jobtype=commandcommand=echo foo第二个job：bar.job依赖foo.job1234# bar.jobtype=commanddependencies=foocommand=echo bar2、将所有job资源文件打到一个zip包中3、在azkaban的web管理界面创建工程并上传zip包4、启动工作流flow HDFS操作任务1、创建job描述文件123#fs.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz2、将job资源文件打包成zip文件3、通过azkaban的web管理平台创建project并上传job压缩包4、启动执行该job MAPREDUCE任务Mr任务依然可以使用command的job类型来执行1、创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的examplejar）123# mrwc.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout2、将所有job资源文件打到一个zip包中3、在azkaban的web管理界面创建工程并上传zip包4、启动job HIVE脚本任务l 创建job描述文件和hive脚本Hive脚本： test.sql1234567891011121314151617use default;load data local inpath '/home/hadoop/demo.json' into table statis;insert into table statis_newselect get_json_object(line,'$.username') as username,get_json_object(line,'$.visitdate') as visitdate,get_json_object(line,'$.visit') as visitdate,get_json_object(line,'$.visit') as sumvisit from statis;insert overwrite table statis_newselect A.username,A.visitdate,max(A.visit) as visit,sum(B.visit) as sumvisitfrom (select username,visitdate,sum(visit) as visitfrom statis_newgroup by username,visitdate)Ainner join(select username,visitdate,sum(visit) as visitfrom statis_newgroup by username,visitdate)Bon A.username = B.usernamewhere B.visitdate &lt;= A.visitdategroup by A.username,A.visitdate;Job描述文件：hivef.job123# hivef.jobtype=commandcommand=/home/hadoop/apps/hive/bin/hive -f 'test.sql'2、将所有job资源文件打到一个zip包中3、在azkaban的web管理界面创建工程并上传zip包4、启动job感谢博主分享，很不错的资料很全面，转载一波https://blog.csdn.net/hblfyla/article/details/74384915","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"luoxiao.cf/tags/Azkaban/"}]},{"title":"flume日志收集系统部署","slug":"2018-05-04-hadoop-flume日志收集","date":"2018-05-04T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/04/2018-05-04-hadoop-flume日志收集/","link":"","permalink":"luoxiao.cf/2018/05/04/2018-05-04-hadoop-flume日志收集/","excerpt":"","text":"flume 是cloudera提供的一个高可靠、高可用、分布式的日志采集、聚合和传输的工具，flume最大的特点就是可以方便的定义各种sources(从哪收)和sinks(放在哪)，来适应我们不同的业务场景。 使用进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME 1. 从网络端口接收数据，下沉到logger在flume的conf目录下新建一个文件，将下面内容写进去1234567891011121314151617181920212223242526# Name the components on this agent#给那三个组件取个名字a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source#类型, 从网络端口接收数据,在本机启动, 所以localhost, type=spoolDir采集目录源,目录里有就采a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memory#下沉的时候是一批一批的, 下沉的时候是一个个eventChannel参数解释：#capacity：默认该通道中最大的可以存储的event数量#trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1启动命令： 告诉flum启动一个agent,指定配置参数, --name:agent的名字$ bin/flume-ng agent --conf conf --conf-file conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console 2. 监视文件夹123456789101112131415161718192021222324############### Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source#监听目录,spoolDir指定目录, fileHeader要不要给文件夹前坠名a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /home/hadoop/flumespoola1.sources.r1.fileHeader = true# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1启动命令：$ bin/flume-ng agent -c ./conf -f ./conf/spool-logger.conf -n a1 -Dflume.root.logger=INFO,console注:测试： 往/home/hadoop/flumeSpool放文件（mv ././xxxFile /home/hadoop/flumeSpool），但是不要在里面生成文件 3.用tail命令获取数据，下沉到hdfs123456789101112131415161718192021222324252627282930313233343536373839404142######### Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /home/hadoop/log/test.loga1.sources.r1.channels = c1# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/a1.sinks.k1.hdfs.filePrefix = events-# 每个10分钟重新生成一个新的时间目录a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minute# 文件的滚动周期(秒)a1.sinks.k1.hdfs.rollInterval = 3# 文件大小滚动(bytes)a1.sinks.k1.hdfs.rollSize = 20# 写入多少个event后滚动,事件个数a1.sinks.k1.hdfs.rollCount = 5a1.sinks.k1.hdfs.batchSize = 1a1.sinks.k1.hdfs.useLocalTimeStamp = true#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1启动命令：bin/flume-ng agent -c conf -f conf/tail-hdfs.conf -n a1 4. 多个agent串联agent1配置如下:1234567891011121314151617181920212223# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1启动:$ bin/flume-ng agent --conf conf --conf-file conf/avro-hdfs.conf --name a1 -Dflume.root.logger=DEBUG,consoleagent2配置如下:1234567891011121314151617181920212223242526272829################### Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /home/hadoop/log/test.loga1.sources.r1.channels = c1# Describe the sinka1.sinks = k1a1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = hadoop01a1.sinks.k1.port = 4141a1.sinks.k1.batch-size = 2# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1启动命令：$ bin/flume-ng agent --conf conf --conf-file conf/tail-avro-avro-logger.conf --name a1 -Dflume.root.logger=DEBUG,console","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"luoxiao.cf/tags/Flume/"}]},{"title":"MapReduce小例子","slug":"2018-05-04-hadoop-MapReduce小例子","date":"2018-05-04T00:00:00.000Z","updated":"2019-11-13T04:56:08.674Z","comments":true,"path":"2018/05/04/2018-05-04-hadoop-MapReduce小例子/","link":"","permalink":"luoxiao.cf/2018/05/04/2018-05-04-hadoop-MapReduce小例子/","excerpt":"","text":"案例一 : 统计单词出现个数a b a b aa b a b ab a b a b a b ab a ba,1 a,1 a,1 a,1 a,1 a,1 a,1 a,1 a,1 | b,1 b,1 b,1 b,1 b,1 b,1 b,1 b,1 b,1k -&gt; a values -&gt; 1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1sum = 0for (int i=0; values.length ; i++){sum ++;}案例二 : 统计手机上下行流量17654565484 80000000 600017654565484 80000000 600017654565484 80000000 600017654565484 80000000 600017654565484 80000000 600017654565484 80000000 600015565658787 54455455 546465415565658787 54455455 546465415565658787 54455455 546465415565658787 54455455 546465415565658787 54455455 546465417654565484,bean 17654565484,bean 17654565484,bean 17654565484,bean | 15565658787,bean 15565658787,bean 15565658787,bean 15565658787,beank -&gt; 17654565484 values -&gt; bean ,bean ,bean ,beansumUpStream = 0sumDownStream = 0for (int i=0; values.length ; i++){sumUpStream += bean.upStream();sumDownStream += bean.DOwnStream();}案例三 : 统计两个人的共同好友A : B,C,DB : E,F,DC : E,DB,A C,A D,A E,B F,B D,B E,C D,C -&gt; B,A | C,A | D,A D,B D,C | E,B E,C | F,BB : AC : AD : A ,B ,CE : B ,CF : BA ,B的好友是DA ,C的好友是D","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"luoxiao.cf/tags/MapReduce/"}]},{"title":"搭建基于Hadoop的Hive数据仓库","slug":"2018-05-03-hadoop-搭建基于Hadoop的Hive数据仓库","date":"2018-05-03T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/05/03/2018-05-03-hadoop-搭建基于Hadoop的Hive数据仓库/","link":"","permalink":"luoxiao.cf/2018/05/03/2018-05-03-hadoop-搭建基于Hadoop的Hive数据仓库/","excerpt":"","text":"Hive是基于Hadoop的一个数据仓库，我们可以将结构化的数据映射为一张数据库表，为此提供类似与SQL的HQL来查询数据。使用Hive可以提高我们的开发效率，缩短开发周期，最重要的是它降低了编写MapReduce编写的难度，可能在应对很复杂的mapReduce程序时,我们只需要写一条简单的sql就可以实现具体功能了。 安装我们可以从Hive的官网https://hive.apache.org/下载对应的安装包。这里我使用的版本是1.2.1。 1.修改配置文件$ cp hive-env.sh.template hive-env.sh在末尾加上下面语句，配置好环境变量1234export JAVA_HOME=/usr/local/jdk1.8.0_161export HADOOP_HOME=/home/hadoop/hadoop-2.7.3export HIVE_HOME=/home/hadoop/apache-hive-1.2.1-binexport HIVE_CONF_DIR=/data/apache-hive-1.2.2-bin/conf创建 conf/hive-site.xml文件$ cd conf;touch hive-site.xml在里面指定要链接的数据库,javax.jdo.option.ConnectionURL参数用于指定我们存放数据元的地方123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://zyh:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;什么是元数据？元数据就是可以理解为描述数据的数据，这边写的是本机的mysql,这些描述数据的数据会存在mysql的hive库中，你可以在启动hive以后查看一下mysql数据这些数据到底长什么样子。拷贝hadoop目录下配置文件到当前目录$ cp $HADOOP_HOME/etc/hadoop/core-site.xml .$ cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml . 2.启动Hive下面两种方法都可以启动$ cd apache-hive-1.2.1-bin ; bin/hive或者$ hive --service hiveserver2 --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.root.logger=INFO,console$ bin/beeline -u jdbc:hive2://cor1:10000 -n hadoop我使用的是第一种方法 Hive一些简单操作下面是一些简单的操作。 1.创建外部表create external table extertable(id string,name string) row format delimited fields terminated by ‘,’; 2.插入数据现在本地生成一个文本extertable.txt,在里面输入下面内容1234561,zhangsan2,lisi3,wangwu4,tom5,jerry6,cetty使用下面命令就可以将数据导入到表中load data local inpath ‘xxxx/xxxx/extertable.txt’ into table extertable; 3.查询select * from extertable; Hive分桶其实Hive的本质还是在帮助我们运行MapReduce，它会将HQL按模板转化成指定MapReduce代码。这里说一下Hive分桶的概念，那么什么是分桶，为什么会用分桶。我们知道在运行MapReduce程序时，可以指定多个Reduce Task来处理不同partition的数据，Hive的分桶就可以很好的实现这个功能，将不同的reduce task 生成的数据导入到不同的文件中。我们可以使用下面的语句来创建一个带分桶的表，值得注意的是，在使用分桶表时，错误的做法是直接往表里insert数据，这是不对的。正确的思路是，从其他的表中查询出数据，然后插入到该表中。create table student(id int, age int,name string) partitioned by (stat_data string) clustered by (id) sorted by (age) into 2 buckets row format delimited fields terminated by ‘,’;那么分桶后有什么好处呢？最大的好处应该是提高join的效率。数据经过分区，排序后，相同的id会被放在同一个桶内，这时再做链接查询时，就不会遍历全表。 Hive 自定义函数Hive为我们提供了很多内置函数。除此之外还提供了自定义的接口，说其中一种实现套路继承UDF 1. 继承UDF，并重载evaluate方法12345678910package com.bim.hive;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public final class Lower extends UDF &#123; public Text evaluate(final Text s)&#123; if(s==null)&#123;return null;&#125; return new Text(s.toString().toLowerCase()); &#125;&#125; 2.打成jar包上传到服务器这边推荐使用Eclipse打包成jar，我使用IDEA打包好久，比较麻烦。。 3.将jar包添加到hive的classpathadd JAR /home/hadoop/udf.jar; 4.4、 创建临时函数与开发好的java class关联create temporary function toprovince as ‘cn.itcast.bigdata.udf.ToProvince’;","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"luoxiao.cf/tags/Hive/"}]},{"title":"HDFS源码刨析-FileSystem初始化","slug":"2018-04-27-hdfs-HDFS源码刨析-FileSystem初始化","date":"2018-04-27T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/27/2018-04-27-hdfs-HDFS源码刨析-FileSystem初始化/","link":"","permalink":"luoxiao.cf/2018/04/27/2018-04-27-hdfs-HDFS源码刨析-FileSystem初始化/","excerpt":"","text":"理一下HDFS上传的工作原理，然后追一下源码。上传工作原理和源码刨析放下一篇总结。本机环境:操作系统ubuntu 16.0.4TLShadoop版本hadoop-2.7.3HA否（随便搭了个分布式） HDFS上传文件原理图Client会将文件切分成指定大小的块(block),块的大小默认128MClient会从第一个块开始，向NameNode发起上传文件请求，通过RPC与NameNode建立通讯。DataNode定时向NameNode汇报自己持有的数据信息(心跳机制)。NameNode收到上传文件请求选择合适的DataNode节点信息(MetaData)返回给Client。Client读取MetaData与DataNode2建立链接，并告诉DataNode_2，还想把这份文件传送给DataNode_3和DataNode_4，随后数据以packet数据包的形式传输，中间会经过chunk校验等。dataNode_2拿到数据将数据存入磁盘，与dataNode_3建立连接，传给dataNode_3,同理完成dataNode_4传输。中间只要有一个成功及判定为成功。 元数据存储可以将元数据理解为描述数据的数据,红色部分就是元数据。元数据是从fsimage文件中读取出来的，在我们对hdfs进行操作时，元数据都会发生改变，那么fsimage的数据是谁写进去的？SecondNameNode中用了一种机制可以帮助实现这个操作，叫checkPoint。这的水有点神，后续专门进行详述。 HDFS下载文件原理图Client请求NameNode下载BLK_1块数据,NameNode查找MetaData元数据NameNode将MetaData元数据返回给ClientClient与元数据中任意一台机器建立链接并下载数据(重复此过程下载BLK_2)在FileOutputStream中完成BLK_1块和BLK_2块的合并 FileSystem初始化源码分析我们先简单使用hadoop提供的API来实现文件的上传下载（文件删除、改名等操作比较简单，这里不演示）。不管我们进行什么操作，只要是对hdfs上的文件进行操作，必须对FileSystem进行初始化，我们先来分析FileSystem的初始化：12345678static&#123; try&#123; // 打断点 fs = FileSystem.get(new URI(\"hdfs://cor1:9000\"),configuration); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;&#125;1234567891011121314151617public static FileSystem get(URI uri, Configuration conf) throws IOException &#123; String scheme = uri.getScheme(); String authority = uri.getAuthority(); if(scheme == null &amp;&amp; authority == null) &#123; return get(conf); &#125; else &#123; if(scheme != null &amp;&amp; authority == null) &#123; URI disableCacheName = getDefaultUri(conf); if(scheme.equals(disableCacheName.getScheme()) &amp;&amp; disableCacheName.getAuthority() != null) &#123; return get(disableCacheName, conf); &#125; &#125; String disableCacheName1 = String.format(\"fs.%s.impl.disable.cache\", new Object[]&#123;scheme&#125;); // 注意这里的CACHE.get(uri,conf)跟进去 return conf.getBoolean(disableCacheName1, false)?createFileSystem(uri, conf):CACHE.get(uri, conf); &#125;&#125;12345FileSystem get(URI uri, Configuration conf) throws IOException &#123; FileSystem.Cache.Key key = new FileSystem.Cache.Key(uri, conf); // 跟进去 return this.getInternal(uri, conf, key);&#125;这个方法最终返回FileSystem的子类DistributedFileSystem123456789101112131415161718192021222324252627282930313233private FileSystem getInternal(URI uri, Configuration conf, FileSystem.Cache.Key key) throws IOException &#123; FileSystem fs; // 这里使用了单例模式，第一次初始化fileSystem的时候会稍微慢点，后续再来拿就直接从map里面取 synchronized(this) &#123; fs = (FileSystem)this.map.get(key); &#125; if(fs != null) &#123; return fs; &#125; else &#123; // 跟进去 fs = FileSystem.createFileSystem(uri, conf); synchronized(this) &#123; FileSystem oldfs = (FileSystem)this.map.get(key); if(oldfs != null) &#123; fs.close(); return oldfs; &#125; else &#123; if(this.map.isEmpty() &amp;&amp; !ShutdownHookManager.get().isShutdownInProgress()) &#123; ShutdownHookManager.get().addShutdownHook(this.clientFinalizer, 10); &#125; fs.key = key; this.map.put(key, fs); if(conf.getBoolean(\"fs.automatic.close\", true)) &#123; this.toAutoClose.add(key); &#125; return fs; &#125; &#125; &#125;&#125;1234567private static FileSystem createFileSystem(URI uri, Configuration conf) throws IOException &#123; Class clazz = getFileSystemClass(uri.getScheme(), conf); FileSystem fs = (FileSystem)ReflectionUtils.newInstance(clazz, conf); // 跟进去注意直接点是不行的，在DistributedFileSystem的initialize方法上打断点 fs.initialize(uri, conf); return fs;&#125;1234567891011121314public void initialize(URI uri, Configuration conf) throws IOException &#123; super.initialize(uri, conf); this.setConf(conf); String host = uri.getHost(); if(host == null) &#123; throw new IOException(\"Incomplete HDFS URI, no host: \" + uri); &#125; else &#123; this.homeDirPrefix = conf.get(\"dfs.user.home.dir.prefix\", \"/user\"); // 在这里初始化DFSClient并指向fileSystem里的一个引用 this.dfs = new DFSClient(uri, conf, this.statistics); this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority()); this.workingDir = this.getHomeDirectory(); &#125;&#125;1234567891011121314151617181920212223242526272829@VisibleForTestingpublic DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode, Configuration conf, Statistics stats) throws IOException &#123; this.clientRunning = true; this.r = new Random(); this.filesBeingWritten = new HashMap(); SpanReceiverHost.get(conf, \"dfs.client.htrace.\"); this.traceSampler = (new SamplerBuilder(TraceUtils.wrapHadoopConf(\"dfs.client.htrace.\", conf))).build(); this.dfsClientConf = new DFSClient.Conf(conf); if(this.dfsClientConf.useLegacyBlockReaderLocal) &#123; LOG.debug(\"Using legacy short-circuit local reads.\"); &#125; // 代码太多直截了部分代码 ..... if(proxyInfo != null) &#123; this.dtService = proxyInfo.getDelegationTokenService(); this.namenode = (ClientProtocol)proxyInfo.getProxy(); &#125; else if(rpcNamenode != null) &#123; Preconditions.checkArgument(nameNodeUri == null); this.namenode = rpcNamenode; this.dtService = null; &#125; else &#123; Preconditions.checkArgument(nameNodeUri != null, \"null URI\"); proxyInfo = NameNodeProxies.createProxy(conf, nameNodeUri, ClientProtocol.class, nnFallbackToSimpleAuth); this.dtService = proxyInfo.getDelegationTokenService(); // 初始化代理对象,负责RPC远程调用,指向DFSClient的namenode引用,当客户端拿到了NameNode的代理对象后，即与NameNode建立了RPC通信 this.namenode = (ClientProtocol)proxyInfo.getProxy(); &#125; .....&#125;到此，FileSystem的初始化就基本完成。后续分析上传、下载源码，然后理一下SecondNameNode的CheckPoint机制。本人水平有限，不当之处希望各位高手指正。另外插入是在word中画的，看起来不精致请见谅。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"HDFS","slug":"HDFS","permalink":"luoxiao.cf/tags/HDFS/"}]},{"title":"FTP多线程批量文件下载","slug":"2018-04-25-thread-FTP多线程批量文件下载","date":"2018-04-25T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/25/2018-04-25-thread-FTP多线程批量文件下载/","link":"","permalink":"luoxiao.cf/2018/04/25/2018-04-25-thread-FTP多线程批量文件下载/","excerpt":"","text":"最近接到个业务需要使用FTP拉取服务器上数据。要求可以任意指定下载对应目录数据，并且目录结构保持要。处理的数据文件特点分散而且很大。处理的思路大概有两个，一个是在服务端压缩成zip，然后传过来。二是使用多线程单个单个文件传输。在这里我使用的是第二中方法。 思路1.服务端提供一个返回指定文件下的List&lt;String&gt; files2.客户端拿到files文件列表，遍历单个单个文件请求服务端拉取数据 FTP下载使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public static boolean downloadFromFtp(String middlePath, String fileName, String localPath) throws IOException &#123; return downloadFromFtp(url, port, username, password,middlePath, fileName, localPath); &#125; public static boolean downloadFromFtp(String url, int port, String username, String password, String path, String fileName, String localpath) throws IOException &#123; boolean flag = false; FTPClient ftp = new FTPClient();//org.apache.commons.net.ftp int reply; try &#123; if (port &gt; -1) &#123; ftp.connect(url, port); &#125; else &#123; ftp.connect(url);//ftp默认的端口是21 &#125; //很多人写的是用ftp.getReplyCode()给获取连接的返回值,但是这样会导致storeFileStream返回null ftp.login(username, password); ftp.enterLocalActiveMode(); ftp.setFileType(FTPClient.BINARY_FILE_TYPE); reply = ftp.getReplyCode(); if (!FTPReply.isPositiveCompletion(reply)) &#123; ftp.disconnect(); return flag; &#125; //切换目录 此处可以判断,切换失败就说明ftp上面没有这个路径 ftp.changeWorkingDirectory(path); //上传文件 OutputStream out = null; InputStream in = null; //创建本地的文件时候要把编码格式转回来 File localDir = new File(localpath +\"/\" + path); if(!localDir.exists())&#123; localDir.mkdirs(); &#125; fileName = new String(fileName.getBytes(\"ISO-8859-1\"), \"utf-8\"); File localFile = new File(localpath + \"/\" + path + \"/\" + fileName); out = new FileOutputStream(localFile); //ftp.enterLocalPassiveMode(); in = ftp.retrieveFileStream(fileName); byte[] byteArray = new byte[4096]; int read = 0; while ((read = in.read(byteArray)) != -1) &#123; out.write(byteArray, 0, read); &#125; //这句很重要 要多次操作这个ftp的流的通道,要等他的每次命令完成 ftp.completePendingCommand(); out.flush(); out.close(); ftp.logout(); flag = true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (ftp.isConnected()) &#123; ftp.disconnect(); &#125; &#125; return flag; &#125;downloadFromFtp传入参数说明String参数解释urlftp的ip地址portftp的端口(默认21)usernameportftp用户名passwordftp密码path特别重要，一开报输入流为null就是因为它写错了;假如你的ftp根目录为C:/ftp/,你想要下载C:/ftp/xxx/下的文件,那么path就要写xxx/fileName下载文件的名称localpath下载到本地的路径 server返回制定文件列表用户给定一个path，查处path下所有的文件，放在list，以json形式返回。12345678910111213// 递归查询所有的文件public ArrayList listFiles(String path,ArrayList files)&#123; File directory = new File(path); File[] currentFiles = directory.listFiles(); for (File file:currentFiles) &#123; if (file.isDirectory())&#123; listFiles(file.getPath(),files); &#125;else&#123; files.add(file); &#125; &#125; return files;&#125; client获取返回列表下载文件四类线程池基本的线程池概述，这边按需求选择，我这里选了定长线程池FixedThreadPoolFixedThreadPool 定长的线程池，初始化时指定线程的个数，当线程池中线程被用完时，其他任务阻塞等待CachedThreadPool 不定长线程池，无限扩大的线程池，来几个任务分配几个线程。SimpleThreadPool 单例线程，底层采用LinkedBlockQueue实现，除了排在队列最前面的线程以外的其他线程都要等着。ScheduleThreadPol 在初始化时可以指定时间帮助我们处理延时任务和定时任务。客户端的思路:1.使用HttpClient从后台发送请求获取待下载files列表2.将DownloadThread分配给FixedThreadPool运行这边主要看DownloadThread.java如何编写，以及如何分配给fixedThreadPoolDownloadThread.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.bim.task;import com.bim.common.FtpUtils;import java.util.List;public class DownLoadThread implements Runnable &#123; List files = null; String ftpPath = null; String baseLocalPath = null; public DownLoadThread(List files,String ftpPath,String baseLocalPath)&#123; this.files = files; this.ftpPath = ftpPath; this.baseLocalPath = baseLocalPath; &#125; // 下载文件的具体业务 // ftpPath ftp地址（c:/ftp） // baseLocalPath 目标地址 private void downloadFile(String file,String ftpPath,String baseLocalPath) throws Exception&#123; String dllPath = file.toString().replaceAll(\"\\\\\\\\\",\"/\"); int lastIndexOf = dllPath.lastIndexOf(\"/\"); String middlePath = \"/\"; if(ftpPath.length() - 1 &lt;= lastIndexOf)&#123; middlePath = dllPath.substring(ftpPath.length(),lastIndexOf+1); &#125; String fileName = dllPath.substring(lastIndexOf + 1,dllPath.length()); System.out.println(\"filename\" + fileName); System.out.println(\"baseLocalPath\" + baseLocalPath); System.out.println(\"开始下载:\" + middlePath + fileName + \"到本地\" + baseLocalPath); // 调用ftp下载文件 FtpUtils.downloadFromFtp(middlePath,fileName,baseLocalPath); System.out.println(middlePath + fileName + \"下载成功\"); &#125; @Override public void run() &#123; try&#123; while(!files.isEmpty())&#123; String file = null; synchronized(files)&#123; file = (String) files.get(0); files.remove(0); &#125; downloadFile(file,ftpPath,baseLocalPath); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125;主程序拿到files后的逻辑代码部分1234567891011121314151617public class PullFileClient &#123; private static ExecutorService fixedThreadPool = Executors.newFixedThreadPool(10); public static void main(String[] args) throws IOException &#123; Map&lt;String,Object&gt; resultMap = client.sendGet(requestUrl); Object code = resultMap.get(\"code\"); // 获取文件列表数据 List files = (List) resultMap.get(\"data\"); // ftp的根目录 (c:/ftp) String ftpPath = \"c:/FTP/\"; String baseLocalPath = \"/home/zyh/Documents/tmp4/\"; fixedThreadPool.execute(new DownLoadThread(files,ftpPath,baseLocalPath)); fixedThreadPool.shutdown(); &#125; &#125;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"FTP","slug":"FTP","permalink":"luoxiao.cf/tags/FTP/"}]},{"title":"MapReduce的Shuffle机制","slug":"2018-04-22-mapreduce-mapreduce的shuffle机制","date":"2018-04-22T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/22/2018-04-22-mapreduce-mapreduce的shuffle机制/","link":"","permalink":"luoxiao.cf/2018/04/22/2018-04-22-mapreduce-mapreduce的shuffle机制/","excerpt":"","text":"在编写MapReduce，启动主程序以后，到底maptask和reducetask之间是怎么工作的？数据是如何进行排序的？1.inputFormat:我们知道mapreduce主程序初始化job以后会对输入的数据进行切片规划，生成job.split文件，inputFormat会读取job.split文件，根据信息从DHFS中找到要读取的数据,调用recordReader将数据读成一行,传入mapper中,默认的使用的是TextInputFormat2.当我们在mapper调用write后,outputCollector会将输出的数据写入环形缓冲区中3.环形缓冲器的大小默认为100M，当环形缓冲区的内容达到自身容量的80%后,进行溢出操作，生成多个文件。期间会经过4、5、6这几个操作4.我们可以在该类中定义要分几个区，例如hashCode % reduceTask,这样我们可以使用job.setNumReduceTask动态划分分区大小5.对分区中的序列进行排序。同样排序的方式可以根据不同的业务做调整6.溢出这个操作是由一个叫做spiller的组件完成的，当缓存区数据达到制定标准后进行溢出到文件操作。8.GroupingComparaor组件，它可以对reducetask最终形成的文件的内容进行分组。在WordCount的mapper程序中，输出write(word,1)经过shuffle处理后的数据就是一个经过分组后的有序数据。reducetask的reduce(k,values)会读的就是每一组的数据。GroupingComparaor组件的逻辑会将文件内容分成三个组,第一组&lt;a,1&gt;&lt;a,1&gt;&lt;a,1&gt;。第二组&lt;c,1&gt;&lt;c,1&gt;…那么这就意味着,reducetask第一次reduce的k为a,values为1,1,1;第二次reduce的k为c,values为1,1。9.将分组后的数据按组分批次发给reducetask的reduce方法。10.write以后OutputFormat组件会调用RecordWriter，将内容上传到hdfs中。本人水平有限，不当之处希望各位高手指正。另外插入是在画图中画的，看起来不精致请见谅。http://www.zonegood.com","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"luoxiao.cf/tags/MapReduce/"}]},{"title":"初识MapReduce","slug":"2018-04-22-mapreduce-初识mapreduce","date":"2018-04-22T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/22/2018-04-22-mapreduce-初识mapreduce/","link":"","permalink":"luoxiao.cf/2018/04/22/2018-04-22-mapreduce-初识mapreduce/","excerpt":"","text":"mapredice其实就是分而治之的一种思想，hadoop的mapreduce是应对大数据产生的一种计算方式。分为两个步骤，maptask和reducetask。多个maptask并发执行运算输入数据，每个maptask各司其职，各自为政。多个reducetask并发执行，但它依赖于maptask，它输入参数是maptask的输出参数。 mapreduce框架中角色核心角色有三个，分别为maptask,reducetask,mrappmaster,他们都是在进行运算过程中生成的进程。mrappmaster 协调maptask和reducetask的工作maptask 对数据进行第一阶段运算reducetask 对maptask输出的数据进行第二阶段运算 运行流程当mapreduce主程序启动后会生成yarnRunner Proxy代理根据用户指定的文件、相关blocksize配置，生成切片，封装成job.split，除此之外生成job.xml参数，wordcount.jar等信息将生成的数据一同发送给yarnyarn收到信息以后，会在resourcemanager维护的队列中初始化tasknamenode发现队列中的task，会在自己内存中生成mrappmaster进程，mrappmaster的主要职责是协调好maptask及reducetask的运行。mrappmaster会根据job.split文件中的切片信息向resourcemanager申请生成指定个数的maptask任务。maptask启动以后会调用InputFormat组件，根据切片的描述信息，到hdfs上下载对应block块，将块的数据作为数据源进行map运算。待maptask程序运行完成以后，mrappmaster进程向resourcemanager申请启动reducetaskyarn收到信息以后，会在resourcemanager维护的队列中初始化tasknodemanager发现队列中的task，为reducetask分配cpu以及内存，运行reducetask。reducetask对maptask生成的数据进行汇总处理，并将结果上传到hdfs目录中。要注意的是生成maptask、resourcetask的资源分配这个过程需要resourcemanager统一管理.","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"luoxiao.cf/tags/MapReduce/"}]},{"title":"MapReduce切片规划源码剖析","slug":"2018-04-22-mapreduce-MapReduce切片规划源码刨析","date":"2018-04-22T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/22/2018-04-22-mapreduce-MapReduce切片规划源码刨析/","link":"","permalink":"luoxiao.cf/2018/04/22/2018-04-22-mapreduce-MapReduce切片规划源码刨析/","excerpt":"","text":"切片规划最终会形成一个文件job.split。里面存放这切片信息，首先要明确一点是maptask的数量于切片的数量有直接对应关系。mrappmaster在启动maptask时，会去job.split文件中找切片信息，有几个切片就启动几个maptask，每个切片分配一个maptask并行实例。我们通过追源码，找到了这个文件。MapReduce框架会把它存在我们本机的某个路径。它是MapReduce对于待处理数据的一个描述信息文件。关于切片需要注意的几点:文件是怎么进行切分的？切片的大小是怎么控制的？ 源码追踪根据源码追踪，发现切片的划分是在FileInputFormat的getSplits()方法中完成的。getSplits的大致业务逻辑可分为下列几个阶段根据配置文件拿到切片大小，源码Math.max(minSize, Math.min(maxSize, blockSize));(如果我们没有设置过mapreduce.input.fileinputformat.split.minsize，mapreduce.input.fileinputformat.split.maxsize,那么默认的切片大小为128M，及为hdfs中block块大小)根据不同的文件系统，获取文件规划的上传目录。（hdfs-&gt; hdfs://…/.staging/jobid或file-&gt;file://…/.staging/jobid）根据用户指定的hdfs目录找到对应的所有文件遍历所有文件，拿到文件的元数据，对每个文件进行切片规划规划的一个策略：根据拿到的切片大小来切每个文件,假如文件大小为300M,切片大小为128M,那么走完循环后文件会被分为3块(0-128,128-256,256-300)将切片封装为job.split生成其他信息,比如job.xml(所有的参数信息)。将生成的所有文件以流的形式写到指定上传目录，也就是hdfs://…/.staging/jobid一步一步断点调试,通过本地运行MapReduce程序,进入debug。先在job.waitForCompletion(true);打断点。发现会进入submitter.submitJobInternal(),跳进去可以看到拿到了jobStagingArea,后续拿到了jobId,最后拼成了submitJobDir。这个目录就是上面提到的job.split存放的目录。拿到submitJobDir以后，MapReduce开始调用this.writeSplits(job, submitJobDir)对文件进行逻辑切分，形成job.split文件(后续详细解析里面的内部详情)后续获取配置信息。形成job.xml文件，这个文件里面定义了hadoop中各种各样的配置信息最后将job.split,job.xml,写到对应的submitJobDir目录下。 深入writeSplits逻辑在this.writeSplites()我们如何跳到具体方法中，我们直接ctrl+左键点进取以后会发现是一个抽象类InputFormat。是这样如果你在初始化job是没有指定用哪个inputFormat,那么mapreduce框架默认会使用TextInputFormat使用快捷键ctrl+shift+T搜索TextInputFormat,我们会发现找不到getSplits方法，这是因为方法在父类里面，进去父类FileInputFormat中就可以找到了，在第一行打断点运行就可以跳进去了。InputFormat的类结构图来看看getSplits方法的具体逻辑，我直接把源码粘过来了,我在关键的地方加了注释1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123; StopWatch sw = (new StopWatch()).start(); // 调节切片大小的参数1 long minSize = Math.max(this.getFormatMinSplitSize(), getMinSplitSize(job)); // 调节切片大小的参数1 long maxSize = getMaxSplitSize(job); ArrayList splits = new ArrayList(); // 拿到待处理的文件 List files = this.listStatus(job); Iterator i$ = files.iterator(); while(true) &#123; while(true) &#123; // 循环处理每个文件 while(i$.hasNext()) &#123; FileStatus file = (FileStatus)i$.next(); Path path = file.getPath(); long length = file.getLen(); if(length != 0L) &#123; BlockLocation[] blkLocations; if(file instanceof LocatedFileStatus) &#123; blkLocations = ((LocatedFileStatus)file).getBlockLocations(); &#125; else &#123; FileSystem blockSize = path.getFileSystem(job.getConfiguration()); blkLocations = blockSize.getFileBlockLocations(file, 0L, length); &#125; if(this.isSplitable(job, path)) &#123; long blockSize1 = file.getBlockSize(); // splitSize是一个重要的参数，它根据minSize和maxSize计算出切片的大小（点进查看计算方式） long splitSize = this.computeSplitSize(blockSize1, minSize, maxSize); long bytesRemaining; int blkIndex; // 到底切不切就看这行代码了，这边的计算方式是判断如果文件的大小比splitSize大的话就对文件进行切分，否则就不要在切了，因为它已经很小了。 for(bytesRemaining = length; (double)bytesRemaining / (double)splitSize &gt; 1.1D; bytesRemaining -= splitSize) &#123; blkIndex = this.getBlockIndex(blkLocations, length - bytesRemaining); splits.add(this.makeSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts())); &#125; // 如果这个文件已经很小了，就不需要在切了直接放到splits中。 if(bytesRemaining != 0L) &#123; blkIndex = this.getBlockIndex(blkLocations, length - bytesRemaining); splits.add(this.makeSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts())); &#125; &#125; else &#123; splits.add(this.makeSplit(path, 0L, length, blkLocations[0].getHosts(), blkLocations[0].getCachedHosts())); &#125; &#125; else &#123; splits.add(this.makeSplit(path, 0L, length, new String[0])); &#125; &#125; job.getConfiguration().setLong(\"mapreduce.input.fileinputformat.numinputfiles\", (long)files.size()); sw.stop(); if(LOG.isDebugEnabled()) &#123; LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size() + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS)); &#125; return splits; &#125; &#125;&#125; 切片的计算方式首先会计算出minSize，默认1long minSize = Math.max(this.getFormatMinSplitSize(), getMinSplitSize(job));1234567protected long getFormatMinSplitSize() &#123; return 1L;&#125;public static long getMinSplitSize(JobContext job) &#123; return job.getConfiguration().getLong(\"mapreduce.input.fileinputformat.split.minsize\", 1L);&#125;计算maxSize，默认128Mlong maxSize = getMaxSplitSize(job);123public static long getMaxSplitSize(JobContext context) &#123; return context.getConfiguration().getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 9223372036854775807L); &#125;最后计算splitSizelong splitSize = this.computeSplitSize(blockSize1, minSize, maxSize);123 protected long computeSplitSize(long blockSize, long minSize, long maxSize) &#123; return Math.max(minSize, Math.min(maxSize, blockSize));&#125;在切文件是采用这样的判断方式(double)bytesRemaining / (double)splitSize &gt; 1.1D只要文件的大小/splitSize &gt; 1.1倍时就会对文件进行切分 问题总结归结一点就是mapreduce主程序运行后，会将每个文件进行逻辑切分，这里的逻辑切分的意思就是说并非真的对文件进行切分，只是生成一些描述信息,存到job.split文件中。有几个切片就会生成几个maptask。maptask数量和切片的数量一一对应，因为yarnRunner会把job.split发给yarn,yarn收到这个文件以后会读取里面的切片信息，然后初始化maptask。文件切分的方法是InputFormat类提供的，在getSplits方法中我们可以看到(double)bytesRemaining / (double)splitSize &gt; 1.1D，只要文件的大小/splitSize &gt; 1.1倍时就会对文件进行切分,splitSize默认是128M。通过查看源码我们可以知道通过两个参数mapreduce.input.fileinputformat.split.minsize和mapreduce.input.fileinputformat.split.maxsize就可以调节。了解了切片后，我们到底切多大？是比HDFS block大好还是比他小好？为什么要对文件进行切分？要明白切片规划的其中一个重要的原因就是mapreduce期望，在运行每个maptask时，任务所需要的输入数据恰好能在本地，这样就能保证每次maptask于hdfs的数据交互时，直接可以从本地拿到数据。理想状态（切片的大小=blocksize,减少maptask于hdfs的跨网络数据传输）。本人水平有限，不当之处希望各位高手指正。邮箱cnnqjban521@gmail.com。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"luoxiao.cf/tags/MapReduce/"}]},{"title":"手写基于Java反射的RPC框架","slug":"2018-04-19-rpc-手写基于Java反射的RPC框架","date":"2018-04-19T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/19/2018-04-19-rpc-手写基于Java反射的RPC框架/","link":"","permalink":"luoxiao.cf/2018/04/19/2018-04-19-rpc-手写基于Java反射的RPC框架/","excerpt":"","text":"目录说明项目已经传至github仓库。rpc-client : 框架中客户端框架核心代码rpc-server : 框架中服务端框架核心代码rpc-common : 框架工具rpc-registry : 框架中两个核心业务类功能注册服务 - ServiceDiscovery发现服务 - ServiceRegistryrpc-sample-app : 客户端程序 (调用被发布的服务,执行对应业务代码)rpc-sample-server : 服务区程序 (用来发布RPC服务) 框架结构预备知识梳理。采用Java语言编写，需要掌握线程、动态代理、反射、Netty、注解、Spring等知识。RPC远程过程调用，是一种常见的底层通信框架，有效的理解它对于后续学习其他开源框架有很大帮助。下面梳理设计思路与具体代码逻辑。见整体框架图：注:实线以上部分是用户需要完成的操作，一下部分则是需要封装后打包成jar包，供用户导入的框架内部核心代码。 用户部分用户部分也分为客户端和服务端，服务端用来发布想要发布的服务，客户端通过本框架来调用被发布的服务。参考新浪的motan以及阿里的dubbo，想要启动RPC框架，首先需要在spring的配置文件application.xml中配置框架类才行，我们这边也同样是这个思路。 1.服务端服务端只需要完成以下两步操作就可以完成业务的发布了。application.xml配置框架ServiceRegistry以及RpcServer在想要发布成服务的具体实现业务类上添加@RpcService(interface.class)注:两个类的细节问题见框架核心代码业务梳理。 2.客户端客户端只需要完成以下两步操作就可以完成远程服务调用。application.xml配置框架ServiceDiscovery以及RpcProxy在调用时使用@autowried构造RpcProxy对象。通过RpcProxy来获取具体调用的业务类接口代理对象，通过该对象调用具体方法即可。注:两个类的细节问题见框架核心代码业务梳理。对于RPC框架的使用者来说,不管是客户端还是服务端，底层细节都是透明的。 框架核心代码业务梳理服务端框架的核心业务包含两个类，ServiceRegistry和RpcServera. RpcServer.java在Spring容器启动后会构造RpcServer，扫描@RpcService(interface.class)注解,拿到具体业务类的接口以及接口实现类，将信息已k,v形式封装到指定的HashMap中。在服务端启动一个netty主程序，在netty中指定具体的Handler业务线字节流的反序列化对象的序列化调用ServiceRegistry的registry()b. ServiceRegistry.java为RpcServer提供registry方法,启动zookeeper主程序，将启动好的netty的ip:port存到zookeeper节点中(后续可以拓展zookeeper节点，应对不同的业务场景)客户端框架的核心业务包含两个类，ServiceDiscovery和RpcProxya. RpcProxy.java在Spring容器启动后会构造RpcProxy用户拿到该对象调用interfaceProxy = create(interface.class)就可以拿到对应的接口代理对象用户使用interfaceProxy调用具体业务时，会触发JDK动态代理中的invoke方法,将方法及方法参数进行封装调用ServiceDiscovery的discoveryy()启动netty客户端程序，执行netty中指定具体的Handler业务线对象的序列化 outputStream字节流的反序列化 inputStream返回数据结果 inputStreamb. ServiceDiscovery.javaServiceDiscovery会从启动的zookeeper中找到服务器的地址(netty server ip:port)这边对象的序列化和反序列化使用的是google的protobuf框架,它最大的特点就是可以跨平台。想要深入这边有链接:https://developers.google.com/protocol-buffers/框架还有很多地方可以优化，比如可以增加zookeeper的存放根节点，来进行拓展业务，适应不同的业务场景；比如客户端中netty并没进行封装，应该是spring web容器启动时自动装载netty客户端。再有这个框架不能满足同一接口存在多个实现的case，那么想要满足，只需要为@RpcServer注解添加额外属性即可。","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"luoxiao.cf/tags/RPC/"}]},{"title":"部署zookeeper集群","slug":"2018-04-15-hadoop-zookeeper集群搭建及其使用","date":"2018-04-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/15/2018-04-15-hadoop-zookeeper集群搭建及其使用/","link":"","permalink":"luoxiao.cf/2018/04/15/2018-04-15-hadoop-zookeeper集群搭建及其使用/","excerpt":"","text":"zookeeper是一个分布式协调服务框架，本次手写RPC框架需要使用zookeeper做中间件，进行通信的业务的协调。下面开始部署分布式zookeeper。这边我使用的是虚拟机，在虚拟机中部署多台zookeeper。 1.下载我们需要到apache官网上下载Zookeeper,官网地址https://archive.apache.org/dist/zookeeper/,我这边是ubuntu操作系统,下载尾缀是tar.gz,如果是windows下载zip，我下载的是3.4.10版本 2.启动虚拟机集群我在虚拟机中安装了cor1、cor2、cor3、cor4四台机器，用的是centOS操作系统。 关于数量配置的是偶数台集群，其实不太好，最好集群数量是基数，这是因为zookeeper的运行机制，只要有半数的集群数存活的话，zookeeper就能正常工作，我们分别启动每台机器。小技巧：SecureCRT链接集群使用Send commands to all sessions可以一次性控制多台机器,我们只需要打一条命令就可以了，这个很重要，假如集群数太多上千台，我们不可能一台一台去陪，这个时候使用这个功能就可以同时操作了。 3.zookeeper配置配置的话分两步配置zoo.cfg配置配台机器对应的myid配置zookeeper/conf/zoo.cfg:123456789101112131415161718192021222324252627282930313233# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/root/zookeeper/datadataLogDir=/root/zookeeper/log# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to \"0\" to disable auto purge feature#autopurge.purgeInterval=1server.1=cor1:2888:3888server.2=cor2:2888:3888server.3=cor3:2888:3888server.4=cor4:2888:3888配置里面大部分都不需要我们动，我们只需要配好我们的集群就可以了server.1=cor1:2888:3888server.2=cor2:2888:3888server.3=cor3:2888:3888server.4=cor4:2888:38882888是内部每个zookeeper的通信接口3888是zookeeper的投票选举leader的接口后续我们需要在zookeeper/data/目录下创建每台zookeeper的id创建myid文件 在里面写上每台机器对应的编号,如果是cor1 就在myid文件中写上1,cor2就写2 4.配置集群服务器在一台机器上部署好zookeeper后，我们使用scp命令将配好的zookeeper分发到其他机器上就可以了。但是运行zookeeper还需要Java环境，我们不可能手动为每台电脑一一做配置，这边就用脚本。其中牵扯两个脚本auto_install_jdk.sh和install_jdk.sh思路：ssh免密登录--&gt;发送jdk安装包--&gt;发送jdk安装脚本（自动解压jdk,自动配置环境变量,自动运行zookeeper）--&gt;发送已配好的zookeeper包--&gt;运行已发送的jdk安装脚本auto_install_jdk.sh1234567891011121314151617181920212223242526272829303132#!/bin/bashSERVERS=\"192.168.0.102\"PASSWORD=\"1\"INDEX=1auto_ssh()&#123; expect -c \"set timeout -1; spawn ssh-copy-id root@$1; expect &#123; *(yes/no)* &#123;send -- yes\\r;exp_continue;&#125; *password:* &#123;send -- $2\\r;exp_continue;&#125; eof &#123;exit 0;&#125; &#125;\";&#125;each_server()&#123; for SERVER in $SERVERS do auto_ssh $SERVER $PASSWORD done&#125;each_server#发送jdk安装包scp $HOME/Documents/jdk-8u161-linux-x64.tar.gz root@$SERVER:/root#发送jdk安装命令并自动配置环境变量的脚本1scp $HOME/workspace/shell/install_jdk.sh root@$SERVER:/root#发送zookeeper包scp -r /home/zyh/zookeeper root@$SERVER:/root#脚本1 $INDEX 指的就是每台机器对应的myidssh root@$SERVER /root/install_jdk.sh $INDEXinstall_jdk.sh1234567891011121314151617#/bin/bashtar -zxvf /root/jdk-8u161-linux-x64.tar.gz -C ./cat &gt;&gt; /etc/profile &lt;&lt; EOFexport JAVA_HOME=/root/jdk1.8.0_161export PATH=\\$PATH:\\$JAVA_HOME/binEOFsource /etc/profile#关闭防火墙service iptables stopchkconfig iptables offecho \"\" &gt; /root/zookeeper/data/myidecho $1 &gt; /root/zookeeper/data/myidcd /root/zookeeper/bin./zkServer.sh start 5.运行脚本这边想要运行修改一下下面的参数SERVERS=“192.168.0.102”PASSWORD=“1”INDEX=1执行./auto_install_jdk.sh，脚本还有很多优化的地方，可以结合自己的业务场景适当修改。","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"luoxiao.cf/tags/Zookeeper/"}]},{"title":"jvm调优","slug":"2018-04-15-jvm-jvm调优实战","date":"2018-04-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/15/2018-04-15-jvm-jvm调优实战/","link":"","permalink":"luoxiao.cf/2018/04/15/2018-04-15-jvm-jvm调优实战/","excerpt":"","text":"JVM中可分为方法区(非堆)、堆、桟、本地桟、程序计数器。今天主要说堆，我们new的对象会存在该区域内，其中可分为Young Generation(新生代)、Old Generation(老年代)和Permanent Generation(永久代)，它被多个线程所共享同时也是GC主要负责垃圾回收的主要区域之一。 垃圾回收机制刚被实例化的对象会存在于Eden Space中，当Eden Space空间满了以后,GC会进行垃圾回收，将不需要的对象回收掉，幸存下来的对象会被分到 Surivor Ratio区域中，我们注意到 Survivor Ratio 区域被分为两个空间，那么它们有什么意义呢？其实分为两个空间的主要目的是避免在垃圾回收以后，产生大量的碎片。空间碎片对Java的性能影响是巨大的，所以要极力避免这种情况。当Eden Space 第一次满了以后，经历过GC回收后，幸存的对象会被放在S0区域内，当Eden Space再次满GC对其进行再次回收以后，幸存的对象会和S0内的对象进行合并，然后复制到S1区域内，当第三次回收后和S1合并复制到S0…如此反复16次以后。最终幸存的对象会被送到Old Generation。这复制的办法其实只是GC回收算法中的一种。GC 回收策略标记-清理 （Mark-Sweep）标记-删除 （Mark-Compact）复制 （Copying） jvm参数列表java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m-XX:MaxTenuringThreshold=0-Xmx3550m：最大堆内存为3550M。-Xms3550m：初始堆内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xmn2g：设置年轻代大小为2G。整个堆大小=年轻代大小 + 年老代大小 +持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。-Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000左右。-XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6-XX:MaxPermSize=16m:设置持久代大小为16m。-XX:MaxTenuringThreshold=15：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象 再年轻代的存活时间，增加在年轻代即被回收的概论。 收集器设置常用的GC垃圾收集器有下面三种-XX:+UseSerialGC:设置串行收集器-XX:+UseParallelGC:设置并行收集器-XX:+UseParalledlOldGC:设置并行年老代收集器 JVM调优实战JDK中自带了两个监控内存堆的工具，分别是jconsole和jvisualvm，他们位于jdk的bin目录下。建议玩前几个例子，后面的例子很不友好，会让电脑卡死。悲催的我电脑强行重启了好几次。。。 1.内存溢出Exception in thread “main” java.lang.OutOfMemoryError: Java heap space内存堆溢出,示例代码:12345678910111213141516171819202122232425package com.edu;import java.util.ArrayList;public class TestMemory &#123; static class OOMObject &#123; public byte[] placeholder = new byte[64 * 1024*40]; &#125; public static void fillHeap(int num) throws Exception &#123; ArrayList&lt;OOMObject&gt; list = new ArrayList&lt;OOMObject&gt;(); for (int i = 0; i &lt; num; i++) &#123; Thread.sleep(50); list.add(new OOMObject()); &#125; System.gc(); &#125; public static void main(String[] args) throws Exception &#123; Thread.sleep(10000); fillHeap(100); Thread.sleep(20000000); &#125;&#125;需要为虚拟机设置参数-Xms100m -Xmx100m -XX:+UseSerialGC在IDEA设置找到run--&gt;Edit configurations…运行使用jconsole查看新生代堆内存状况。 2.检测死锁当线程出现死锁时，进程永远不能完成，并且阻碍使用系统资源，阻止了其他作业开始执行，导致系统的资源利用率急剧下载，造成很严重的后果。下面例子中会产生死锁线程，我们需要使用jconsole来找到对应死锁的进程。123456789101112131415161718192021222324252627282930package com.edu;/** * 检测死锁 */public class TestDeadThread implements Runnable &#123; int a, b; public TestDeadThread(int a, int b) &#123; this.a = a; this.b = b; &#125; @Override public void run() &#123; synchronized (Integer.valueOf(a)) &#123; synchronized (Integer.valueOf(b)) &#123; System.out.println(a + b); &#125; &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(new TestDeadThread(1, 2)).start(); new Thread(new TestDeadThread(2, 1)).start(); &#125; &#125;&#125;运行代码后,打开jconsole,在线程一栏中可以找到检测死锁按钮，通过它就可以找到对应死锁的进程 3.检测死循环、阻塞某些线程进入死循环，还有一些线程会阻塞在某个位置，但是它们其实并不是死锁，那么我们要如何排查？我们可以通过jvisualvm工具查看线程运行图来进行排查,下面是示例代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.edu;import java.io.BufferedReader;import java.io.InputStreamReader;public class TestThread &#123; public static void createBusyThread() &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"createBusyThread\"); while (true) ; &#125; &#125;, \"testBusyThread\"); thread.start(); &#125; public static void createLockThread(final Object lock) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"createLockThread\"); synchronized (lock) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;, \"testLockThread\"); thread.start(); &#125; public static void main(String[] args) throws Exception &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); br.readLine(); createBusyThread(); br.readLine(); Object object = new Object(); createLockThread(object); &#125;&#125;main线程追踪到需要键盘录入方便我们检测testBusyThread线程将在while（true）中一直运行，直到线程切换，很耗性能testLockThread线程由于调用了watit(),它会一直处于阻塞状态，等待notify()被唤醒我们运行程序打开jvisualvm工具 在终端输入后查看testBusyThread状态，之后在终端再次输入查看testLockThread状态，我们看到testBusyThread线程运行一栏一直是100%,而testLockThread线程的等待一栏一直是100% 4.直接内存溢出本例自很危险，会造成机器假死，慎跑…VM Args：-Xmx20M -XX:MaxDirectMemorySize=10M123456789101112131415161718package com.outofmemory;import java.lang.reflect.Field;import sun.misc.Unsafe;public class DirectMemoryOOM &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws Exception &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) &#123; unsafe.allocateMemory(_1MB); &#125; &#125;&#125; 5.堆内存溢出-XX：+HeapDumpOnOutOfMemoryError可以让虚拟机在出现内存溢出异常时Dump出当前的内存堆转储快照以便事后使用MemoryAnalyzer工具进行分析,关于MemoryAnalyzer可以百度其使用方法。VM Args：-Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError12345678910111213141516package com.outofmemory;import java.util.ArrayList;import java.util.List;public class HeapOOM &#123; static class OOMObject &#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;OOMObject&gt;(); while (true) &#123; list.add(new OOMObject()); &#125; &#125;&#125; 6.桟内存溢出本例用于体验OutOfMemoryError异常，容易让电脑假死，参数可以设置大一些VM Args：-Xss2M12345678910111213141516171819202122232425package com.outofmemory;public class JavaVMStackOOM &#123; private void dontStop() &#123; while (true) &#123; &#125; &#125; public void stackLeakByThread() &#123; while (true) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; dontStop(); &#125; &#125;); thread.start(); &#125; &#125; public static void main(String[] args) throws Throwable &#123; JavaVMStackOOM oom = new JavaVMStackOOM(); oom.stackLeakByThread(); &#125;&#125; 7.桟内存溢出本例用于体验StackOverFlow异常，容易让电脑假死VM Args：-Xss128k1234567891011121314151617181920package com.outofmemory;public class JavaVMStackSOF &#123; private int stackLength = 1; public void stackLeak() &#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) throws Throwable &#123; JavaVMStackSOF oom = new JavaVMStackSOF(); try &#123; oom.stackLeak(); &#125; catch (Throwable e) &#123; System.out.println(\"stack length：\" + oom.stackLength); throw e; &#125; &#125;&#125;","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"luoxiao.cf/tags/JVM/"}]},{"title":"mybatis like相关问题","slug":"2018-04-10-mybatis-mybatis like相关问题","date":"2018-04-10T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/10/2018-04-10-mybatis-mybatis like相关问题/","link":"","permalink":"luoxiao.cf/2018/04/10/2018-04-10-mybatis-mybatis like相关问题/","excerpt":"","text":"Mybatis 中关于like的使用使用字符串传值我们要使用#{0}#{1}#{2去匹配传入的每个参数},因为带like,所以在传入值的时候要在名字左右拼上%号，像这样’%tom%’,而不是’tom’，还有一点就是一定注意传入的顺序Mapper.java1public List&lt;BdipChatPoint&gt; selectListByPage(String modelUrl,String username);Mapper.xml12345&lt;select id=\"selectListByPage\" parameterType=\"java.lang.String\" resultMap=\"pointMap\"&gt; select * from bdip_chat_point where tree_id = #&#123;0&#125; and user_name like #&#123;1&#125;&lt;/select&gt;main.java12mapper.selectListByPage(\"1\",\"%tom%\");有的人在Mapper.xml可能会这样写,是取不到值的,下面是错误演示12345&lt;select id=\"selectListByPage\" parameterType=\"java.lang.String\" resultMap=\"pointMap\"&gt; select * from bdip_chat_point where tree_id = #&#123;modelUrl&#125; and user_name like #&#123;username&#125; &lt;/select&gt;如果我们就想使用这种方法查,要怎么办呢？可以用Map封装参数，看一下使用Map传值2.使用Map传值Mapper.java1public List&lt;BdipChatPoint&gt; selectListByPage(Map map);Mapper.xml123456&lt;select id=\"selectListByPage\" parameterType=\"Map\" resultMap=\"pointMap\"&gt; select * from bdip_chat_point where tree_id = #&#123;modelUrl&#125; and user_name like #&#123;username&#125; &lt;/select&gt;调用的时候初始化map对象1234Map map = new HashMap();map.put(\"modelUrl\", \"624\");map.put(\"username\", \"%tom%\");需要注意的是like #{username}和map.put(&quot;username&quot;, &quot;%tom%&quot;);如果你不想去拼%号,怎么办？我们可以这样子写。Mapper.xml123456&lt;select id=\"selectListByPage\" parameterType=\"Map\" resultMap=\"pointMap\"&gt; select * from bdip_chat_point where tree_id = #&#123;modelUrl&#125; and user_name like '%$&#123;username&#125;%' &lt;/select&gt;1234Map map = new HashMap();map.put(\"modelUrl\", \"624\");map.put(\"username\", \"tom\");good luck!~","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"luoxiao.cf/tags/Mybatis/"}]},{"title":"各种数据库分页","slug":"2018-04-09-分页-paging各种分页","date":"2018-04-09T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/09/2018-04-09-分页-paging各种分页/","link":"","permalink":"luoxiao.cf/2018/04/09/2018-04-09-分页-paging各种分页/","excerpt":"","text":"sqlserver分页 辅助类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.utils;import java.util.Collections;import java.util.List;/** * 分页辅助类 * @author ZhaoYihao * */public class Page &#123; /** * 当前页码 */ private int pageNum; /** * 开始条数 */ private int startNum; /** * 结束条数 */ private int endNum; /** * 每页条数 */ private int recordPerPage = 10; /** * 总页数 */ private int pageCount; /** * 查询数据列表 */ private List records = Collections.emptyList(); public Page() &#123;&#125; public Page(int pageNum) &#123; init(pageNum); &#125; private void init(int pageNum2) &#123; this.startNum = ((pageNum2-1)*recordPerPage)+1; this.endNum = startNum + recordPerPage; &#125; public int getPageNum() &#123; return pageNum; &#125; public void setPageNum(int pageNum) &#123; this.pageNum = pageNum; &#125; public int getStartNum() &#123; return startNum; &#125; public void setStartNum(int startNum) &#123; this.startNum = startNum; &#125; public int getEndNum() &#123; return endNum; &#125; public void setEndNum(int endNum) &#123; this.endNum = endNum; &#125; public int getRecordPerPage() &#123; return recordPerPage; &#125; public void setRecordPerPage(int recordPerPage) &#123; this.recordPerPage = recordPerPage; &#125; public int getPageCount() &#123; return pageCount; &#125; public void setPageCount(int pageCount) &#123; this.pageCount = pageCount; &#125; public List getRecords() &#123; return records; &#125; public void setRecords(List records) &#123; this.records = records; &#125; &#125;需要从前台传过来当前所在页面。后台初始化Page对象,将当前页面set进实体类,自动初始化其他信息。 sqlserver12345SELECT * FROM ( SELECT row_number() over(order by create_time) rownum,A.* FROM ( select * from bdip_chat_point where tree_id = '624' and user_name like '%杨%' )A)B where rownum between 1 AND 2; oracle方案一:1234select u_name,u_salfrom (select users.*,rownum rn from users where rownum &lt;= end)where rn &gt; start方案二:1234select u_name,u_salfrom (select users.*,rownum rn from users)where rn between start and end方案一效率更高!!! mysql使用limit 完成分页","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"ActiveMQ使用","slug":"2018-04-08-activemq-ActiveMQ使用","date":"2018-04-08T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/08/2018-04-08-activemq-ActiveMQ使用/","link":"","permalink":"luoxiao.cf/2018/04/08/2018-04-08-activemq-ActiveMQ使用/","excerpt":"","text":"ActiveMQ 是一个apache提供的一个消息队列服务,为我们在生产中提供了一个很好的服务器,我们不需要使用Java BloingQueue,使用这个服务就能很好的应对各种生产场景。 下载我们需要到apache官网上下载ActiveMQ,官网地址http://activemq.apache.org/activemq-5145-release.html,我这边是ubuntu操作系统,下载尾缀是tar.gz,如果是windows下载zip 配置activemq.xml解压下好的软件进入apache-activemq-5.14.5/conf找到activemq.xml,将其中的0.0.0.0修改成127.0.0.112345678 &lt;transportConnectors&gt; &lt;transportConnector name=\"openwire\" uri=\"tcp://localhost:61616\"/&gt; &lt;transportConnector name=\"ssl\" uri=\"ssl://localhost:61617\"/&gt; &lt;transportConnector name=\"stomp\" uri=\"stomp://localhost:61613\"/&gt; &lt;transportConnector uri=\"http://localhost:8081\"/&gt; &lt;transportConnector uri=\"udp://localhost:61618\"/&gt;&lt;/transportConnectors&gt; 启动进入到apache-activemq-5.14.5/bin 目录下执行命令$ ./activemq start打印下列语句就是开启成功了访问localhost:8161/admin。 简单的Demo既然队列跑起来了，我们编写一个简单的Java程序往队列里面放几个东西，玩玩…写一个生产者和消费者的小例子.Sender.java1234567891011121314151617181920212223242526272829303132333435363738394041package com.bim.activemq;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;public class Sender &#123; private static ConnectionFactory factory; private static Connection connection; private static Session session; private static Destination destination; private static MessageProducer producer; public static void main(String[] args) &#123; // 1.创建工厂 factory = new ActiveMQConnectionFactory(ActiveMQConnection.DEFAULT_USER,ActiveMQConnection.DEFAULT_PASSWORD, \"tcp://localhost:61616\"); try &#123; // 2.获取Connection connection = factory.createConnection(); connection.start(); // 3.获取session session = connection.createSession(true,Session.AUTO_ACKNOWLEDGE); // 4.发消息 4.1指定消息的目的地 destination 4.2 指定接受人 consumer destination = session.createQueue(\"foo\"); producer = session.createProducer(destination); for (int i = 0; i &lt; 5; i++) &#123; producer.send(destination,session.createTextMessage(\"新消息\" + i)); &#125; session.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;这个是生产者,往队列中存值,这边我们为队列里面存放了5条消息,分别存于foo节点下.,我们在运行它之前先查看一下队列的样子。Queues中是空的什么都没有,我们运行代码,往队列中存入值我们可以点进去查看详情Consumer.java12345678910111213141516171819202122232425262728293031323334package com.bim.activemq;import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;public class Consumer &#123; public static void main(String[] args) &#123; // 1.创建工厂 ConnectionFactory factory = new ActiveMQConnectionFactory(ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, \"tcp://127.0.0.1:61616\"); try &#123; // 2. 创建链接 Connection connection = factory.createConnection(); connection.start(); // 3.创建session 参数详解https://www.cnblogs.com/MIC2016/p/6086321.html Session session = connection.createSession(true,Session.AUTO_ACKNOWLEDGE); // 4.创建producer生产者发消息 Destination destination = session.createQueue(\"foo\"); MessageConsumer consumer = session.createConsumer(destination); while(true)&#123; TextMessage message = (TextMessage) consumer.receive(); System.out.println(message.getText()); &#125; &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125;这个是消费程序,每当队列里面存放一条消息后,该程序就会去队列中取数据了!!!","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"luoxiao.cf/tags/ActiveMQ/"}]},{"title":"Javaweb实现云端回收站功能","slug":"2018-04-02-Javaweb云端回收站功能总结","date":"2018-04-02T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/04/02/2018-04-02-Javaweb云端回收站功能总结/","link":"","permalink":"luoxiao.cf/2018/04/02/2018-04-02-Javaweb云端回收站功能总结/","excerpt":"","text":"最近接手JavaWeb回收站功能,业务场景：在云端为用户删除模型操作提供一个回收站功能，主要业务。模型删除、模型找回。下面只梳理业务逻辑 关系表主要业务牵扯表有这几张:bim_space_manager(用户空间)、bim_model_tree_trash(回收站)和bim_model_tree(模型)bim_space_manager 可以理解为百度网盘的用户空间。bim_model_tree 云端存放模型的主要表bim_model_tree_trash 删除记录 字段详述表字段说明:bim_model_tree_trashenable_:0 回收站记录弃用(同时表示模型记录启用,很绕…主要为了回收站的找回逻辑) 1 回收站启用(同时表示模型记录被逻辑删除) 2 对应模型记录被永久删除user_id:模型所有者的idtree_id:被删除的模型id 与bim_model_tree 中的id 一对一关系left_days:回收站中初始保留7天,后台每天自动检测该字段,并更新生于天数,出现负数时,即表示该模型已经被物理删除,不会再显示在回收站中model_size:该节点文件的大小 单位KB表字段说明:bim_space_managerenable_: 在新建用户为其初始化默认空间,默认为1,用户注销时为0user_id:用户id,与用户表中的用户一一对应space_total:用户初始化可用空间大小 2048 单位MB 计算方式:space_size*1024space_used:已经使用的size 单位MBspace_size: 用户空间等级,初始化为2 (在user_space_size.properties文件中的space_size_tool配置)user_type: 用户空间使用类型 默认tool 工具级 主要业务梳理回收站记录与模型节点记录一一对应,通过tree_id关联 1.删除文件用户删除文件时,先将bim_model_tree中记录enable设为0,再在bim_model_tree_trash中创建记录,创建之前会查重是否已经有该记录,有的话,将trash中enable变为1,否则新建trash记录 2.找回文件：用户从回收站中找回节点，先删除trash表中对应记录，然后判断该节点是文件还是文件夹。如果是文件的话，先查询用户空间表的剩余空间是否够用，够用的话直接重新启用model_tree中对应记录，同时启用该节点所有的父祖节点，同时更新用户空间表。如果是文件夹的话，先查出其下所有子节点，然后排除其中所有之前删除过所有节点。先将其全部恢复，然后查询用户空间是否超过额定大小，超出的话回滚所有操作，并提示剩余空间不足。 3.彻底删除文件：即删除trash中的记录，说是物理删除也并非真的物理删除。而是通过修改status字段为2 进行彻底删除。文件进入回收站后，定时7天，期间用户可找回，也可彻底删除。超期则不会再显示trash记录。同时后台有定时任务，每天扫描一次trash表，定时修改left_day ，当剩余天数不大于0时，也不会再显示trash记录，算是彻底删除文件","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"wangEditor粘贴图片上传","slug":"2018-03-27-wangEditor-wangEditor图片粘贴上传","date":"2018-03-27T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/27/2018-03-27-wangEditor-wangEditor图片粘贴上传/","link":"","permalink":"luoxiao.cf/2018/03/27/2018-03-27-wangEditor-wangEditor图片粘贴上传/","excerpt":"","text":"wangEditor粘贴图片上传功能 思路使用paste.js辅助上传,开源的github地址 https://github.com/layerssss/paste.js/如果需要的话可以试用提供的压缩工具对base64进行压缩后上传,在这边没压缩的图片10M,压缩完200KB,还是推荐一用。初始化富文本编辑器定位编辑器的textarea div框,为其绑定paste事件解析粘贴板中的数据,数据有可能不是image,所以要做判断转成base64编码，获取一个图压缩图片，封装FormData对象使用ajax提交数据到后台处理后台返回上传后的图片路径，路径可以是绝对路径也可以是相对路径,但必须是在浏览器的url中可以访问的在ajax success回调中动态为编辑器的textarea div框append一个标签注意初始化那块的代码，我这里业务需要在一个页面初始化多个Editor,所以我写在循环里了,需要根据代码，结合自身业务做适当修改 代码 workflow-collision.js我主要的业务在这个js里面123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160function initTextarea(nodes,createBy)&#123; var currentUserId = BimBdip.currentUserid; var divId = \"\"; var textId = \"\"; var E = window.wangEditor var menu1 = [ ]; if(gloubleVariable.status == '0' &amp;&amp; currentUserId == createBy || typeof(createBy) == 'undefined')&#123; menu1 = [ 'head', // 标题 'bold', // 粗体 'italic', // 斜体 'underline', // 下划线 'strikeThrough', // 删除线 'foreColor', // 文字颜色 'backColor', // 背景颜色 'list', // 列表 'justify', // 对齐方式 'emoticon', // 表情 'image', // 插入图片 'undo', // 撤销 ]; &#125; nodes.forEach((node,index)=&gt;&#123; divId = \"twoDimesional-div-\"+index; var editor = new E('#'+divId); textId = \"twoDimesional-text-\"+index; var $text = $('#' + textId); editor.customConfig.onchange = function (html) &#123; // 监控变化，同步更新到 textarea $text.val(html) &#125; editor.customConfig.uploadImgServer = publicJS.tomcat_url + '/workflow/uploadTwoDimesionalImgage.action'; editor.customConfig.uploadFileName = 'files'; editor.customConfig.menus = menu1; editor.create(); $text.val(editor.txt.html()); if(gloubleVariable.status == '0' &amp;&amp; workflowCollision.currentListName != 'handle' &amp;&amp; workflowCollision.currentListName != 'cc')&#123; editor.$textElem.attr('contenteditable', true); &#125;else&#123; editor.$textElem.attr('contenteditable', false); &#125; var selector = '.w-e-text:eq('+index+')'; var a = $(selector); // 为每个节点框添加粘贴事件,依赖paste.js copyBindEvent(a[0]); &#125;)&#125;/** * 绑定粘贴事件 * @param _target需要绑定粘贴事件的js对象 * @returns */function copyBindEvent(_target)&#123; _target.addEventListener('paste', function (event) &#123; workflowCollision.operateEditorTarget = _target; // 获取浏览器类型 var broswerType = GetBrowserType(); if (broswerType == \"Chrome\") &#123; var clipboard = event.clipboardData; var blob = null; for (var i = 0; i &lt; event.clipboardData.items.length; i++) &#123; if (clipboard.items[i].type.indexOf(\"image\") != -1) &#123; blob = clipboard.items[i].getAsFile(); //转成base64编码，获取一个图片 var reader = new window.FileReader(); reader.readAsDataURL(blob); reader.onloadend = function () &#123; // 压缩图片 discussCompress(reader.result); &#125; &#125; else if (clipboard.items[i].type.indexOf(\"html\") != -1) &#123; var html = $(event.clipboardData.getData(\"text/html\")); html.find(\"img\").each(function () &#123; var canvas = document.createElement(\"canvas\"); var context = canvas.getContext(\"2d\"); var img = document.createElement(\"img\"); var src = $(this).attr(\"src\"); var _this = this; img.src = src; var imageS = new Image(); context.drawImage(img, 0, 0, img.width, img.height); imageS.src = canvas.toDataURL(\"image/png\"); // 压缩图片 discussCompress(imageS.src); &#125;); &#125; &#125; &#125; //阻止默认粘贴事件// event.originalEvent.preventDefault(); &#125;);&#125;// 图片压缩function discussCompress(base64code)&#123; var fd = new FormData(); //压缩图片 compressImg(base64code,450,250,function(base64code)&#123; var blob = dataURLToBlob(base64code); if (blob !== null || blob !== undefined || blob !== '') &#123; fd.append(\"files\",blob); // 上传服务器 copyDoAjax(fd); &#125;else&#123; Dialog.alert(\"上传图片失败,请重试!\"); &#125; &#125;);&#125;/** * 上传服务器 * @param fd * @returns */function copyDoAjax(fd) &#123; var url = publicJS.tomcat_url + '/workflow/uploadTwoDimesionalImgage.action'; $.ajax(&#123; url:url, type:'POST', data:fd, processData:false, contentType:false, dataType : \"json\", //jsonp: \"jsonpCallBack\", success: function (data, status) &#123;//操作成功后的操作！data是后台传过来的值 //console.log(\"上传图片成功\"); //往富文本框添加图片 &lt;img src=\"http://test-file.bimbdip.com/discuss/1522142071651.tmp\" style=\"max-width:100%;\"&gt; var html = `&lt;img src=\"$&#123;data.data&#125;\" style=\"max-width:100%;\"&gt;`; $(workflowCollision.operateEditorTarget).append(html); &#125;, error: function (xhr, textStatus, errorThrown) &#123; alert(\"上传图片失败\"); &#125; &#125;);&#125;// 获取浏览器类型function GetBrowserType() &#123; var userAgent = navigator.userAgent; var isOpera = userAgent.indexOf(\"Opera\") &gt; -1; if (isOpera) &#123; return \"Opera\" &#125; ; //判断是否Opera浏览器 if (userAgent.indexOf(\"Firefox\") &gt; -1) &#123; return \"FF\"; &#125; //判断是否Firefox浏览器 if (userAgent.indexOf(\"Chrome\") &gt; -1) &#123; return \"Chrome\"; &#125; if (userAgent.indexOf(\"Safari\") &gt; -1) &#123; return \"Safari\"; &#125; //判断是否Safari浏览器 return \"IE\";//其他的就当IE吧&#125; 压缩的相关库megapix-image.js 开源库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263/** * Mega pixel image rendering library for iOS6 Safari * * Fixes iOS6 Safari's image file rendering issue for large size image (over mega-pixel), * which causes unexpected subsampling when drawing it in canvas. * By using this library, you can safely render the image with proper stretching. * * Copyright (c) 2012 Shinichi Tomita &lt;shinichi.tomita@gmail.com&gt; * Released under the MIT license */(function() &#123; /** * Detect subsampling in loaded image. * In iOS, larger images than 2M pixels may be subsampled in rendering. */ function detectSubsampling(img) &#123; var iw = img.naturalWidth, ih = img.naturalHeight; if (iw * ih &gt; 1024 * 1024) &#123; // subsampling may happen over megapixel image var canvas = document.createElement('canvas'); canvas.width = canvas.height = 1; var ctx = canvas.getContext('2d'); ctx.drawImage(img, -iw + 1, 0); // subsampled image becomes half smaller in rendering size. // check alpha channel value to confirm image is covering edge pixel or not. // if alpha value is 0 image is not covering, hence subsampled. return ctx.getImageData(0, 0, 1, 1).data[3] === 0; &#125; else &#123; return false; &#125; &#125; /** * Detecting vertical squash in loaded image. * Fixes a bug which squash image vertically while drawing into canvas for some images. */ function detectVerticalSquash(img, iw, ih) &#123; var canvas = document.createElement('canvas'); canvas.width = 1; canvas.height = ih; var ctx = canvas.getContext('2d'); ctx.drawImage(img, 0, 0); var data = ctx.getImageData(0, 0, 1, ih).data; // search image edge pixel position in case it is squashed vertically. var sy = 0; var ey = ih; var py = ih; while (py &gt; sy) &#123; var alpha = data[(py - 1) * 4 + 3]; if (alpha === 0) &#123; ey = py; &#125; else &#123; sy = py; &#125; py = (ey + sy) &gt;&gt; 1; &#125; var ratio = (py / ih); return (ratio===0)?1:ratio; &#125; /** * Rendering image element (with resizing) and get its data URL */ function renderImageToDataURL(img, options, doSquash) &#123; var canvas = document.createElement('canvas'); renderImageToCanvas(img, canvas, options, doSquash); return canvas.toDataURL(\"image/jpeg\", options.quality || 0.8); &#125; /** * Rendering image element (with resizing) into the canvas element */ function renderImageToCanvas(img, canvas, options, doSquash) &#123; var iw = img.naturalWidth, ih = img.naturalHeight; if (!(iw+ih)) return; var width = options.width, height = options.height; var ctx = canvas.getContext('2d'); ctx.save(); transformCoordinate(canvas, ctx, width, height, options.orientation); var subsampled = detectSubsampling(img); if (subsampled) &#123; iw /= 2; ih /= 2; &#125; var d = 1024; // size of tiling canvas var tmpCanvas = document.createElement('canvas'); tmpCanvas.width = tmpCanvas.height = d; var tmpCtx = tmpCanvas.getContext('2d'); var vertSquashRatio = doSquash ? detectVerticalSquash(img, iw, ih) : 1; var dw = Math.ceil(d * width / iw); var dh = Math.ceil(d * height / ih / vertSquashRatio); var sy = 0; var dy = 0; while (sy &lt; ih) &#123; var sx = 0; var dx = 0; while (sx &lt; iw) &#123; tmpCtx.clearRect(0, 0, d, d); tmpCtx.drawImage(img, -sx, -sy); ctx.drawImage(tmpCanvas, 0, 0, d, d, dx, dy, dw, dh); sx += d; dx += dw; &#125; sy += d; dy += dh; &#125; ctx.restore(); tmpCanvas = tmpCtx = null; &#125; /** * Transform canvas coordination according to specified frame size and orientation * Orientation value is from EXIF tag */ function transformCoordinate(canvas, ctx, width, height, orientation) &#123; switch (orientation) &#123; case 5: case 6: case 7: case 8: canvas.width = height; canvas.height = width; break; default: canvas.width = width; canvas.height = height; &#125; switch (orientation) &#123; case 2: // horizontal flip ctx.translate(width, 0); ctx.scale(-1, 1); break; case 3: // 180 rotate left ctx.translate(width, height); ctx.rotate(Math.PI); break; case 4: // vertical flip ctx.translate(0, height); ctx.scale(1, -1); break; case 5: // vertical flip + 90 rotate right ctx.rotate(0.5 * Math.PI); ctx.scale(1, -1); break; case 6: // 90 rotate right ctx.rotate(0.5 * Math.PI); ctx.translate(0, -height); break; case 7: // horizontal flip + 90 rotate right ctx.rotate(0.5 * Math.PI); ctx.translate(width, -height); ctx.scale(-1, 1); break; case 8: // 90 rotate left ctx.rotate(-0.5 * Math.PI); ctx.translate(-width, 0); break; default: break; &#125; &#125; var URL = window.URL &amp;&amp; window.URL.createObjectURL ? window.URL : window.webkitURL &amp;&amp; window.webkitURL.createObjectURL ? window.webkitURL : null; /** * MegaPixImage class */ function MegaPixImage(srcImage) &#123; if (window.Blob &amp;&amp; srcImage instanceof Blob) &#123; if (!URL) &#123; throw Error(\"No createObjectURL function found to create blob url\"); &#125; var img = new Image(); img.src = URL.createObjectURL(srcImage); this.blob = srcImage; srcImage = img; &#125; if (!srcImage.naturalWidth &amp;&amp; !srcImage.naturalHeight) &#123; var _this = this; srcImage.onload = srcImage.onerror = function() &#123; var listeners = _this.imageLoadListeners; if (listeners) &#123; _this.imageLoadListeners = null; for (var i=0, len=listeners.length; i&lt;len; i++) &#123; listeners[i](); &#125; &#125; &#125;; this.imageLoadListeners = []; &#125; this.srcImage = srcImage; &#125; /** * Rendering megapix image into specified target element */ MegaPixImage.prototype.render = function(target, options, callback) &#123; if (this.imageLoadListeners) &#123; var _this = this; this.imageLoadListeners.push(function() &#123; _this.render(target, options, callback); &#125;); return; &#125; options = options || &#123;&#125;; var imgWidth = this.srcImage.naturalWidth, imgHeight = this.srcImage.naturalHeight, width = options.width, height = options.height, maxWidth = options.maxWidth, maxHeight = options.maxHeight, doSquash = !this.blob || this.blob.type === 'image/jpeg'; if (width &amp;&amp; !height) &#123; height = (imgHeight * width / imgWidth) &lt;&lt; 0; &#125; else if (height &amp;&amp; !width) &#123; width = (imgWidth * height / imgHeight) &lt;&lt; 0; &#125; else &#123; width = imgWidth; height = imgHeight; &#125; if (maxWidth &amp;&amp; width &gt; maxWidth) &#123; width = maxWidth; height = (imgHeight * width / imgWidth) &lt;&lt; 0; &#125; if (maxHeight &amp;&amp; height &gt; maxHeight) &#123; height = maxHeight; width = (imgWidth * height / imgHeight) &lt;&lt; 0; &#125; var opt = &#123; width : width, height : height &#125;; for (var k in options) opt[k] = options[k]; var tagName = target.tagName.toLowerCase(); if (tagName === 'img') &#123; target.src = renderImageToDataURL(this.srcImage, opt, doSquash); &#125; else if (tagName === 'canvas') &#123; renderImageToCanvas(this.srcImage, target, opt, doSquash); &#125; if (typeof this.onrender === 'function') &#123; this.onrender(target); &#125; if (callback) &#123; callback(); &#125; if (this.blob) &#123; this.blob = null; URL.revokeObjectURL(this.srcImage.src); &#125; &#125;; /** * Export class to global */ if (typeof define === 'function' &amp;&amp; define.amd) &#123; define([], function() &#123; return MegaPixImage; &#125;); // for AMD loader &#125; else if (typeof exports === 'object') &#123; module.exports = MegaPixImage; // for CommonJS &#125; else &#123; this.MegaPixImage = MegaPixImage; &#125;&#125;)();compress-image.js 对开源库做了简单封装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576//************压缩图片************************************************************/** * 压缩图片 * @param base64code //被压缩图片的base64编码 * @param maxWidth //压缩后图片最大宽度 * @param maxHeight //压缩后图片的最大高度 * @param callback(dataUrl) 压缩回调方法,参数为压缩后的url，若压缩失败，则返回\"\"; * @returns */function compressImg(base64code,maxWidth,maxHeight,callback) &#123; //var base64code = \"\"; var image = new Image(); image.src = base64code; var dataURL = \"\"; imgLoad(image, function() &#123; //解决canvas无法读取画布问题 image.setAttribute('crossOrigin', 'anonymous'); var mpImg = new MegaPixImage(image); //console.log('加载完毕') var resCanvas2 =document.createElement(\"canvas\"); if(isIOS())&#123; mpImg.render(resCanvas2, &#123;maxWidth: maxWidth, maxHeight: maxHeight, orientation: 4&#125;); &#125;else&#123; mpImg.render(resCanvas2, &#123;maxWidth: maxWidth, maxHeight: maxHeight, orientation: 0&#125;); &#125; dataURL = resCanvas2.toDataURL(\"image/jpeg\"); callback(dataURL); &#125;); // return dataURL;&#125;function imgLoad(img, callback) &#123; var timer = setInterval(function() &#123; if (img.complete) &#123; callback(img) clearInterval(timer) &#125; &#125;, 50)&#125;//********************************************************************************************function isIOS()&#123; var userAgentInfo = navigator.userAgent; var Agents = new Array(\"iPhone\",\"iPad\", \"iPod\"); /*\"Android\", */ var flag = false; var v=0 for ( v = 0; v &lt; Agents.length; v++)&#123; if (userAgentInfo.indexOf(Agents[v]) &gt; 0) &#123; flag = true; break; &#125; &#125; return flag;&#125;//********************************************************************************************function dataURLToBlob(dataURL) &#123; var BASE64_MARKER = ';base64,'; var parts, contentType, raw; if (dataURL.indexOf(BASE64_MARKER) == -1) &#123; parts = dataURL.split(','); contentType = parts[0].split(':')[1]; raw = decodeURIComponent(parts[1]); return new Blob([raw], &#123;type: contentType&#125;); &#125; parts = dataURL.split(BASE64_MARKER); contentType = parts[0].split(':')[1]; raw = window.atob(parts[1]); var rawLength = raw.length; var uInt8Array = new Uint8Array(rawLength); for (var i = 0; i &lt; rawLength; ++i) &#123; uInt8Array[i] = raw.charCodeAt(i); &#125; return new Blob([uInt8Array], &#123;type: contentType&#125;);&#125; 粘贴库paste.js 开源项目123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390// 源地址 https://github.com/layerssss/paste.js/(function() &#123; var $, Paste, createHiddenEditable, dataURLtoBlob, isFocusable; $ = window.jQuery; $.paste = function(pasteContainer) &#123; var pm; if (typeof console !== \"undefined\" &amp;&amp; console !== null) &#123; console.log(\"DEPRECATED: This method is deprecated. Please use $.fn.pastableNonInputable() instead.\"); &#125; pm = Paste.mountNonInputable(pasteContainer); return pm._container; &#125;; $.fn.pastableNonInputable = function() &#123; var el, j, len, ref; ref = this; for (j = 0, len = ref.length; j &lt; len; j++) &#123; el = ref[j]; if (el._pastable || $(el).is('textarea, input:text, [contenteditable]')) &#123; continue; &#125; Paste.mountNonInputable(el); el._pastable = true; &#125; return this; &#125;; $.fn.pastableTextarea = function() &#123; var el, j, len, ref; ref = this; for (j = 0, len = ref.length; j &lt; len; j++) &#123; el = ref[j]; if (el._pastable || $(el).is(':not(textarea, input:text)')) &#123; continue; &#125; Paste.mountTextarea(el); el._pastable = true; &#125; return this; &#125;; $.fn.pastableContenteditable = function() &#123; var el, j, len, ref; ref = this; for (j = 0, len = ref.length; j &lt; len; j++) &#123; el = ref[j]; if (el._pastable || $(el).is(':not([contenteditable])')) &#123; continue; &#125; Paste.mountContenteditable(el); el._pastable = true; &#125; return this; &#125;; dataURLtoBlob = function(dataURL, sliceSize) &#123; var b64Data, byteArray, byteArrays, byteCharacters, byteNumbers, contentType, i, m, offset, ref, slice; if (sliceSize == null) &#123; sliceSize = 512; &#125; if (!(m = dataURL.match(/^data\\:([^\\;]+)\\;base64\\,(.+)$/))) &#123; return null; &#125; ref = m, m = ref[0], contentType = ref[1], b64Data = ref[2]; byteCharacters = atob(b64Data); byteArrays = []; offset = 0; while (offset &lt; byteCharacters.length) &#123; slice = byteCharacters.slice(offset, offset + sliceSize); byteNumbers = new Array(slice.length); i = 0; while (i &lt; slice.length) &#123; byteNumbers[i] = slice.charCodeAt(i); i++; &#125; byteArray = new Uint8Array(byteNumbers); byteArrays.push(byteArray); offset += sliceSize; &#125; return new Blob(byteArrays, &#123; type: contentType &#125;); &#125;; createHiddenEditable = function() &#123; return $(document.createElement('div')).attr('contenteditable', true).attr('aria-hidden', true).attr('tabindex', -1).css(&#123; width: 1, height: 1, position: 'fixed', left: -100, overflow: 'hidden' &#125;); &#125;; isFocusable = function(element, hasTabindex) &#123; var fieldset, focusableIfVisible, img, map, mapName, nodeName; map = void 0; mapName = void 0; img = void 0; focusableIfVisible = void 0; fieldset = void 0; nodeName = element.nodeName.toLowerCase(); if ('area' === nodeName) &#123; map = element.parentNode; mapName = map.name; if (!element.href || !mapName || map.nodeName.toLowerCase() !== 'map') &#123; return false; &#125; img = $('img[usemap=\\'#' + mapName + '\\']'); return img.length &gt; 0 &amp;&amp; img.is(':visible'); &#125; if (/^(input|select|textarea|button|object)$/.test(nodeName)) &#123; focusableIfVisible = !element.disabled; if (focusableIfVisible) &#123; fieldset = $(element).closest('fieldset')[0]; if (fieldset) &#123; focusableIfVisible = !fieldset.disabled; &#125; &#125; &#125; else if ('a' === nodeName) &#123; focusableIfVisible = element.href || hasTabindex; &#125; else &#123; focusableIfVisible = hasTabindex; &#125; focusableIfVisible = focusableIfVisible || $(element).is('[contenteditable]'); return focusableIfVisible &amp;&amp; $(element).is(':visible'); &#125;; Paste = (function() &#123; Paste.prototype._target = null; Paste.prototype._container = null; Paste.mountNonInputable = function(nonInputable) &#123; var paste; paste = new Paste(createHiddenEditable().appendTo(nonInputable), nonInputable); $(nonInputable).on('click', (function(_this) &#123; return function(ev) &#123; if (!isFocusable(ev.target, false)) &#123; return paste._container.focus(); &#125; &#125;; &#125;)(this)); paste._container.on('focus', (function(_this) &#123; return function() &#123; return $(nonInputable).addClass('pastable-focus'); &#125;; &#125;)(this)); return paste._container.on('blur', (function(_this) &#123; return function() &#123; return $(nonInputable).removeClass('pastable-focus'); &#125;; &#125;)(this)); &#125;; Paste.mountTextarea = function(textarea) &#123; var ctlDown, paste, ref, ref1; if ((typeof DataTransfer !== \"undefined\" &amp;&amp; DataTransfer !== null ? DataTransfer.prototype : void 0) &amp;&amp; ((ref = Object.getOwnPropertyDescriptor) != null ? (ref1 = ref.call(Object, DataTransfer.prototype, 'items')) != null ? ref1.get : void 0 : void 0)) &#123; return this.mountContenteditable(textarea); &#125; paste = new Paste(createHiddenEditable().insertBefore(textarea), textarea); ctlDown = false; $(textarea).on('keyup', function(ev) &#123; var ref2; if ((ref2 = ev.keyCode) === 17 || ref2 === 224) &#123; ctlDown = false; &#125; return null; &#125;); $(textarea).on('keydown', function(ev) &#123; var ref2; if ((ref2 = ev.keyCode) === 17 || ref2 === 224) &#123; ctlDown = true; &#125; if ((ev.ctrlKey != null) &amp;&amp; (ev.metaKey != null)) &#123; ctlDown = ev.ctrlKey || ev.metaKey; &#125; if (ctlDown &amp;&amp; ev.keyCode === 86) &#123; paste._textarea_focus_stolen = true; paste._container.focus(); paste._paste_event_fired = false; setTimeout((function(_this) &#123; return function() &#123; if (!paste._paste_event_fired) &#123; $(textarea).focus(); return paste._textarea_focus_stolen = false; &#125; &#125;; &#125;)(this), 1); &#125; return null; &#125;); $(textarea).on('paste', (function(_this) &#123; return function() &#123;&#125;; &#125;)(this)); $(textarea).on('focus', (function(_this) &#123; return function() &#123; if (!paste._textarea_focus_stolen) &#123; return $(textarea).addClass('pastable-focus'); &#125; &#125;; &#125;)(this)); $(textarea).on('blur', (function(_this) &#123; return function() &#123; if (!paste._textarea_focus_stolen) &#123; return $(textarea).removeClass('pastable-focus'); &#125; &#125;; &#125;)(this)); $(paste._target).on('_pasteCheckContainerDone', (function(_this) &#123; return function() &#123; $(textarea).focus(); return paste._textarea_focus_stolen = false; &#125;; &#125;)(this)); return $(paste._target).on('pasteText', (function(_this) &#123; return function(ev, data) &#123; var content, curEnd, curStart; curStart = $(textarea).prop('selectionStart'); curEnd = $(textarea).prop('selectionEnd'); content = $(textarea).val(); $(textarea).val(\"\" + content.slice(0, curStart) + data.text + content.slice(curEnd)); $(textarea)[0].setSelectionRange(curStart + data.text.length, curStart + data.text.length); return $(textarea).trigger('change'); &#125;; &#125;)(this)); &#125;; Paste.mountContenteditable = function(contenteditable) &#123; var paste; paste = new Paste(contenteditable, contenteditable); $(contenteditable).on('focus', (function(_this) &#123; return function() &#123; return $(contenteditable).addClass('pastable-focus'); &#125;; &#125;)(this)); return $(contenteditable).on('blur', (function(_this) &#123; return function() &#123; return $(contenteditable).removeClass('pastable-focus'); &#125;; &#125;)(this)); &#125;; function Paste(_container, _target) &#123; this._container = _container; this._target = _target; this._container = $(this._container); this._target = $(this._target).addClass('pastable'); this._container.on('paste', (function(_this) &#123; return function(ev) &#123; var clipboardData, file, item, j, k, len, len1, reader, ref, ref1, ref2, ref3, text; if (ev.currentTarget !== ev.target) &#123; return ev.preventDefault(); &#125; _this._paste_event_fired = true; if (((ref = ev.originalEvent) != null ? ref.clipboardData : void 0) != null) &#123; clipboardData = ev.originalEvent.clipboardData; if (clipboardData.items) &#123; ref1 = clipboardData.items; for (j = 0, len = ref1.length; j &lt; len; j++) &#123; item = ref1[j]; if (item.type.match(/^image\\//)) &#123; reader = new FileReader(); reader.onload = function(event) &#123; return _this._handleImage(event.target.result); &#125;; try &#123; reader.readAsDataURL(item.getAsFile()); &#125; catch (error) &#123;&#125; ev.preventDefault(); break; &#125; if (item.type === 'text/plain') &#123; item.getAsString(function(string) &#123; return _this._target.trigger('pasteText', &#123; text: string &#125;); &#125;); &#125; &#125; &#125; else &#123; if (-1 !== Array.prototype.indexOf.call(clipboardData.types, 'text/plain')) &#123; text = clipboardData.getData('Text'); setTimeout(function() &#123; return _this._target.trigger('pasteText', &#123; text: text &#125;); &#125;, 1); &#125; _this._checkImagesInContainer(function(src) &#123; return _this._handleImage(src); &#125;); &#125; &#125; if (clipboardData = window.clipboardData) &#123; if ((ref2 = (text = clipboardData.getData('Text'))) != null ? ref2.length : void 0) &#123; setTimeout(function() &#123; _this._target.trigger('pasteText', &#123; text: text &#125;); return _this._target.trigger('_pasteCheckContainerDone'); &#125;, 1); &#125; else &#123; ref3 = clipboardData.files; for (k = 0, len1 = ref3.length; k &lt; len1; k++) &#123; file = ref3[k]; _this._handleImage(URL.createObjectURL(file)); &#125; _this._checkImagesInContainer(function(src) &#123;&#125;); &#125; &#125; return null; &#125;; &#125;)(this)); &#125; Paste.prototype._handleImage = function(src) &#123; var loader; if (src.match(/^webkit\\-fake\\-url\\:\\/\\//)) &#123; return this._target.trigger('pasteImageError', &#123; message: \"You are trying to paste an image in Safari, however we are unable to retieve its data.\" &#125;); &#125; loader = new Image(); loader.crossOrigin = \"anonymous\"; loader.onload = (function(_this) &#123; return function() &#123; var blob, canvas, ctx, dataURL; canvas = document.createElement('canvas'); canvas.width = loader.width; canvas.height = loader.height; ctx = canvas.getContext('2d'); ctx.drawImage(loader, 0, 0, canvas.width, canvas.height); dataURL = null; try &#123; dataURL = canvas.toDataURL('image/png'); blob = dataURLtoBlob(dataURL); &#125; catch (error) &#123;&#125; if (dataURL) &#123; return _this._target.trigger('pasteImage', &#123; blob: blob, dataURL: dataURL, width: loader.width, height: loader.height &#125;); &#125; &#125;; &#125;)(this); loader.onerror = (function(_this) &#123; return function() &#123; return _this._target.trigger('pasteImageError', &#123; message: \"Failed to get image from: \" + src, url: src &#125;); &#125;; &#125;)(this); return loader.src = src; &#125;; Paste.prototype._checkImagesInContainer = function(cb) &#123; var img, j, len, ref, timespan; timespan = Math.floor(1000 * Math.random()); ref = this._container.find('img'); for (j = 0, len = ref.length; j &lt; len; j++) &#123; img = ref[j]; img[\"_paste_marked_\" + timespan] = true; &#125; return setTimeout((function(_this) &#123; return function() &#123; var k, len1, ref1; ref1 = _this._container.find('img'); for (k = 0, len1 = ref1.length; k &lt; len1; k++) &#123; img = ref1[k]; if (!img[\"_paste_marked_\" + timespan]) &#123; cb(img.src); $(img).remove(); &#125; &#125; return _this._target.trigger('_pasteCheckContainerDone'); &#125;; &#125;)(this), 1); &#125;; return Paste; &#125;)();&#125;).call(this);","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"Editor","slug":"Editor","permalink":"luoxiao.cf/tags/Editor/"}]},{"title":"Java workflow","slug":"2018-03-22-workflow-Java  workflow","date":"2018-03-22T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/22/2018-03-22-workflow-Java  workflow/","link":"","permalink":"luoxiao.cf/2018/03/22/2018-03-22-workflow-Java  workflow/","excerpt":"","text":"最近一直被工作流纠缠这，浪费了很多时间。对于一个流程的开发，完全可以手动写一个，何必要使用框架来完成这件事情呢？开始我也是这么想的，但是自打我尝试了之后才发现，这其实并不是一件简单的事情。下面是在开发过程中遇到了各种各样的困难，做一个总结 1.流程业务描述A --&gt; 发起碰撞报告报表 -&gt; B --&gt; 对报表进行审批 --&gt; A --&gt; 确认归档 2.生产每个活动点都需要开发交互界面、处理相关业务逻辑在当前活动点硬性判断下一活动点和相应的操作人每次操作都要维护业务、流程、流转的数据 3.结果梳理后台各种业务逻辑、维护数据更新以及编写各个交互界面，使流程的开发难度提升了不少。导致开发周期长，无法满足预期目标。 4.开源框架针对这一问题，市面上了解有六款Java Workflow 工作流框架OBE、Shark、OSWorkflow、JBPM、YAWL、Bossa。 使用框架的好处维护流程、流转等数据变得很容易框架提供了流程流转模型设计工具避免定死流程产生的硬编码降低开发风险，提高开发速度提高迭代开发的支持提供后台API，满足业务需求，应对各种需求变更深入了解各种开源框架 http://blog.csdn.net/SAM_XIE_52/article/details/79362080","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"ubuntu16.04TLS tar系统整机备份及恢复","slug":"2018-03-20-ubuntu-ubuntu16.04TLS tar系统备份恢复","date":"2018-03-20T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/20/2018-03-20-ubuntu-ubuntu16.04TLS tar系统备份恢复/","link":"","permalink":"luoxiao.cf/2018/03/20/2018-03-20-ubuntu-ubuntu16.04TLS tar系统备份恢复/","excerpt":"","text":"老实说，备份是系统损毁时的救星。事实上，没有人希望自己的系统损坏，往往由于不预期的伤害导致系统发生错误，比如之前我想在电脑上卸载一个依赖包，结果卸载以后，系统导致不能正常启动。不用慌，插上硬盘，执行一条命令电脑就恢复正常了！正是因为计算机是个相当不可靠的机器，所以我们要时常对它进行备份，进而来预防不测。 预言linux是一个文件系统，因此可以利用tar来备份。备份之前首先要了解哪些是我们需要备份的数据，哪些是我们不需要备份的数据。 排除的文件下面这些文件是我们不需要考虑备份的文件12345678910111213/proc/tmp/mnt/dev/sys/run /media /var/log #日志文件/var/cache/apt/archives #apt下载的任何.deb文件/usr/src/linux-headers*/home/*/.gvfs/home/*/.cache/home/*/.local/share/Trash 需要备份的文件下面这些文件是我们需要考虑备份的文件12345678/etc/home/var/spool/mail/boot//root#自己安装过的套件会存放在/user/local和/opt所以他俩也备份/user/local/opt 完整备份使用管理员身份，进入到根目录下运行tar命令这里有一点需要注意的是，第三条命令不是死的，你要灵去修改，假如你把备份的文件放在了根目录下，那么我们在tar的命令里面要再加上一句 --exclude=/备份的文件名上面这一点很重要，因为我们备份过的文件也是要排除的。如果忘了这一点可能会备份失败，我为什么没有加，是因为把备份文件放在/media目录下了.可能是因为界面样式问题 -- 有点像 - Orz$ sudo su$ cd /1tar --exclude=/home/zyh/Pictures/* --exclude=/proc --exclude=/tmp --exclude=/mnt --exclude=/dev --exclude=/sys --exclude=/run --exclude=/media --exclude=/var/log --exclude=/var/cache/apt/archives --exclude=/usr/src/linux-headers* --exclude=/home/*/.gvfs --exclude=/home/*/.cache --exclude=/home/*/.local/share/Trash -jcvp -f /media/zyh/software/ubuntubackup/system.tar.bz2 /–exclude 将会排除掉我们不希望备份的内容-j 压缩为bz2 能有一个更好的压缩比最后-f /media/zyh/software/ubuntubackup/system.tar.bz2 是我们要备份到哪个目录以及文件名字，后面的/表示压缩目录从根目录开始 恢复注意：恢复操作很危险，如果你不知道自己在做什么，将可能会导致系统数据丢失！我们将备份好的system.tar.bz2文件解压到根目录$ cd /media/zyh/software/ubuntubackup/$ tar -jxvf system.tar.bz2 -C /-j 类型为bz2-x 解压-v 显示解压过程-f 使用档名，请留意，在 f 之后要立即接档名通常我们把f放在最后参考blog:https://help.ubuntu.com/community/BackupYourSystem/TAR#Alternate_backup参考blog:http://linux.vbird.org/linux_basic/0580backup.php","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"Ubuntu 16.04TLS 安装SecureCRT破解版","slug":"2018-03-18-ubuntu-Ubuntu 16.04TLS 安装SecureCRT破解版","date":"2018-03-18T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/18/2018-03-18-ubuntu-Ubuntu 16.04TLS 安装SecureCRT破解版/","link":"","permalink":"luoxiao.cf/2018/03/18/2018-03-18-ubuntu-Ubuntu 16.04TLS 安装SecureCRT破解版/","excerpt":"","text":"由于最近使用vm搭建集群使用ubuntu的终端连接节点太繁琐,来安装SecureCRT 本机环境本机使用的是ubuntu16.04 TLS 64版,在本机上安装过程无任何问题(仅限本机) 1.下载securecrt_linux_crack.pl 破解程序scrt-8.3.1-1537.ubuntu16-64.x86_64.deb 程序包百度网盘 密码: 7j9k 2.安装安装secureCRTsudo dpkg -i scrt-8.3.1-1537.ubuntu16-64.x86_64.deb 3.破解运行破解脚本程序sudo perl securecrt_linux_crack.pl /usr/bin/SecureCRT认证信息:到dash中搜索securecrt启动,securecrt会提示你产品没有license，需要购买不然只能使用30天,我们输入上图中的认证信息就可以完成注册。 4.解决读取串口设备的权限问题当在启动器中启动SecureCRT 时，我们是以当前用户的权限启动的软件，因此没有操作串口的权限。使用命令：$ls -l /dev/ttyS0查看当前串口信息，会显示如下信息$ ls -l /dev/ttyS0可以看到，串口设备文件是 dialout这个组的，我们我们只要将当前用户添加到这个组就行了。加入当前登录的用户名为：fox，则执行如下命令$sudo usermod -a -G dialout zyh重启系统，之后就可以直接打开SecureCRT来操作串口了转自:http://blog.csdn.net/a499957739/article/details/79582999参考1:http://www.cnblogs.com/wangkongming/p/3533240.html参考2:http://www.vandyke.com/products/securecrt/","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"Ubuntu 16.04TLS 安装最新版VMware14.1.1","slug":"2018-03-18-ubuntu-安装虚拟机VMware","date":"2018-03-18T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/18/2018-03-18-ubuntu-安装虚拟机VMware/","link":"","permalink":"luoxiao.cf/2018/03/18/2018-03-18-ubuntu-安装虚拟机VMware/","excerpt":"","text":"由于最近搭建集群需要虚拟机,来安装VMware效果图: 本机环境本机使用的是ubuntu16.04 TLS 64版,在本机上安装过程无任何问题(仅限本机) 1.下载注册过帐号以后进入这个链接https://my.vmware.com/cn/group/vmware/details?downloadGroup=WKST-1411-LX&amp;productId=686&amp;download=true&amp;fileId=cd30e7c2359a3dc1a5e1b1d1b17727b9&amp;secureParam=d7a9ad9d3128741ea1642469b8d12b95&amp;uuId=1c8d56a7-02ad-4d06-9964-a4ab7419388c&amp;downloadType=下载的版本VMware-Workstation-Full-14.1.1-7528167.x86_64.bundle 2.破解码版本VMware-Workstation-Full-14.1.1-7528167.x86_64.bundle的破解码：VF19H-8YY5L-48DQY-JEWNG-YPKF6获得破解码的地址：http://beikeit.com/post-513.html 3.安装*.bundle 文件比较特殊，只有在给它了执行权限后才能执行安装操作。所以安装的第一步就是给 *.bundle 文件添加执行权限。介绍两种方法：1.在 *.bundle 文件上右击鼠标，选择最后一项“属性” 选项，在弹出的“属性”窗口中选择“权限”选项卡，在该选项卡中，可以看到有“允许以程序执行文件”的选项，把它选上，如图：2.另一种方法就是在终端用命令给 bundle 文件添加执行权限。打开终端(快捷键为 Ctrl+Alt+T )，打开终端后，进入 bundle 文件所在目录(便于操作)，然后用以下命令给 bundle 文件添加执行权限：$ sudo chmod +x *.bundle回车，输入用户密码，这样就给 bundle 文件添加了执行权限了。在命令行运行*.bundle进行安装$ sudo ./*.bundle 4.配置VMware vmnet0、vmnet1和vmnet8开始因为没有配置这个,导致ip不能分配,主机不能ping通虚拟机中的节点。（注意：假如你在安装CenOS时选用的是桥接而非NAT类型，可以忽略,你可以直接跳转到第6直接配置文件后重启网卡,看看能否成功。）三者的区别参考博文:http://blog.csdn.net/guizaijianchic/article/details/72190394进入编辑-&gt;虚拟网络编辑器 配置VMware vmnet0、vmnet1和vmnet8下图是主机配置，注意我主机的ip是192.168.100.110$ ifconfig1.vmnet0使用桥接链接方式,基本上不用动,下图是我本机的配置:2.vmnet1使用宿主链接方式,需要修改的是子网和子网掩码，并使用DHCP分配模式,下图是我本机的配置:3.vmnet8使用NAT链接方式,需要修改的是子网和子网掩码,并使用DHCP分配模式，下图是我本机的配置:打开网络设置.可以查看里面的网关参考blog:http://blog.csdn.net/w20228396/article/details/77507908和http://blog.csdn.net/didi8206050/article/details/51872682 5.安装CentOS minimal镜像自己去网上下载好了安装过程除了在选择网络那里选择桥接类型以外其他全部可以默认。参考blog:http://blog.csdn.net/capricorn90/article/details/52476228 6.配置网络(手动)如果安装过后发现不能上网，这个时候我们就要手动配置网络了，在这里我碰到的是虚拟网卡没有设置ip,解决办法：补充：如果你发现你连的ifconfig都不能用,可以使用ip addr查看相应信息:打开/etc/systemconfig/network-scripts/ifcfg-eth0，然后在里面加上如下的内容：注意打开的是ifcfg-eth0 你打开什么要根据你自己的网卡名称/$ /etc/systemconfig/network-scripts/ifcfg-eth0DEVICE=ens33#设备名称，可根据ifcofnig命令查看到。BOOTPROTO=dhcp #连接方式，dhcp会自动分配地址，此时不需要在下面设置ip和网关HWADDR=00:0C:29:AD:66:9F #硬件地址，可根据ifcofnig命令查看到。ONBOOT=yes #yes表示启动就执行该配置，需要改为yes 我本地开始是no,所以在开机后没有给我自动分配ip,将它改为yes,在终端重启网卡两个命令选一个:$ service network restart$ /etc/init.d/network restart参考blog:http://blog.csdn.net/w20228396/article/details/77507908和http://blog.csdn.net/didi8206050/article/details/51872682 7.克隆克隆和快照的区别:http://blog.csdn.net/whatday/article/details/52538031右键已安装的centos 管理-&gt;克隆，一路下一步,由于磁盘空间比较充足所以直接选择全克隆mini3是我刚从mini1克隆的一个新版本,启动以后发现无法正常分配ip因为mini1的网卡名称叫eth0,而克隆的叫eth1我们到**/etc/systemconfig/network-scripts/**发现根本没有ifcfg-eth1文件,因为我们是克隆的…所以在这里我们吧ifcfg-eth0改成ifcfg-eth1$ cd /etc/systemconfig/network-scripts/$ mv ifcfg-eth0 ifcfg-eth1$ vim ifcfg-eth1重新修改里面的物理地址和网卡名称,其他可以不用动,然后重启网卡/etc/init.d/network restart重新查看一下 8.ping主机名访问一直老使用ssh root@192.168.0.128,ssh root@192.168.0.130访问好麻烦,而且在以后编写脚本时容易弄错哪个ip对应是哪一台,我们希望通过这样ssh root@mini1,ssh root@mini2就可以访问到每台机器$ vim /etc/hosts像上图一样设置就可以了","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"同域名不同端口下iframe跨域问题处理","slug":"2018-03-15-跨域-同域名不同端口下跨域问题处理","date":"2018-03-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/15/2018-03-15-跨域-同域名不同端口下跨域问题处理/","link":"","permalink":"luoxiao.cf/2018/03/15/2018-03-15-跨域-同域名不同端口下跨域问题处理/","excerpt":"","text":"场景描述,现有两个界面A和B,他们的IP和端口情况如下：A : 192.168.1.2:8081B : 192.168.1.2:8082A界面中使用标签引入B界面。我们想在A界面来获取B界面中的元素时，在浏览器的console中就会提示cross…等字样的错误。 解决跨域问题 1. 使用设置请求头response.setHeader();response.setHeader(); 2. 使用domain 3. 使用ngnix左请求转发我使用了第三种方式来解决问题，让ngnix监听8083端口，分别将请求转发到A和B。这样就可以做到在访问A和B界面的时候，使用的都是同一个域名+端口。192.168.1.2：8083","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"mybatis数据库字段和实体类字段不一致解决方案","slug":"2018-03-15-mybatis-数据库字段和实体类字段不一致解决方案","date":"2018-03-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/15/2018-03-15-mybatis-数据库字段和实体类字段不一致解决方案/","link":"","permalink":"luoxiao.cf/2018/03/15/2018-03-15-mybatis-数据库字段和实体类字段不一致解决方案/","excerpt":"","text":"数据库字段与实体类字段不一致，导致功能正常，但是就是查不出数据 1.使用resultMap在xml中定义resultMap标签12345678910111213141516171819&lt;resultMap type=\"com.entity.BdipChatPoint\" id=\"pointMap\"&gt; &lt;id column=\"id_\" property=\"id\"/&gt; &lt;result column=\"json\" property=\"json\"/&gt; &lt;result column=\"picture\" property=\"picture\"/&gt; &lt;result column=\"model_url\" property=\"modelUrl\"/&gt; &lt;result column=\"db_id\" property=\"dbId\"/&gt; &lt;result column=\"frag_id\" property=\"fragId\"/&gt; &lt;result column=\"view_point_num\" property=\"viewPointNum\"/&gt; &lt;result column=\"user_name\" property=\"userName\"/&gt; &lt;result column=\"head_image\" property=\"headImage\"/&gt; &lt;result column=\"tree_id\" property=\"treeId\"/&gt; &lt;result column=\"enable_\" property=\"enable\"/&gt; &lt;result column=\"remark_\" property=\"remark\"/&gt; &lt;result column=\"create_by\" property=\"createBy\"/&gt; &lt;result column=\"create_time\" property=\"createTime\"/&gt; &lt;result column=\"update_time\" property=\"updateTime\"/&gt; &lt;result column=\"update_by\" property=\"updateBy\"/&gt;&lt;/resultMap&gt;column填写数据库字段 property填写实体类字段在使用标签的时候用resultMap=&quot;pointMap&quot;替换resultType=“com.entity.BdipChatPoint” 2.在写sql是使用as1234&lt;select id=\"selectBimChatPoint\" parameterType=\"com.entity.BdipChatPoint\" resultType=\"com.entity.BdipChatPoint\"&gt; select id_ as id,json,picture,model_url as modelUrl from bdip_chat_point where db_id = #&#123;dbId&#125; and model_url = #&#123;modelUrl&#125; and create_by = #&#123;createBy&#125;&lt;/select&gt;注意id_ as id和model_url as modelUrl依旧使用resultType来接收","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"ubuntu中没啥用的命令","slug":"2018-03-13-无聊-ubuntu没啥用的命令","date":"2018-03-13T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/13/2018-03-13-无聊-ubuntu没啥用的命令/","link":"","permalink":"luoxiao.cf/2018/03/13/2018-03-13-无聊-ubuntu没啥用的命令/","excerpt":"","text":"$ echo “大家好!!” | pv -qL 5$ xeyes$ oneko$ cowsay “MmmmmmmmmmOOOOOO…”$ cmatrix","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"利用wine安装迅雷和QQ","slug":"2018-03-13-ubuntu-利用wine安装迅雷和QQ","date":"2018-03-13T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/13/2018-03-13-ubuntu-利用wine安装迅雷和QQ/","link":"","permalink":"luoxiao.cf/2018/03/13/2018-03-13-ubuntu-利用wine安装迅雷和QQ/","excerpt":"","text":"本机环境:这个方法在本机试了。不行。小伙伴们还是散了吧！！前一篇安装QQ2012国际版由于版本太老，部分功能不能使用，下面我们安装新版QQ。本机环境$ cat /proc/versionLinux version 4.10.0-28-generic (buildd@lgw01-12) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) ) #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 核心内容1.迅雷运行几乎完美，能够用手机号登录，能够正常下载和加速。已知部分图片无法正常显示，无伤大雅。2.QQ运行效率比起其他版本的wineqq来说要高，能够记住密码和自动登录，无法视频通话、远程演示。 所需文件一览123456789101112131415161718192021222324+-- deepin.com.qq.im_8.9.19983deepin16_i386.deb+-- deepin.com.thunderspeed_7.10.35.366deepin15_i386.deb+-- deepin-wine-d| +-- deepin-fonts-wine_1.9-26_all.deb| +-- deepin-libwine_1.9-26_i386.deb| +-- deepin-wine32_1.9-26_i386.deb| +-- udis86_1.72-2_i386.deb+-- deepin-wine-helper-d| +-- libgif4_4.1.6-11_i386.deb| +-- libgnutls26_2.12.23-18_i386.deb| +-- libgnutls-deb0-28_3.3.20-1_i386.deb| +-- libgstreamer0.10-0_0.10.36-1.5_i386.deb| +-- libgstreamer-plugins-base0.10-0_0.10.36-2_i386.deb| +-- libpng16-16_1.6.26-1_i386.deb| +-- libreadline7_7.0-1_i386.deb+-- dependences+-- install.sh+-- qq-d| +-- deepin-wine_1.9-26_all.deb| +-- deepin-wine32-preloader_1.9-26_i386.deb| +-- deepin-wine-helper_1.0deepin17_i386.deb| +-- deepin-wine-uninstaller_0.1deepin2_i386.deb+-- README+-- remove.shqq-d文件夹中为搭建deepin-wine环境所需的第一级依赖软件，包括deepin-wine，deepin-wine-helper等。deepin-wine-d中为软件包deepin-wine所需的依赖软件，包括deepin-libwine等。deepin-wine-helper-d中为软件包deepin-wine-helper所需的部分依赖软件，包括32位的libreadline7等。网盘链接（链接: https://pan.baidu.com/s/1S69P6tK9St6ZDGBX2E1f2w 密码: 06i9） 安装方法64位操作系统安装前需检查dpkg是否包含了i386架构的软件包。终端输入$ dpkg --print-foreign-architectures如果输出i386则继续下面步骤，如果没有的话，需要先执行：$ sudo dpkg --add-architecture i386$ sudo apt-get update接下来安装deepin-qq和deepin-thunder，方法为打开终端，执行:$ sudo bash install.sh如果是 32位操作系统请查看原博客 https://www.ubuntukylin.com/ukylin/forum.php?mod=viewthread&amp;tid=30614转自https://www.ubuntukylin.com/ukylin/forum.php?mod=viewthread&amp;tid=30614","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"shiro学习","slug":"2018-03-13-shiro-shiro学习","date":"2018-03-13T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/13/2018-03-13-shiro-shiro学习/","link":"","permalink":"luoxiao.cf/2018/03/13/2018-03-13-shiro-shiro学习/","excerpt":"","text":"永不止步!感谢开老师的分享!http://jinnianshilongnian.iteye.com/blog/2018398https://github.com/zhangkaitao/shiro-example","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Shiro","slug":"Shiro","permalink":"luoxiao.cf/tags/Shiro/"}]},{"title":"SpringMVC接收复杂集合参数","slug":"2018-03-12-springmvc-SpringMVC接收复杂集合参数","date":"2018-03-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/12/2018-03-12-springmvc-SpringMVC接收复杂集合参数/","link":"","permalink":"luoxiao.cf/2018/03/12/2018-03-12-springmvc-SpringMVC接收复杂集合参数/","excerpt":"","text":"记两个方式 1.使用form的ajax方式提交 后台代码后台使用的是springmvc框架实体类：123456789101112131415public class BimWorkflowDetailBase &#123; @TableField(\"model_id\") private String modelId; @TableField(\"cc_name\") private String ccName; @TableField(\"cc_names\") private String ccNames; @TableField(\"status_\") private String status; ....&#125;实体类包装起来123456789101112public class WorkFlowCollisionWrapper &#123; private List&lt;BimWorkflowCollision&gt; collisions; public List&lt;BimWorkflowCollision&gt; getCollisions() &#123; return collisions; &#125; public void setCollisions(List&lt;BimWorkflowCollision&gt; collisions) &#123; this.collisions = collisions; &#125;&#125;在控制器的参数中直接写上包装类12345@RequestMapping(value=\"/check\",method=RequestMethod.POST,produces=\"application/json\")public @ResponseBody String check(WorkFlowCollisionWrapper collisions) &#123; System.out.println(collisions.getCollisions().size()); return null;&#125; 前台代码这里我使用了ES6动态生成html 里面的${index} 是ES6的写法,我们在这边的写法是name=&quot;collisions[0].id&quot; name=&quot;collisions[1].id以此类推就可以正常传入到后台1234567891011121314151617&lt;table class=\"m-initating-table hei30\" name=\"initating-table-$&#123;index&#125;\"&gt; &lt;tr&gt; &lt;td class=\"center\"&gt;专业编号&lt;/td&gt; &lt;td&gt; &lt;input readonly=\"readonly\" name=\"collisions[$&#123;index&#125;].zybh\" value=\"$&#123;typeof(node.zybh)=='undefined'?'':node.zybh&#125;\" class=\"tableinput first firstStageInput fourthStageInput\" type=\"text\" maxlength=\"20\"&gt; &lt;/td&gt; &lt;td class=\"center\"&gt;类别编号&lt;/td&gt; &lt;td&gt; &lt;select disabled=\"disabled\" name=\"collisions[$&#123;index&#125;].lbbh\" class=\"tableSelect first firstStageSelect fourthStageSelect\" style=\"width: 80%;margin: 0 auto;display: block;\"&gt; &lt;option value=\"A\" $&#123;typeof(node.lbbh)=='undefined' || node.lbbh == 'A'?'selected':''&#125;&gt;A&lt;/option&gt; &lt;option value=\"B\" $&#123;node.lbbh == 'B'?'selected':''&#125;&gt;B&lt;/option&gt; &lt;option value=\"C\" $&#123;node.lbbh == 'C'?'selected':''&#125;&gt;C&lt;/option&gt; &lt;option value=\"D\" $&#123;node.lbbh == 'D'?'selected':''&#125;&gt;D&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;这里使用form表单序列化所有的值,发送ajax请求后台1234567891011121314var url = publicJS.tomcat_url + '/workflow/check';var data = $('#collisionForm').serialize();$.ajax(&#123; url : url, type : 'POST', data : data, dataType : 'json', success : function(data)&#123; &#125;, error : function(error)&#123; &#125;&#125;) 2.使用纯ajax方式提交 后台实体类12345public class User &#123; private String name; private String pwd; //省略getter/setter &#125;控制器12345678910@Controller @RequestMapping(\"/catalog.do\") public class CatalogController &#123; @RequestMapping(params = \"fn=saveUsers\") @ResponseBody public AjaxJson saveUsers(@RequestBody List&lt;User&gt; userList) &#123; … &#125; &#125; 前台12345678910111213141516var userList = new Array(); userList.push(&#123;name: \"李四\",pwd: \"123\"&#125;); userList.push(&#123;name: \"张三\",pwd: \"332\"&#125;); $.ajax(&#123; type: \"POST\", url: \"&lt;%=path%&gt;/catalog.do?fn=saveUsers\", data: JSON.stringify(userList),//将对象序列化成JSON字符串 dataType:\"json\", contentType : 'application/json;charset=utf-8', //设置请求头信息 success: function(data)&#123; … &#125;, error: function(res)&#123; … &#125; &#125;);","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"content-disposition使用","slug":"2018-03-12-java-content-disposition","date":"2018-03-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/12/2018-03-12-java-content-disposition/","link":"","permalink":"luoxiao.cf/2018/03/12/2018-03-12-java-content-disposition/","excerpt":"","text":"content-disposition 是RFC中定义的文件下载标识字段,详情查看rfc2616章节19.5 Additional Features,其中有两种形式供给我们选择，一个是inline，一个是attachment 在页面内打开代码：12345File file = new File(\"rfc1806.txt\"); String filename = file.getName(); response.setHeader(\"Content-Type\",\"text/plain\"); response.addHeader(\"Content-Disposition\",\"inline;filename=\" + new String(filename.getBytes(),\"utf-8\")); response.addHeader(\"Content-Length\",\"\" + file.length()); 弹出保存框代码：12345File file = new File(\"rfc1806.txt\"); String filename = file.getName(); response.setHeader(\"Content-Type\",\"text/plain\"); response.addHeader(\"Content-Disposition\",\"attachment;filename=\" + new String(filename.getBytes(),\"utf-8\")); response.addHeader(\"Content-Length\",\"\" + file.length()); 场景应用，导出word文档：控制层使用的springmvc1234567891011121314151617181920212223242526272829303132333435363738/** * 导出word功能 * @param workflowId 流程的id * @throws IOException */@ApiOperation(value=\"导出word功能\",notes=\"导出word功能\")@ApiResponses(value= &#123;@ApiResponse(code=200,message=\"导出成功\",response=BimWorkflowDetailController.class)&#125;)@RequestMapping(value=\"/exportCollision\")public String exportCollision(String workflowId,HttpServletResponse response) throws IOException &#123; //response.setContentType(\"application/octet-stream; charset=UTF-8\"); response.setHeader(\"content-disposition\", \"attachment;filename=\" + new SimpleDateFormat(\"yyyyMMddHH:mm:ss\").format(new Date(System.currentTimeMillis())) + \".doc\"); // opera和firefox可以正常使用，而ie不能正常使用 添加下列头 response.setHeader(\"Pragma\",\"No-cache\"); response.setHeader(\"Cache-Control\",\"No-cache\"); response.setDateHeader(\"Expires\",0); ApiResultEntity apiResultEntity = new ApiResultEntity(); apiResultEntity.setDataEncode(true); // 1.根据workflowid从数据库查询相关信息 Parameter parameter = new Parameter(BizServiceDefine.bimWorkflowDetailService, \"getBimWorkflowInfoById\").setId(NumberUtil.tryParseLong(workflowId)); parameter = bizProvider.execute(parameter); apiResultEntity = (ApiResultEntity) parameter.getResult(); Map&lt;String,Object&gt; param = (Map&lt;String,Object&gt;) apiResultEntity.getData(); BimWorkflowDetail workflowBase = (BimWorkflowDetail) param.get(\"bimWorkflowDetail\"); List&lt;BimWorkflowCollision&gt; collisionLists = (List&lt;BimWorkflowCollision&gt;) param.get(\"collisionLists\"); OutputStream out = null; try &#123; out = response.getOutputStream(); // 存word saveDoc(workflowBase, collisionLists, out); &#125; catch (Exception e) &#123; logger.error(\"导出word异常!!!!\"); e.printStackTrace(); &#125; out.flush(); out.close(); return null; &#125;","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"An invalid character [32] was present in the Cookie value","slug":"2018-03-12-java-An invalid character [32] was present in the Cookie value","date":"2018-03-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/12/2018-03-12-java-An invalid character [32] was present in the Cookie value/","link":"","permalink":"luoxiao.cf/2018/03/12/2018-03-12-java-An invalid character [32] was present in the Cookie value/","excerpt":"","text":"这是因为Cookies中存储特殊字符串引起的，当我们在里面存储分号空格等一些特殊符号时，就会抛异常。解决办法我们只需要存入和取出的时候用URLEncoder.encode(xxx，&quot;UTF-8&quot;)和URLDcoder.decoder(xxx,&quot;UTF-8&quot;)就可以得到解决。","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[{"name":"问题总结","slug":"问题总结","permalink":"luoxiao.cf/tags/问题总结/"}]},{"title":"IntelliJ IDEA安装python插件分享","slug":"2018-03-3-idea-idea安装python插件","date":"2018-03-03T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/03/03/2018-03-3-idea-idea安装python插件/","link":"","permalink":"luoxiao.cf/2018/03/03/2018-03-3-idea-idea安装python插件/","excerpt":"","text":"开始在File-&gt;Settings-&gt;Plugins-&gt;中尝试集成python,下载了半天提示了个错误框,然后只好通过手动方式安装了 本机环境:本机是在ubuntu16上使用的是2016.3.7版本的IntelliJ IDEA 开始安装首先我们到官网插件中心http://plugins.jetbrains.com/plugin/631-python下载我们要的插件,在下载的时候尽量选择版本比较接近的,这样不容易有冲突我下载的是163.12024—163.*下载好以后在Intellij IDEA File——&gt;Settings——&gt;IDE setttings——&gt;Plugins——&gt;Install plugin from disk…——&gt;选择插件所在路径即可。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"luoxiao.cf/tags/Python3/"}]},{"title":"ubuntu16.04安装QQ国际版","slug":"2018-03-3-ubuntu-ubuntu16.04安装qq","date":"2018-03-03T00:00:00.000Z","updated":"2019-11-13T04:56:08.673Z","comments":true,"path":"2018/03/03/2018-03-3-ubuntu-ubuntu16.04安装qq/","link":"","permalink":"luoxiao.cf/2018/03/03/2018-03-3-ubuntu-ubuntu16.04安装qq/","excerpt":"","text":"经过不断折腾,终于找到了一个可安装的QQ,下面的方法不出意外应该是适用与所有的ubuntu16版本,本人已经使用这个方法正常安装了QQ2012国际版,如图:QQ2012国际版:支持密码键盘输入并记住密码。可以发送QQ表情无问题。传送文件无问题。IBus输入法正常。QQ设置常用功能，比较精简，且占用CPU少。 本机环境:本机使用的是ubuntu16.04 TLS 64版,内核版本为4.10.0-28-generic,qq在本机上安装过程无任何问题(仅限本机)$ cat /proc/versionLinux version 4.10.0-28-generic (buildd@lgw01-12) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) ) #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 开始安装首先我们需要安装依赖库 libgtk2.0-0:i386 因为我们是64位系统,所以我们还需要安装 ia32-libs , 但是ubuntu16.04中该软件包已经被其他版本代替了,所以我们选择安装lib32ncurses5 1. 安装libgtk2.0-0:i386在终端输入sudo apt-get install libgtk2.0-0:i386$ sudo apt-get install libgtk2.0-0:i386 2. 安装 lib32ncurses5在终端输入sudo apt-get install lib32ncurses5$ sudo apt-get install lib32ncurses5 3. 下载QQ2012国际版下载链接https://pan.baidu.com/s/1bpF3p7L,我们只需要下载wine-qqintl-www.linuxidc.com.tar.xz到本地,随后我们解压出里面的三个deb文件:我们使用dpkg分别安装三个deb文件 4. 安装 wine-qqintl_0.1.3-2_i386.deb$ sudo dpkg -i wine-qqintl_0.1.3-2_i386.deb初次安装这个文件的时候,如果有部分lib没有配置,就会安装失败.我们的解决办法 执行一遍sudo apt-get install -f,这个命令的意思是假如用户的系统上有某个package不满足依赖条件，这个命令就会自动修复,安装程序包所依赖的包.完成以后我们再运行一遍sudo dpkg -i wine-qqintl_0.1.3-2_i386.deb就会发现成功安装了 5. 安装ttf-wqy-microhei_0.2.0-beta-2_all.deb和fonts-wqy-microhei_0.2.0-beta-2_all.deb安装完后面两个deb文件$ sudo dpkg -i ttf-wqy-microhei_0.2.0-beta-2_all.deb$ sudo dpkg -i fonts-wqy-microhei_0.2.0-beta-2_all.deb 运行QQ2012国际版在开始菜单中我们可以找到QQ2012国际版我们可以在终端使用命令sudo dpkg -l|grep qq 和 sudo find / -name qq* 查看QQ的安装情况如果在登录时提示版本过低,我们只需要将 手机QQ–&gt;设置–&gt;帐号、设备安全–&gt;设备锁 设置为未启用状态即可 卸载QQ2012国际版 1.查看有关qq的详情信息$ sudo dpkg -l | grep qq我们可以发现存在两个package,使用dpkg -P命令分别卸载两个package 2.卸载 wine-qqintl:i386$ sudo dpkg -P wine-qqintl:i386 3.卸载 libqqwing2v5:amd64$ sudo dpkg -P libqqwing2v5:amd64在卸载的时候提示存在依赖,所以卸载失败了,我们使用-l查看gnome-sudoku详情,并将其卸载后一切就正常了$ sudo dpkg -l | grep gnome-sudoku$ sudo dpkg -P gnome-sudoku","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"Firefox下利用微博图床优化markdown粘贴图片链接体验分享","slug":"2018-02-26-markdown-利用微博图床优化markdown粘贴图片链接体验","date":"2018-02-26T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/02/26/2018-02-26-markdown-利用微博图床优化markdown粘贴图片链接体验/","link":"","permalink":"luoxiao.cf/2018/02/26/2018-02-26-markdown-利用微博图床优化markdown粘贴图片链接体验/","excerpt":"","text":"网上有很多利用图床优化markdown粘贴图片链接的优化方式,我的这种并不是最好的,也不是最简便的.但是这个方法很容易配置,适合不懂代码的新手,具体配置步骤如下：在ubuntu中配置自定义快捷键,执行xxx.sh脚本命令,该脚本命令由我们自己编写,其中会调用gnome-screenshot进行截图(如果不想使用gnome-screenshot截图工具,可以自行寻找其他截图方式代替).调用xclip将剪切板中的截图保存到某磁盘下下载firefox图床拓展围脖就是好图床手动将磁盘中的图片托到firefox图床拓展中上传。(如果有能力自己可以研究一下如何自动上传) 本机环境:本机使用的是ubuntu16.04 TLS 64版,内核版本为4.10.0-28-generic,qq在本机上安装过程无任何问题(仅限本机)$ cat /proc/versionLinux version 4.10.0-28-generic (buildd@lgw01-12) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) ) #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 开始安装首先我们需要安装xclip工具,这个命令其实是命令建立了终端和剪切板之间通道，可以用命令的方式将终端输出或文件的内容保存到剪切板中，也可以将剪切板的内容输出到终端或文件中,具体命令的细节有兴趣的小伙伴可以去研究一下。其次我们需要用到ubuntu自带的gnome-screenshot截图工具,我们可以打开终端 输入gnome-screenshot -a -c 1. 安装xclip$ sudo apt-get install xclip 2. 编写snapshot.sh打开/home/zyh/Documents/screenshot/snapshot.sh,具体内容如下1234#!/bin/bashgnome-screenshot -a -cxclip -selection clipboard -t image/png -o &gt; $HOME/Desktop/snapshot.png第二行的意思是将剪切板中的图片以snapshot.png为名保存到桌面上编写完snapshot.sh,在终端输入chmod +x snapshot.sh,修改snapshot.sh为可执行状态 3. 自定义截图快捷键运行snapshot.sh我们可以在系统设置的键盘选项中自定义快捷键来运行我们想要运行的脚本文件点击+号添加自定义快捷键,名称随意填写即可,命令填写我们刚刚编写的snapshot.sh,记得要写全路径,本机填写的是/home/zyh/Documents/screenshot/snapshot.sh我这里设置的快捷键是Ctrl+Alt+W,自己根据喜好调整即可。 测试我们使用Ctrl+Alt+W截图,然后会发现桌面上多出一个snapshot.png的文件,随后我们打开firefox将文件托到拓展中上传就可以拿到想要的链接了！ 其他如果你喜欢chrome浏览器,你可以在chrome下载新浪图床拓展程序,因为最近GWF很严,本人惨造封杀,无法科学上网,chrome拓展不太好下载,所以选择了firefox 😦","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[]},{"title":"前端一些知识汇总分享","slug":"2018-02-24-经验分享-每天进步一点点","date":"2018-02-24T00:00:00.000Z","updated":"2019-11-13T04:56:08.672Z","comments":true,"path":"2018/02/24/2018-02-24-经验分享-每天进步一点点/","link":"","permalink":"luoxiao.cf/2018/02/24/2018-02-24-经验分享-每天进步一点点/","excerpt":"","text":"在segmentfault.com看到一篇好的文章面试的信心来源于过硬的基础，来分享一下。1.viewport2.跨域的几种方式3.渲染优化4.事件的各个阶段5.let var const6.箭头函数7.快速的让一个数组乱序8.字体font-family9.可能用到的meta标签10.消除transition闪屏11.android 4.x bug12.JS 判断设备来源13.audio元素和video元素在ios和andriod中无法自动播放14.css实现单行文本溢出显示 …15.实现多行文本溢出显示…16.让图文不可复制17.盒子垂直水平居中18.改变placeholder的字体颜色大小19.最快捷的数组求最大值20.更短的数组去重写法21.vue 父子组件嵌套时，组件内部的各个生命周期钩子触发先后顺序 1.viewport12345678&lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no\" /&gt;// width 设置viewport宽度，为一个正整数，或字符串‘device-width’// device-width 设备宽度// height 设置viewport高度，一般设置了宽度，会自动解析出高度，可以不用设置// initial-scale 默认缩放比例（初始缩放比例），为一个数字，可以带小数// minimum-scale 允许用户最小缩放比例，为一个数字，可以带小数// maximum-scale 允许用户最大缩放比例，为一个数字，可以带小数// user-scalable 是否允许手动缩放延伸 提问:怎样处理 移动端 1px 被 渲染成 2px 问题?局部处理mata标签中的 viewport属性 ，initial-scale 设置为 1rem 按照设计稿标准走，外加利用transfrome 的scale(0.5) 缩小一倍即可；全局处理mata标签中的 viewport属性 ，initial-scale 设置为 0.5rem 按照设计稿标准走即可 2.跨域的几种方式首先了解下浏览器的同源策略同源策略/SOP（Same origin policy）是一种约定，由Netscape公司1995年引入浏览器，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，浏览器很容易受到XSS、CSRF等攻击。所谓同源是指&quot;协议+域名+端口&quot;三者相同，即便两个不同的域名指向同一个ip地址，也非同源。那么怎样解决跨域问题的呢？通过jsonp跨域12345678910111213&lt;script&gt; var script = document.createElement('script'); script.type = 'text/javascript'; // 传参并指定回调执行函数为onBack script.src = 'http://www.....:8080/login?user=admin&amp;callback=onBack'; document.head.appendChild(script); // 回调执行函数 function onBack(res) &#123; alert(JSON.stringify(res)); &#125; &lt;/script&gt;document.domain + iframe跨域(此方案仅限主域相同，子域不同的跨域应用场景)123456789101112131415&lt;!--父窗口：(http://www.domain.com/a.html)--&gt;&lt;iframe id=\"iframe\" src=\"http://child.domain.com/b.html\"&gt;&lt;/iframe&gt;&lt;script&gt; document.domain = 'domain.com'; var user = 'admin';&lt;/script&gt;&lt;!--子窗口：(http://child.domain.com/b.html)--&gt;&lt;script&gt; document.domain = 'domain.com'; // 获取父窗口中变量 alert('get js data from parent ---&gt; ' + window.parent.user);&lt;/script&gt;&lt;!--弊端：请看下面渲染加载优化--&gt;nginx代理跨域nodejs中间件代理跨域后端在头部信息里面设置安全域名更多跨域的具体内容请看 https://segmentfault.com/a/1190000011145364 3.渲染优化禁止使用iframe（阻塞父文档onload事件）；iframe会阻塞主页面的Onload事件；搜索引擎的检索程序无法解读这种页面，不利于SEO;iframe和主页面共享连接池，而浏览器对相同域的连接有限制，所以会影响页面的并行加载。使用iframe之前需要考虑这两个缺点。如果需要使用iframe，最好是通过javascript动态给iframe添加src属性值，这样可以绕开以上两个问题。禁止使用gif图片实现loading效果（降低CPU消耗，提升渲染性能）；使用CSS3代码代替JS动画（尽可能避免重绘重排以及回流）；对于一些小图标，可以使用base64位编码，以减少网络请求。但不建议大图使用，比较耗费CPU；小图标优势在于：减少HTTP请求；避免文件跨域；修改及时生效；页面头部的会阻塞页面；（因为 Renderer进程中 JS线程和渲染线程是互斥的）；页面头部&lt;script会阻塞页面；（因为 Renderer进程中 JS线程和渲染线程是互斥的）；页面中空的 href 和 src 会阻塞页面其他资源的加载 (阻塞下载进程)；网页Gzip，CDN托管，data缓存 ，图片服务器；前端模板 JS+数据，减少由于HTML标签导致的带宽浪费，前端用变量保存AJAX请求结果，每次操作本地变量，不用请求，减少请求次数用innerHTML代替DOM操作，减少DOM操作次数，优化javascript性能。当需要设置的样式很多时设置className而不是直接操作style。少用全局变量、缓存DOM节点查找的结果。减少IO读取操作。避免使用CSS Expression（css表达式)又称Dynamic properties(动态属性)。图片预加载，将样式表放在顶部，将脚本放在底部 加上时间戳。避免在页面的主体布局中使用table，table要等其中的内容完全下载之后才会显示出来，显示比div+css布局慢。对普通的网站有一个统一的思路，就是尽量向前端优化、减少数据库操作、减少磁盘IO。向前端优化指的是，在不影响功能和体验的情况下，能在浏览器执行的不要在服务端执行，能在缓存服务器上直接返回的不要到应用服务器，程序能直接取得的结果不要到外部取得，本机内能取得的数据不要到远程取，内存能取到的不要到磁盘取，缓存中有的不要去数据库查询。减少数据库操作指减少更新次数、缓存结果减少查询次数、将数据库执行的操作尽可能的让你的程序完成（例如join查询），减少磁盘IO指尽量不使用文件系统作为缓存、减少读写文件次数等。程序优化永远要优化慢的部分，换语言是无法“优化”的。 4.事件的各个阶段1：捕获阶段 —&gt; 2：目标阶段 —&gt; 3：冒泡阶段document —&gt; target目标 ----&gt; document由此，addEventListener的第三个参数设置为true和false的区别已经非常清晰了：true表示该元素在事件的“捕获阶段”（由外往内传递时）响应事件；false表示该元素在事件的“冒泡阶段”（由内向外传递时）响应事件。 5.let var constlet 允许你声明一个作用域被限制在块级中的变量、语句或者表达式let绑定不受变量提升的约束，这意味着let声明不会被提升到当前该变量处于从块开始到初始化处理的“暂存死区”。var 声明变量的作用域限制在其声明位置的上下文中，而非声明变量总是全局的由于变量声明（以及其他声明）总是在任意代码执行之前处理的，所以在代码中的任意位置声明变量总是等效于在代码开头声明const 声明创建一个值的只读引用 (即指针)这里就要介绍下 JS 常用类型String、Number、Boolean、Array、Object、Null、Undefined基本类型 有 Undefined、Null、Boolean、Number、String，保存在栈中；复合类型 有 Array、Object ，保存在堆中；基本数据当值发生改变时，那么其对应的指针也将发生改变，故造成 const申明基本数据类型时，再将其值改变时，将会造成报错， 例如 const a = 3 ; a = 5 时 将会报错；但是如果是复合类型时，如果只改变复合类型的其中某个Value项时， 将还是正常使用； 6.箭头函数语法比函数表达式更短，并且不绑定自己的this，arguments，super或 new.target。这些函数表达式最适合用于非方法函数，并且它们不能用作构造函数。这个是我本地的profile作参考 7.快速的让一个数组乱序12345var arr = [1,2,3,4,5,6,7,8,9,10];arr.sort(function()&#123; return Math.random() - 0.5;&#125;)console.log(arr);此处解释：（语言组织能力不足，请勿吐槽）首先： 当return 的值小于 0 ，那么 a 会被排列到 b 之前； 等于 0 ， a 和 b 的相对位置不变； 大于 0 ， b 会被排列到 a 之前； 这里你会 发现起始 的时候数组是正序排列，每当进行一次排列的时候， 都会先随机一个随机数（注意这里的每一次排列 指 每一个红框指一次排列， 共9次排列 ， 一次排列中可能存在多次比较）；当一次排列的 随机数大于0.5 时 将会进行第二次比较， 当第二次随机数 仍然大于0.5 时 ，将会再 进行一次比较， 直到 随机数大于0.5 或者排列到第一位；当一次排列的 随机数 小于0.5时 当前比较的两项 索引将不会改变 ，继续下一次 的排列； 8.字体font-family@ 宋体 SimSun@ 黑体 SimHei@ 微信雅黑 Microsoft Yahei@ 微软正黑体 Microsoft JhengHei@ 新宋体 NSimSun@ 新细明体 MingLiU@ 细明体 MingLiU@ 标楷体 DFKai-SB@ 仿宋 FangSong@ 楷体 KaiTi@ 仿宋_GB2312 FangSong_GB2312@ 楷体_GB2312 KaiTi_GB2312@@ 说明：中文字体多数使用宋体、雅黑，英文用Helveticabody { font-family: Microsoft Yahei,SimSun,Helvetica; } 9.可能用到的meta标签123456789101112131415161718192021222324252627&lt;!-- 仅针对IOS的Safari顶端状态条的样式（可选default/black/black-translucent ） --&gt;&lt;meta name=\"apple-mobile-web-app-status-bar-style\" content=\"black\" /&gt;&lt;!-- IOS中禁用将数字识别为电话号码/忽略Android平台中对邮箱地址的识别 --&gt;&lt;meta name=\"format-detection\"content=\"telephone=no, email=no\" /&gt;&lt;!--其他meta标签--&gt;&lt;!-- 启用360浏览器的极速模式(webkit) --&gt;&lt;meta name=\"renderer\" content=\"webkit\"&gt;&lt;!-- 避免IE使用兼容模式 --&gt;&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;&lt;!-- 针对手持设备优化，主要是针对一些老的不识别viewport的浏览器，比如黑莓 --&gt;&lt;meta name=\"HandheldFriendly\" content=\"true\"&gt;&lt;!-- 微软的老式浏览器 --&gt;&lt;meta name=\"MobileOptimized\" content=\"320\"&gt;&lt;!-- uc强制竖屏 --&gt;&lt;meta name=\"screen-orientation\" content=\"portrait\"&gt;&lt;!-- QQ强制竖屏 --&gt;&lt;meta name=\"x5-orientation\" content=\"portrait\"&gt;&lt;!-- UC强制全屏 --&gt;&lt;meta name=\"full-screen\" content=\"yes\"&gt;&lt;!-- QQ强制全屏 --&gt;&lt;meta name=\"x5-fullscreen\" content=\"true\"&gt;&lt;!-- UC应用模式 --&gt;&lt;meta name=\"browsermode\" content=\"application\"&gt;&lt;!-- QQ应用模式 --&gt;&lt;meta name=\"x5-page-mode\" content=\"app\"&gt;&lt;!-- windows phone 点击无高光 --&gt;&lt;meta name=\"msapplication-tap-highlight\" content=\"no\"&gt; 10.消除transition闪屏12345.css &#123;-webkit-transform-style: preserve-3d;-webkit-backface-visibility: hidden;-webkit-perspective: 1000;&#125;过渡动画（在没有启动硬件加速的情况下）会出现抖动的现象， 以上的 解决方案只是改变 视角 来启动硬件加速的一种方式；启动硬件加速的 另外一种方式：123456.css &#123;-webkit-transform: translate3d(0,0,0);-moz-transform: translate3d(0,0,0);-ms-transform: translate3d(0,0,0);transform: translate3d(0,0,0);&#125;启动硬件加速:最常用的方式：translate3d、translateZ、transformopacity属性/过渡动画（需要动画执行的过程中才会创建合成层，动画没有开始或结束后元素还会回到之前的状态）will-chang属性（这个比较偏僻），一般配合opacity与translate使用（而且经测试，除了上述可以引发硬件加速的属性外，其它属性并不会变成复合层），弊端： 硬件加速会导致 CPU性能占用量过大，电池电量消耗加大 ；因此 尽量避免泛滥使用硬件加速。 11.android 4.x bug1.三星 Galaxy S4中自带浏览器不支持border-radius缩写2.同时设置border-radius和背景色的时候，背景色会溢出到圆角以外部分3.部分手机(如三星)，a链接支持鼠标:visited事件，也就是说链接访问后文字变为紫色4.android无法同时播放多音频audio5.oppo 的border-radius 会失效 12.JS 判断设备来源12345678910111213141516171819202122function deviceType()&#123; var ua = navigator.userAgent; var agent = [\"Android\", \"iPhone\", \"SymbianOS\", \"Windows Phone\", \"iPad\", \"iPod\"]; for(var i=0; i&lt;len,len = agent.length; i++)&#123; if(ua.indexOf(agent[i])&gt;0)&#123; break; &#125; &#125;&#125;deviceType();window.addEventListener('resize', function()&#123; deviceType();&#125;)//微信的 有些不太一样function isWeixin()&#123; var ua = navigator.userAgent.toLowerCase(); if(ua.match(/MicroMessenger/i)=='micromessenger')&#123; return true; &#125;else&#123; return false; &#125;&#125; 13.audio元素和video元素在ios和andriod中无法自动播放原因： 因为各大浏览器都为了节省流量，做出了优化，在用户没有行为动作时（交互）不予许自动播放；1234567891011121314151617181920//音频，写法一&lt;audio src=\"music/bg.mp3\" autoplay loop controls&gt;你的浏览器还不支持哦&lt;/audio&gt;//音频，写法二&lt;audio controls=\"controls\"&gt; &lt;source src=\"music/bg.ogg\" type=\"audio/ogg\"&gt;&lt;/source&gt; &lt;source src=\"music/bg.mp3\" type=\"audio/mpeg\"&gt;&lt;/source&gt; 优先播放音乐bg.ogg，不支持在播放bg.mp3&lt;/audio&gt;//JS绑定自动播放（操作window时，播放音乐）$(window).one('touchstart', function()&#123; music.play();&#125;)//微信下兼容处理document.addEventListener(\"WeixinJSBridgeReady\", function () &#123; music.play();&#125;, false);//小结//1.audio元素的autoplay属性在IOS及Android上无法使用，在PC端正常；//2.audio元素没有设置controls时，在IOS及Android会占据空间大小，而在PC端Chrome是不会占据任何空间；//3.注意不要遗漏微信的兼容处理需要引用微信JS； 14.css实现单行文本溢出显示 …直接上效果：相对于多行文本溢出做处理， 单行要简单多，且更容易理解。实现方法1234overflow: hidden;text-overflow:ellipsis;white-space: nowrap;//当然还需要加宽度width属来兼容部分浏览。 15.实现多行文本溢出显示…效果：实现方法：1234display: -webkit-box;-webkit-box-orient: vertical;-webkit-line-clamp: 3;overflow: hidden;适用范围：因使用了WebKit的CSS扩展属性，该方法适用于WebKit浏览器及移动端；注：-webkit-line-clamp用来限制在一个块元素显示的文本的行数。 为了实现该效果，它需要组合其他的WebKit属性。常见结合属性：display: -webkit-box; 必须结合的属性 ，将对象作为弹性伸缩盒子模型显示 。-webkit-box-orient 必须结合的属性 ，设置或检索伸缩盒对象的子元素的排列方式 。如果你觉着这样还不够美观， 那么就接着往下看：效果：这样 是不是你想要的呢？实现方法：12345678910111213div &#123; position: relative; line-height: 20px; max-height: 40px; overflow: hidden;&#125;div:after &#123; content: \"...\"; position: absolute; bottom: 0; right: 0; padding-left: 40px; background: -webkit-linear-gradient(left, transparent, #fff 55%); background: -o-linear-gradient(right, transparent, #fff 55%); background: -moz-linear-gradient(right, transparent, #fff 55%); background: linear-gradient(to right, transparent, #fff 55%);&#125;不要只顾着吃，要注意胃口，此方法有个弊端 那就是 【未超出行的情况下也会出现省略号】 ，这样会不会很挫！！！ 没办法，只能结合JS 进行优化该方法了。注：将height设置为line-height的整数倍，防止超出的文字露出。给p::after添加渐变背景可避免文字只显示一半。由于ie6-7不显示content内容，所以要添加标签兼容ie6-7（如：…）；兼容ie8需要将::after替换成:after。 16.让图文不可复制这点应该大家 都很熟悉了， 某些时候【你懂的】为了快捷搜索答案，可是打死也不让你复制12345-webkit-user-select: none; -ms-user-select: none;-moz-user-select: none;-khtml-user-select: none;user-select: none;那有些网页为了尊重原创，复制的文本 都会被加上一段来源说明，是如何做到的呢？问的好！ 等的就是你这个问题 -_- 。大致思路：答案区域监听copy事件，并阻止这个事件的默认行为。获取选中的内容（window.getSelection()）加上版权信息，然后设置到剪切板（clipboarddata.setData()）。 17.盒子垂直水平居中这个问题好像面试必问的吔！反正我是必问的，哈哈！！！ 其实无关多少种实现思路，只要你能实现就可以！提供4种方法定位 盒子宽高已知， position: absolute; left: 50%; top: 50%; margin-left:-自身一半宽度; margin-top: -自身一半高度;table-cell布局 父级 display: table-cell; vertical-align: middle; 子级 margin: 0 auto;定位 + transform ; 适用于 子盒子 宽高不定时； （这里是本人常用方法）1234567position: relative / absolute;/*top和left偏移各为50%*/top: 50%;left: 50%;/*translate(-50%,-50%) 偏移自身的宽和高的-50%*/transform: translate(-50%, -50%);/*注意这里启动了3D硬件加速哦 会增加耗电量的 （至于何是3D加速 请看浏览器进程与线程篇）*/flex 布局父级：123456/*flex 布局*/display: flex;/*实现垂直居中*/align-items: center;/*实现水平居中*/justify-content: center;再加一种水平方向上居中 ：margin-left : 50% ; transform: translateX(-50%); 18.改变placeholder的字体颜色大小其实这个方法也就在PC端可以，真机上屁用都没有，当时我就哭了。 但 还是贴出来吧123456789101112131415input::-webkit-input-placeholder &#123; /* WebKit browsers */ font-size:14px; color: #333;&#125; input::-moz-placeholder &#123; /* Mozilla Firefox 19+ */ font-size:14px; color: #333;&#125; input:-ms-input-placeholder &#123; /* Internet Explorer 10+ */ font-size:14px; color: #333;&#125; 19.最快捷的数组求最大值12var arr = [ 1,5,1,7,5,9];Math.max(...arr) // 9 20.更短的数组去重写法12[...new Set([2,\"12\",2,12,1,2,1,6,12,13,6])]// [2, \"12\", 12, 1, 6, 13] 21.vue 父子组件嵌套时，组件内部的各个生命周期钩子触发先后顺序首先 我们可以把 子组件当做function函数来看待，当父组件 import 子组件的时候， 就当是声明了 并加载了这个函数，在调用的时候才会去执行这个函数（子组件）。那么父子组件中的各个声明周期钩子触发的先后顺序是怎样的呢？如下图：下图带222 的 是为子组件，所以一次顺序是为 先创建父组件，然后才穿件子组件，当子组件创建完成并且实体dom挂载完成后父组件才挂载完成注：资源来源于https://segmentfault.com/a/1190000013331105?utm_source=index-hottest#articleHeader15","categories":[{"name":"转载","slug":"转载","permalink":"luoxiao.cf/categories/转载/"}],"tags":[]},{"title":"Python五大标准数据类型","slug":"2018-02-23-python-标准数据类型","date":"2018-02-23T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2018/02/23/2018-02-23-python-标准数据类型/","link":"","permalink":"luoxiao.cf/2018/02/23/2018-02-23-python-标准数据类型/","excerpt":"","text":"12345678910111213141516171819a,b,c,d = 1,2.0,3.00,\"HHHH\";print(a);print(b); print(c);print(d);lists = ['1','2','3'];print(lists);tuples = ('1','2','3');print(tuples);dictionary = &#123;1:'a',2:'b',3:'c'&#125;;print(dictionary);``` ```python# 99乘法表for i in range(1,10): for j in range(i,10): print(i,\"*\",j,\"=\",i*j,end='\\t'); print(\"\\n\");","categories":[{"name":"编程语言","slug":"编程语言","permalink":"luoxiao.cf/categories/编程语言/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"luoxiao.cf/tags/Python3/"}]},{"title":"selenium资料馆库","slug":"2018-02-12-js-selenium资料库","date":"2018-02-12T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2018/02/12/2018-02-12-js-selenium资料库/","link":"","permalink":"luoxiao.cf/2018/02/12/2018-02-12-js-selenium资料库/","excerpt":"","text":"今天把手头有的一些关于selenium测试的资源整理了一下，分享出来。 1.所有版本chrome下载是不是很难找到老版本的chrome？博主收集了几个下载chrome老版本的网站，其中哪个下载的是原版的就不得而知了。http://www.slimjet.com/chrome/google-chrome-old-version.phphttp://google_chrome.en.downloadastro.com/old_versions/http://filehippo.com/zh/download_google_chrome/http://www.chromedownloads.net/下面这个网址大多是Mac的多些，Windows下的很缺，但是有好多其他的软件下载，可以去看看：uptodown 2. 所有版本firefox下载火狐相对比较容易找，这里也贴出来其ftp链接：http://ftp.mozilla.org/pub/firefox/releases/至于火狐的版本与selenium的对应关系，确实没有这方面合适的资料，不过建议selenium 2.53以及以下的朋友，用47以下的火狐。 3. 所有版本chromedriver下载chromedriver的版本也不容易找：http://chromedriver.storage.googleapis.com/index.html其中各版本下的notes.txt中说明了该版本以及以前一些版本支持的chrome浏览器版本，不过，老司机早就给你整理了一份一目了然的表格：selenium之 chromedriver与chrome版本映射表这样，该下载哪个版本的chrome与chromedriver是不是就很清楚了。 4. 所有版本selenium以及IEDriverServer下载最后，当然还有selenium和IEDriverServer，Python版的selenium直接pip就可以了，下面的链接里主要是Java版的和.NET版的：http://selenium-release.storage.googleapis.com/index.html好吧，暂时就这些吧。如果你有更好地资源站，也请告知下博主，资源共享，共同进步。博文转自：huilan_same","categories":[{"name":"自动化测试","slug":"自动化测试","permalink":"luoxiao.cf/categories/自动化测试/"}],"tags":[{"name":"Selenium","slug":"Selenium","permalink":"luoxiao.cf/tags/Selenium/"}]},{"title":"DataTables与requireJs冲突的解决","slug":"2018-01-29-js-datatables","date":"2018-01-29T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2018/01/29/2018-01-29-js-datatables/","link":"","permalink":"luoxiao.cf/2018/01/29/2018-01-29-js-datatables/","excerpt":"","text":"DataTables与requireJs冲突的解决当我在项目中同时使用requireJs和DataTablesJs的时候，提示了一个错误**$(…).DataTable is not a function**，我通过尝试修改源码并解决了这个问题。具体操作步骤如下:先看datatables.js源码总体的一个结构:123456(function( factory ) &#123;//......&#125;(function() &#123; \"use strict\"; //....... return $.fn.dataTable;&#125;));修改为以下两种任意一种形式都可以（一本机测试为准）:123456(function( factory ) &#123;//......&#125;(function( $ ) &#123; \"use strict\"; //....... return $.fn.dataTable;&#125;($));123456(function( factory ) &#123;//......&#125;(function( ) &#123; \"use strict\"; //....... return $.fn.dataTable;&#125;());","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"DataTable","slug":"DataTable","permalink":"luoxiao.cf/tags/DataTable/"}]},{"title":"Ubuntu sublime不能输入中文","slug":"2017-9-19-ubuntu-sublime","date":"2017-09-19T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/09/19/2017-9-19-ubuntu-sublime/","link":"","permalink":"luoxiao.cf/2017/09/19/2017-9-19-ubuntu-sublime/","excerpt":"","text":"ubuntu 16.0.4使用sublime不能输入中文 解决从github上下载下面这个玩意1$ git clone https://github.com/lyfeyaj/sublime-text-imfix.git进入目录1$ cd sublime-text-imfix/像这样把文件拷到对应目录下12$ sudo cp ./lib/libsublime-imfix.so /opt/sublime_text/$ sudo cp ./src/subl /usr/bin/编写一个sh脚本启动sublime1$ vim sublime12#!/bin/bashLD_PRELOAD=/opt/sublime_text/libsublime-imfix.so subl","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"luoxiao.cf/tags/Ubuntu/"}]},{"title":"SpringMVC乱码","slug":"2017-8-22-SpringMVC乱码","date":"2017-08-22T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/22/2017-8-22-SpringMVC乱码/","link":"","permalink":"luoxiao.cf/2017/08/22/2017-8-22-SpringMVC乱码/","excerpt":"","text":"解决SpringMvc的POST和GET乱码问题 POST乱码在web.xml配置SpringMVC提供的Filter就可以解决POST乱码了123456789101112131415161718&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; GET乱码Filter只针对POST请求的，tomacat对GET和POST请求处理方式是不同的，要处理针对GET请求的编码问题，则需要改tomcat的server.xml配置文件，如下：12&lt;Connector connectionTimeout=\"20000\" port=\"8080\" protocol=\"HTTP/1.1\" redirectPort=\"8443\"/&gt;注意:如果你用的是MyEclipse,到tomcat的根目录下修改server.xml,运行即可生效,如果你是eclipse需要在eclipse中修改server.xml才能生效","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"luoxiao.cf/tags/Spring/"}]},{"title":"HDFS and YARN","slug":"2017-8-21-hadoop-HDFS和YARN","date":"2017-08-21T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/21/2017-8-21-hadoop-HDFS和YARN/","link":"","permalink":"luoxiao.cf/2017/08/21/2017-8-21-hadoop-HDFS和YARN/","excerpt":"","text":"本文主要说明HDFS和MapReduce两门技术的入门使用.因为MapReduce太长,后面就简称MR,我们到官网下载hadoop-2.4.1.tar.gz,并解压,进入到hadoop-2.4.1/etc/hadoop,分别配置以下几个文件hadoop-env.shcore-site.xmlhdfs-site.xmlmapred-site.xml.template(重命名为mapred-site.xml)yarn-site.xml 配置 hadoop-env.sh需要到hadoop-env.sh中配置Java环境,值得注意的是在hadoop-env.sh文件中使用的变量名不会生效,我们在这里写死它,查询Java存放路径ehco $JAVA_HOME 配置 core-site.xmlfs.defaultFS: 指定HADOOP所使用的文件系统访问的URIhadoop.tmp.dir: 指定hadoop运行时产生文件的存储目录12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zyh:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/media/zyh/software/hadoop-2.4.1/data/&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;这里hdfs://zyh:9000/中的zyh相当于localhost,因为我在/etc/hosts文件中修改了 配置 hdfs-site.xmldfs.replication: 指定HDFS切块(blk)的副本数量123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 mapred-site.xmlmapreduce.framework.name:指定MR的运行的资源由yarn分配123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 yarn-site.xmlyarn.resourcemanager.hostname: 指定YARN的ResourceManager的地址,其实就是我本机zyh(127.0.0.1)yarn.nodemanager.aux-services: reducer获取数据的方式12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;zyh&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 将hadoop添加到环境变量12345$ vim /etc/proflie$ export JAVA_HOME=/usr/java/jdk1.6.0_45$ export HADOOP_HOME=/media/zyh/software/software for linux/hadoop-2.4.1$ export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin$ source /etc/profile这个是我本地的profile作参考 初始化namenode（又叫格式化namenode）$ hdfs namenode -format (hadoop namenode -format)弹出一大片log,英语很nice可以稍微看看,不好的可以键入命令echo $?来校验上一个操作是否正确,如果是0就是正确的 启动hadoop如果你格式化namenode已成功,就可以运行hadoop了！在hadoop-2.4.1/sbin目录下面,hadoop为我们提供了很多命令start-all.sh : 启动dfs和yarnstart-dfs.sh : 启动dfsstart-yarn.sh : 启动yarn作为入门的新手,我们当然是一步一步启动,先启动HDFS,再启动YARN.并观察启动后的运行状态 启动HDFS$ sbin/start-dfs.sh启动报错,如果说本地ssh拒绝访问,那么我们需要安装openssh-server,如果没有报错请无视！安装后再次键入命令.HDFS会提示会让我们输入很多次密码并确认,因为部署的是分布式系统,尽管我们只使用了一台电脑,但是HDSF不知道,他会使用SSH去访问我们远程的服务器(本例中远程服务器就是本机啦！)之后它会启动一系列进程,这些进程分别是:NameNodeDataNodeSecondaryNameNode 启动YARN$ sbin/start-yarn.shyarn也是一样的一路yes并输入密码即可,yarn会先后开启两个进程,他们分别是ResourceManagerNodeManager 查看启动进程的状态$ jps 使用HDFS打开网址http://zyh:50070 （HDFS管理界面）,在该界面我们可以查看HDFS的运行状态信息,在Browse the file system一栏中可以查看HDFS目录结构因为我们什么都没有做,所以在Browse Directory下面什么都没有,HDFS的目录结构和linux的目录结构差不多,都是以/为根目录.编写test.txt并上传到HDFS中(被上传的文件会被分为若干个切块，分别放于不同的datanode中)键入命令,将test.txt上传到hdfs上$ hadoop fs -put test.txt hdfs://zyh:9000/再次打开http://zyh:50070,查看HDFS就会发现多出了一个test.txt文件 使用MR到hadoop-2.4.1/share/hadoop/mapreduce目录下可以找到hadoop-mapreduce-examples-2.4.1.jar,这个jar是Hadoop为我们编写的mapreduce小例子,我们可以使用它来做一些测试我们使用MR做一些小测试！打印pi的值到hadoop-mapreduce-examples-2.4.1.jar根目录pi : 方法名5 : 参数1100 : 参数2$ hadoop jar hadoop-mapreduce-examples-2.4.1.jar pi 5 100统计我们之前test.txt中字符串出现的次数$ cat test.txt这一次我们在HDFS文件系统中创建一些目录,将test.txt上传到指定目录中去创建/wordcount/input用来放被统计的文件,值得注意的是我们必须先创建/wordcount,才能再创建/input$ hadoop fs -mkdir /wordcount$ hadoop fs -mkdir /wordcount/input同理创建创建/wordcount/output用来放被统计后的输出文件$ hadoop fs -mkdir /wordcount$ hadoop fs -mkdir /wordcount/out将test.txt文件上传到HDFS文件系统中的input目录$ hadoop fs -put text.txt /wordcount/input使用命令对test.txt进行统计,wordcount:指定本次运行的是统计方法,/wordcount/input:指定被统计的文件,/wordcount/out: 指定统计后的输出目录wordcount : 方法名/wordcount/input : 参数1/wordcount/out : 参数2$ hadoop jar hadoop-mapreduce-examples-2.4.1.jar wordcount /wordcount/input /wordcount/out到/wordcount/out查看MR分析结果,并使用命令下载文件$ hadoop fs -ls /wordcount/out/$ hadoop fs -get /wordcount/out part-r-00000查看MR分析结果","categories":[{"name":"大数据","slug":"大数据","permalink":"luoxiao.cf/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"luoxiao.cf/tags/Hadoop/"}]},{"title":"Maven骨架无法生成在IDEA中","slug":"2017-8-20-maven骨架无法生成","date":"2017-08-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/15/2017-8-20-maven骨架无法生成/","link":"","permalink":"luoxiao.cf/2017/08/15/2017-8-20-maven骨架无法生成/","excerpt":"","text":"在IDEA中maven骨架生成速度缓慢，只见进度条在疯狂运动就不见导入成功,是不是有点崩溃？解决办法:找到 file -&gt; other settings -&gt; Default settings … -&gt; maven -&gt; runner,在VM Options中添加-DarchetypeCatalog=internal","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[]},{"title":"JQuery弹出框小Demo","slug":"2017-8-15-JQuery弹出框","date":"2017-08-15T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/15/2017-8-15-JQuery弹出框/","link":"","permalink":"luoxiao.cf/2017/08/15/2017-8-15-JQuery弹出框/","excerpt":"","text":"编写css123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156&lt;style type=\"text/css\"&gt; .gray&#123; width:100%; height:100%; background:rgba(0,0,0,0.3); position:absolute; top:0px; left:0; display:none; z-index:99; &#125; .popup&#123; width:532px; height:auto; background-color:#fff; position:absolute; z-index:100; border:1px solid #ebeaea; left:400px; top:96px; display:none; &#125; .popup .top_nav&#123; width:532px; height:46px; background-image: url(images/popup_top_bj.jpg); border-bottom:1px solid #ebeaea; position:relative; cursor:move; &#125; .popup .top_nav i&#123; width:35px; height:35px; background: #ccc; position:absolute; top:6px; left:8px; display:block; &#125; .popup .top_nav span&#123; font:18px/18px 'microsoft yahei'; color:#707070; display:block; position:absolute; top:13px; left:50px; &#125; .popup .top_nav a.guanbi &#123; background:url(images/popup_guanbi.png) repeat 0px 0px; width:35px; height: 35px; display: block; position:absolute; top:8px; right:10px; cursor:pointer; &#125; .popup .top_nav a.guanbi span &#123; display: none;&#125; .popup .top_nav a.guanbi:hover&#123; background: url(images/popup_guanbi.png) repeat 0px -35px; &#125; .popup .min&#123;height:auto;padding: 20px;&#125; .box_mid&#123; width: 100%; height: 50px; margin-top: 20px; border-bottom: 1px solid silver; &#125; .checkbox &#123; position: relative; display: inline-block; &#125; .checkbox input &#123; position: absolute; left: 0; top: 0; width: 100%; height: 100%; z-index: 5; opacity: 0; cursor: pointer; &#125; input[type=\"checkbox\"], input[type=\"radio\"] &#123; box-sizing: border-box; padding: 0; &#125; .checkbox label &#123; &#125; .checkbox label &#123; width: 55px; height: 25px; background: #ccc; position: relative; display: inline-block; border-radius: 46px; -webkit-transition: 0.4s; transition: 0.4s; &#125; .checkbox label:after &#123; content: ''; position: absolute; width: 25px; height: 25px; border-radius: 100%; left: 0; top: 0px; z-index: 2; background: #fff; box-shadow: 0 0 5px rgba(0, 0, 0, 0.2); -webkit-transition: 0.4s; transition: 0.4s; &#125; .checkbox input:checked + label &#123; background: #4BD865; &#125; .checkbox input:checked + label:after &#123; left: 30px; &#125; .box_mid span&#123; position: relative; top: -10px; color: #888; left: 20px; &#125; .qr_code&#123; width: 100%; height:200px; &#125; .qr_code .left_box&#123; width: 50%; height:100%; /*background: blue;*/ float: left; &#125; .left_box&#123; justify-content: center; /*子元素水平居中*/ align-items: center; /*子元素垂直居中*/ display: -webkit-flex; &#125; .right_box&#123; width: 50%; height: 100%; /*background: red;*/ float: right; &#125; .right_box img&#123; width: 160px; height: 160px; margin-top: 30px; margin-left: 40px; &#125; .box_mids&#123; margin-top: 0px; border:0px; height: 100%; &#125; .foot p&#123; &#125;&lt;/style&gt; 编写body123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;body&gt; &lt;input type=\"button\" value=\"点击\" onclick=\"jQuery.tanchuk();\" /&gt; &lt;div id=\"qrcodeGray\" class=\"gray\"&gt;&lt;/div&gt; &lt;div class=\"popup\" id=\"popup\"&gt; &lt;div class=\"top_nav\" id='top_nav'&gt; &lt;div align=\"center\"&gt; &lt;i&gt;&lt;/i&gt; &lt;span&gt;视点共享&lt;/span&gt; &lt;a class=\"guanbi\"&gt;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"min\"&gt; &lt;div style=\"width:100%;height:400px;\"&gt; &lt;!-- 打开共享链接 --&gt; &lt;div class=\"box_mid\"&gt; &lt;div class=\"checkbox\"&gt; &lt;input type=\"checkbox\"/&gt; &lt;label&gt;&lt;/label&gt; &lt;/div&gt; &lt;span&gt;打开共享链接&lt;/span&gt; &lt;/div&gt; &lt;!-- 二维码 --&gt; &lt;div class=\"qr_code\"&gt; &lt;div class=\"left_box\"&gt; &lt;div&gt; &lt;p&gt;将此项目用以下链接与他人共享&lt;/p&gt; &lt;p&gt; &lt;input type=\"text\" value=\"\"/&gt; &lt;input type=\"button\" value=\"复制\" /&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"right_box\"&gt; &lt;img src=\"jiam.png\" alt=\"\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"foot\"&gt; &lt;div&gt;&lt;p&gt;隐私设置&lt;/p&gt;&lt;/div&gt; &lt;!-- 打开共享链接 --&gt; &lt;div class=\"box_mid box_mids\"&gt; &lt;div class=\"checkbox\"&gt; &lt;input type=\"checkbox\"/&gt; &lt;label&gt;&lt;/label&gt; &lt;/div&gt; &lt;span&gt;需要密码才能访问此公共链接&lt;/span&gt; &lt;/div&gt; &lt;div&gt; &lt;p&gt; &lt;input type=\"text\" value=\"\"/&gt; &lt;input type=\"button\" value=\"设置密码\" /&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt; JQuery1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;script type=\"text/javascript\"&gt; //窗口效果 //点击按钮显示 jQuery.tanchuk= function()&#123; $(\"#qrcodeGray\").show(); $(\"#popup\").show();//查找ID为popup的DIV show()显示#qrcodeGray tc_center(); &#125;; //点击关闭按钮 $(\"a.guanbi\").click(function()&#123; $(\"#qrcodeGray\").hide(); $(\"#popup\").hide();//查找ID为popup的DIV hide()隐藏 &#125;) //窗口水平居中 $(window).resize(function()&#123; tc_center(); &#125;); function tc_center()&#123; var _top=($(window).height()-$(\".popup\").height())/2; var _left=($(window).width()-$(\".popup\").width())/2; $(\".popup\").css(&#123;top:_top,left:_left&#125;); &#125; &lt;/script&gt;&lt;script type=\"text/javascript\"&gt; $(document).ready(function()&#123; $(\".top_nav\").mousedown(function(e)&#123; //鼠标按下事件 $(this).css(\"cursor\",\"move\");//改变鼠标指针的形状 鼠标移动 var offset = $(this).offset();//DIV在页面的位置 offset() 方法返回或设置匹配元素相对于文档的偏移（位置） var x = e.pageX - offset.left;//获得鼠标指针离DIV元素左边界的距离 pageX() 属性是鼠标指针的位置，相对于文档的左边缘。 var y = e.pageY - offset.top;//获得鼠标指针离DIV元素上边界的距离 pageY() 属性是鼠标指针的位置，相对于文档的上边缘。 $(document).bind(\"mousemove\",function(ev)&#123; //绑定鼠标的移动事件，因为光标在DIV元素外面也要有效果，所以要用doucment的事件，而不用DIV元素的事件 //bind() 方法为被选元素添加一个或多个事件处理程序，并规定事件发生时运行的函数 $(\".popup\").stop();//加上这个之后 var _x = ev.pageX - x;//获得X轴方向移动的值 var _y = ev.pageY - y;//获得Y轴方向移动的值 $(\".popup\").animate(&#123;left:_x+\"px\",top:_y+\"px\"&#125;,10); &#125;); &#125;); $(document).mouseup(function() &#123; //mouseup 鼠标松开时 $(\".popup\").css(\"cursor\",\"default\"); // default 默认光标（通常是一个箭头） $(this).unbind(\"mousemove\"); // unbind() 方法移除被选元素的事件处理程序。mousemove触发 &#125;); &#125;);&lt;/script&gt; 最终样式展示","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[]},{"title":"Hibernate one to many 和many to one 有什么区别？","slug":"2017-8-13-hibernate一和多","date":"2017-08-13T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/13/2017-8-13-hibernate一和多/","link":"","permalink":"luoxiao.cf/2017/08/13/2017-8-13-hibernate一和多/","excerpt":"","text":"室友问了我个奇怪的问题问：我们在配Hibernate的时候有也有,为什么会有两个？问：我的意思one to many 要是反过来不就是many to one了吗？问：我要是把相关的配置 调换一下，那么one to many和many to one不就是可以互换了吗？从字面意思上看，one to many 是一对多,many to one是多对一。很明显就是区别（说实话，问这样的问题，一开始我也有点诧异，后来想了想还是不对劲）。从实际开发角度来说，一对多反过来就是多对一。我们以学生和班级为例。学生对班级是many to one，班级对学生就是one to many。那么问题来了，到底我们配置班级方的one to many？还是配置学生方的many to one呢？这个问题就要看实际开发需求了。在这个需求中，从经验上可以想象，我们查看班级的时候，并不一定要看到每个学生的信息。因为学生很多。但我们查看学生的时候，可能想看班级的信息，因为一个学生对应一个班级。在这种情况下，我们当然是配置学生方的many to one，然后做级联操作。以便取出学生时取出对应班级。而事实上，大多数情况下，many to one比one to many的应用也更为防范，这主要是基于一个效率考虑","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"luoxiao.cf/tags/Hibernate/"}]},{"title":"NodeJS学习","slug":"2017-8-10-nodejs","date":"2017-08-10T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/10/2017-8-10-nodejs/","link":"","permalink":"luoxiao.cf/2017/08/10/2017-8-10-nodejs/","excerpt":"","text":"安装安装Nodejs，不用说他是核心包 $ sudo apt install nodejs-legacy $ node -v 安装npm,npm是什么东东？npm其实是Node.js的包管理工具(package manager).为啥我们需要一个包管理工具呢？因为我们在Node.js上开发时，会用到很多别人写的JavaScript代码。如果我们要使用别人写的某个包，每次都根据名称搜索一下官方网站，下载代码，解压，再使用，非常繁琐。于是一个集中管理的工具应运而生：大家都把自己开发的模块打包后放到npm官网上，如果要使用，直接通过npm安装就可以直接用，不用管代码存在哪，应该从哪下载。更重要的是，如果我们要使用模块A，而模块A又依赖于模块B，模块B又依赖于模块X和模块Y，npm可以根据依赖关系，把所有依赖的包都下载下来并管理起来。否则，靠我们自己手动管理，肯定既麻烦又容易出错。 $ sudo apt install npm $ npm -v 入门Demo进入一个目录新建一个hello.js文件,use strict指的是以严格模式运行JavaScript代码，避免各种潜在陷阱. 'use strict' console.log(\"hello nodejs!!\"); 在命令行下输入node hello.js,就可以看到哦啊打印出来的信息了! $ node hello.js 如果不喜欢在文件中使用use strict,我们可以在命令行中使用! $ node --use_strict hello.js 搭建Node开发环境如果只是用vim,EditPlus,sublime等一些编辑软件写效率太低,Java一般使用Eclipse,Intellij idea,C#使用的集成开发工具isual Studio,Node开发工具哪家强？考察Node.js的集成开发环境，重点放在启动速度快，执行简单，调试方便这三点上。当然，免费使用是一个加分项。综合考察后，我们隆重向大家推荐Node.js集成开发环境： Visual Studio CodeVisual Studio Code由微软出品，但它不是那个大块头的Visual Studio，它是一个精简版的迷你Visual Studio，并且，Visual Studio Code可以跨！平！台！Windows、Mac和Linux通用。 安装可以从Visual Studio Code的官方网站下载并安装最新的1.4版本。网速慢的童鞋请移步国内镜像。windows安装过程中，请务必钩上以下选项：[x] 将“通过Code打开”操作添加到Windows资源管理器目录上下文菜单这将大大提升将来的操作快捷度。Mac在Mac系统上，Finder选中一个目录，右键菜单并没有“通过Code打开”这个操作。不过我们可以通过Automator自己添加这个操作。先运行Automator，选择“服务”：然后，执行以下操作：1.在右侧面板选择“服务”收到选定的“文件夹”，位于“Finder.app“，该选项是为了从Finder中接收一个文件夹；2.在左侧面板选择”实用工具“，然后找到”运行Shell脚本“，把它拽到右侧面板里；3.在右侧”运行Shell脚本“的面板里，选择Shell”/bin/bash“，传递输入“作为自变量”，然后修改Shell脚本如下： for f in \"$@\" do open -a \"Visual Studio Code\" \"$f\" done 保存为“Open With VSCode”后，打开Finder，选中一个文件夹，点击右键，“服务”，就可以看到“Open With VSCode”菜单：linux可以自行到官网找到tar.gz安装包下载解压即可使用也可以使用这条命令,后面的网址是我从官网上copy下来的 $ wget https://vscode.cdn.azure.cn/stable/cb82febafda0c8c199b9201ad274e25d9a76874e/code-stable-code_1.14.2-1500506907_amd64.tar.gz 解压双击/media/zyh/software/software for linux/VSCode-linux-x64/code文件即可打开应用使用 运行和调试在VS Code中，我们可以非常方便地运行JavaScript文件。VS Code以文件夹作为工程目录（Workspace Dir），所有的JavaScript文件都存放在该目录下。此外，VS Code在工程目录下还需要一个.vscode的配置目录，里面存放里VS Code需要的配置文件。假设我们在C:\\Work\\目录下创建了一个hello目录作为工程目录，并编写了一个hello.js文件，则该工程目录的结构如下：1234567hello/ &lt;-- workspace dir|+- hello.js &lt;-- JavaScript file|+- .vscode/ &lt;-- VS Code config | +- launch.json &lt;-- VS Code config file for JavaScript可以用VS Code快速创建launch.json，然后修改如下： { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Run hello.js\", \"type\": \"node\", \"request\": \"launch\", \"program\": \"${workspaceRoot}/hello.js\", \"stopOnEntry\": false, \"args\": [], \"cwd\": \"${workspaceRoot}\", \"preLaunchTask\": null, \"runtimeExecutable\": null, \"runtimeArgs\": [ \"--nolazy\" ], \"env\": { \"NODE_ENV\": \"development\" }, \"externalConsole\": false, \"sourceMaps\": false, \"outDir\": null } ] } 有了配置文件，即可使用VS Code调试JavaScript。视频演示：推荐链接https://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"NodeJS","slug":"NodeJS","permalink":"luoxiao.cf/tags/NodeJS/"}]},{"title":"markdown使用","slug":"2017-8-10-markdown","date":"2017-08-10T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/10/2017-8-10-markdown/","link":"","permalink":"luoxiao.cf/2017/08/10/2017-8-10-markdown/","excerpt":"","text":"传送门https://www.zybuluo.com/AntLog/note/63228#2-次常用标记https://www.zybuluo.com/mdeditor","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[]},{"title":"liquid","slug":"2017-8-10-jekyll-liquid","date":"2017-08-10T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/10/2017-8-10-jekyll-liquid/","link":"","permalink":"luoxiao.cf/2017/08/10/2017-8-10-jekyll-liquid/","excerpt":"","text":"传送门https://alfred-sun.github.io/blog/2015/01/10/jekyll-liquid-syntax-documentation/","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"Jekyll","slug":"Jekyll","permalink":"luoxiao.cf/tags/Jekyll/"}]},{"title":"FFmpeg下的录屏录音","slug":"2017-8-5-ffmpeg-录屏录音","date":"2017-08-05T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/05/2017-8-5-ffmpeg-录屏录音/","link":"","permalink":"luoxiao.cf/2017/08/05/2017-8-5-ffmpeg-录屏录音/","excerpt":"","text":"前面我们安装了FFmpeg,FFmpeg还提供了录屏+录音的功能，可以方便录制直播视频,教学视频等等.我们使用alsa录音功能，alsa driver驱动被内嵌志linux内核中, 基本上只要不是内核本版太老，是不需要再进行安装的.如果小伙伴们发现机器上不能使用alsa,可以去安装一下alsa的三个库alsa-driver,alsa-lib,alsa-utils到FFmpeg官网查看相应资料FFmpeg在linux中录音录屏发现了三条指令$ ffmpeg -video_size 1024x768 -framerate 25 -f x11grab -i :0.0+100,200 output.mp4$ ffmpeg -video_size 1024x768 -framerate 25 -f x11grab -i :0.0+100,200 -f alsa -ac 2 -i hw:0 output.mkv$ ffmpeg -video_size 1024x768 -framerate 25 -f x11grab -i :0.0+100,200 -f pulse -ac 2 -i default output.mkv 参数详解-video_size 录制视频的分辨率$ echo $(xdpyinfo|grep ‘dimensions:’ | awk ‘{print $2}’)-f device 指定设备,这里我们使用x11grab设备去录屏，使用alsa设备去录音,请在使用前确保该设备被安装,并且确保在FFmpeg编译时已经添加到FFmpeg的devices列表中$ ffmpeg -devices-i :0.0+100,200 从左上角开始，向x轴偏移100,y轴偏移200,我们可以不用偏移-ac 设置音频通道数-i hw:1 指定音频设备在linux中使用alsamixer可以打开alsa控制面板$ alsamixer按F6选择声卡，注意前面的标号根据实际情况选择,我的音频设备是1号,与hw:后面的1相对应到下面的界面后,按F4跳转到捕获音频界面在这里我们可以调整录音时音量的大小,左右切换至capture,上下调整音量 FFmpeg录屏录音x11grab录屏$ ffmpeg -video_size 1920x1080 -framerate 25 -f x11grab -i :0.0+0,0 ~/output.mp4alsa录音$ ffmpeg -f alsa -ac 2 -i hw:1 ~/out.wav录屏+录音$ ffmpeg -video_size 1920x1080 -framerate 25 -f x11grab -i :0.0+0,0 -f alsa -ac 2 -i hw:1 out.mp4 录屏演示","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","permalink":"luoxiao.cf/tags/FFmpeg/"}]},{"title":"highlight网页代码高亮显示","slug":"2017-8-5-highlight","date":"2017-08-05T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/08/05/2017-8-5-highlight/","link":"","permalink":"luoxiao.cf/2017/08/05/2017-8-5-highlight/","excerpt":"","text":"使用引入highlight.min.js和monokai_sublime.min.css，使用initHighlightingOnLoad()初始化.123&lt;link href=&quot;http://cdn.bootcss.com/highlight.js/8.0/styles/monokai_sublime.min.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;script src=&quot;http://cdn.bootcss.com/highlight.js/8.0/highlight.min.js&quot;&gt;&lt;/script&gt;&lt;script &gt;hljs.initHighlightingOnLoad();&lt;/script&gt;使用&lt;pre&gt;和&lt;code class=&quot;html&quot;&gt; 指定文本的内容信息例如&lt;pre&gt;&lt;code class=&quot;js&quot;&gt;code.....&lt;/code&gt;&lt;/pre&gt; 展示js代码: var arr = new Array(); var input_element = document.getElementById(\"input\"); alert(input_element.value); java代码: class Demo{ public static void main(String[] args){ Systemm.out.println(\"java\"); } } C代码: int main(){ printf(\"c\"); return 1; } linux $ echo $(xdpyinfo|grep 'dimensions:' | awk '{print $2}') 官网 https://highlightjs.org/download/","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"Jekyll","slug":"Jekyll","permalink":"luoxiao.cf/tags/Jekyll/"}]},{"title":"redis的使用总结","slug":"2017-7-31-redis","date":"2017-07-31T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/07/31/2017-7-31-redis/","link":"","permalink":"luoxiao.cf/2017/07/31/2017-7-31-redis/","excerpt":"","text":"redis是一个开源的数据存储系统，与memcached一样内部都是使用内存来存储数据，但与之不同的是redis会将数据定期写入磁盘，防止数据丢失。redis内部可以存放字符串、链表、散列表、集合。它支持主从同步，我们可以从主服务器将数据同步到任意数量的其他服务器上。当内存满了的时候，它会采用LUR算法来置换出长久未被使用的数据，以此来确保数据库中一直存放的是最有用的数据。 使用下载3.2.100-version-msi安装包(windows)https://github.com/MicrosoftArchive/redis/tags安装redis,一路next默认端口6379链接redis,先进入安装目录,找到redis-cli.exeredis-cli.exe -h 127.0.0.1 -p 6379存入数据setex mykey 60 “redis”取数据get mykeyJava 编程需要两个jar Jedis.jar和commons-pool.jar;(tip:可以到maven repository 中搜索相关jar,点开版本后下面有Compile Dependencies一栏可以看到依赖其他的jar包)使用Jedis链接redis Jedis jedis = new Jedis(\"127.0.0.1\",\"6379\"); // jedis.auth(\"admin\"); jedis.set(\"myKey\", \"testStr\"); jedis.setex... jedis.get...","categories":[{"name":"后端","slug":"后端","permalink":"luoxiao.cf/categories/后端/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"luoxiao.cf/tags/Redis/"}]},{"title":"zTree的使用总结","slug":"2017-7-26-zTree","date":"2017-07-26T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/07/26/2017-7-26-zTree/","link":"","permalink":"luoxiao.cf/2017/07/26/2017-7-26-zTree/","excerpt":"","text":"zTree是开源的一个插件，它是一款JQuery实现的多功能“树型”列表插件。 使用需要引入下面的库123&lt;link rel=&quot;stylesheet&quot; href=&quot;zTreeStyle/zTreeStyle.css&quot; type=&quot;text/css&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;jquery-1.4.2.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;jquery.ztree.core-3.x.js&quot;&gt;&lt;/script&gt;编写一个div&lt;div&gt;&lt;ul id=&quot;treeDemo&quot; class=&quot;ztree&quot;&gt;&lt;/ul&gt;&lt;/div&gt;编写js var zTree ; var setting = { callback: { onCheck: zTreeOnCheck, onClick: zTreeOnClick }, check:{ enable : true } }; $.ajax({ url:\"/hrm/GetJson\", type:\"get\", data:{}, dataType:\"jsonp\", jsonp:\"callBackJsonp\", success : success, error : function(a,b,c){ alert(\"error\"+c); } }); function success(nodes){ zTree = $.fn.zTree.init($(\"#treeDemo\"), setting, nodes); } function zTreeOnCheck(event,treeNode,treeId){ alert(\"checked\"); } function zTreeOnClick(event,treeNode,treeId){ alert(\"Clicked\"); } function getTree(){ return zTree; } /* // nodes样式 var nodes = [ {id:1, pId:0, name: \"父节点1\"}, {id:11, pId:1, name: \"子节点1\"}, {id:12, pId:1, name: \"子节点2\"} ]; */ 关于后台数据由于跨域请求，所以用了jsonp，如果不跨域可以去掉，后台使用JSONObject和JSONArray拼的JSON串返回到界面并不能被zTree解析(自己当时做的时候是这样的)，用list拼为nodes,再使用out.println(nodes); arrayList.put(\"{id:12, pId:1, name: \"子节点2\"}\"); 问题在父窗体上如何触发子窗体的zTree的onCheck方法在父窗体使用js获取子窗体对象，parent.$(&quot;#iframeID&quot;).get(0).contentWindow; get(0)将Jquery对象转换为普通的js对象!!!在子窗体定义一个获取zTree的方法getTree()，获取到子窗体调用getTree()方法拿到对象，之后相干啥就可以干了。获取选中的节点是getChangeCheckedNodes()我怎么知道zTree有getChangeCheckedNodes教给你一个办法，获取zTree对象，用console.log(zTree)。在控制台就可以看到zTree有什么方法了！","categories":[{"name":"前端","slug":"前端","permalink":"luoxiao.cf/categories/前端/"}],"tags":[{"name":"zTree","slug":"zTree","permalink":"luoxiao.cf/tags/zTree/"}]},{"title":"在ubuntu1.6.04安装ffmpeg","slug":"2017-7-23-在ubuntu1.6.04安装ffmpeg","date":"2017-07-23T00:00:00.000Z","updated":"2019-11-13T04:56:08.670Z","comments":true,"path":"2017/07/23/2017-7-23-在ubuntu1.6.04安装ffmpeg/","link":"","permalink":"luoxiao.cf/2017/07/23/2017-7-23-在ubuntu1.6.04安装ffmpeg/","excerpt":"","text":"FFmpeg的名称来自MPEG视频编码标准，前面的“FF”代表“Fast Forward”，FFmpeg是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。可以轻易地实现多种视频格式之间的相互转换。FFmpeg的用户有Google，Facebook，Youtube，优酷，爱奇艺，土豆等。组成libavformat：用于各种音视频封装格式的生成和解析，包括获取解码所需信息以生成解码上下文结构和读取音视频帧等功能，包含demuxers和muxer库；libavcodec：用于各种类型声音/图像编解码；libavutil：包含一些公共的工具函数；libswscale：用于视频场景比例缩放、色彩映射转换；libpostproc：用于后期效果处理；ffmpeg：是一个命令行工具，用来对视频文件转换格式，也支持对电视卡实时编码；ffsever：是一个HTTP多媒体实时广播流服务器，支持时光平移；ffplay：是一个简单的播放器，使用ffmpeg 库解析和解码，通过SDL显示；需要准备的东西ffmpeg-2.4.1.tar.bz2 下载地址https://www.ffmpeg.org/releases/下载ffmpeg所需要的所有依赖库文件libx264、libfaac、libmp3lame、libtheora、libvorbis、libxvid、libxext、libxfixes 安装libx264因为libx264依赖yasm,所以我们先安装yasmsudo apt-get install yasm使用aptitude安装libx264sudo aptitude install libx264-dev有的同学没有aptitude,下载一个就行了sudo apt-get install aptitudetar.gz安装包安装libx264，我们需要去下载last_x264.tar.bz2tar -zxvf last_x264.tar.bz2进入目录进行编译并安装./configure --enable-shared --enable-picmakemake install 安装ffmpeg我们一个一个来安装ffmpeg的依赖，请确保每个依赖都安装成功！(可以使用echo $?来判断是否成功)libfaacsudo aptitude install libfaac-devlibmp3lame-devsudo aptitude install libmp3lame-devlibtheora-devsudo aptitude install libtheora-devlibvorbis-devsudo aptitude install libvorbis-devlibxvidcore-devsudo aptitude install libxvidcore-devlibxext-devsudo aptitude install libxext-devlibxfixes-devsudo aptitude install libxfixes-devlibasound2-dev (lz后续想使用alsa录音,遇到了麻烦FFMPEG: Unknown input format:‘alsa’,点击可以跳转到lz参考的解决办法)sudo apt-get install libasound2-dev安装ffmpegtar -xjvf ffmpeg-2.4.1.tar.bz2cd ffmpeg-2.4.1编译并安装,–prefix指定安装后的目录(方便卸载),后面我添加上了alsa的支持**–enable-indev=alsa,–enable-outdev=alsa**$ ./configure --prefix=/usr/local/ffmpeg --enable-gpl --enable-version3 --enable-nonfree --enable-postproc --enable-pthreads --enable-libfaac --enable-libmp3lame --enable-libtheora --enable-libx264 --enable-libxvid --enable-x11grab --enable-libvorbis --enable-indev=alsa --enable-outdev=alsa$ sudo make$ sudo make install假如我们不添加alsa支持,后续我们可能无法正常录音,录屏还是可以的.使用ffmpeg -devices可以查看ffmpeg支持的设备 配置ffmpeg环境变量这里有很多配置环境变量的方法，可以按照自己的方式配，也可以按照我的配sudo vim /etc/profile在文件里面添加一行代码export FFMPEG_HOME=/usr/local/ffmpegexport PATH=PATH:PATH:PATH:FFMPEG_HOME/bin保存退出vim使环境变量生效source /etc/profile查看环境变量是否可行echo $PATH查看ffmpeg是否可行ffmpeg -version转换命令：ffmpeg -i “20090401010.mp4” -y -ab 32 -ar 22050 -qscale 10 -s 640*480 -r 15 /opt/a.flv-i是要转换文件名-y是覆盖输出文件-ab是音频数据流，大家在百度听歌的时候应该都可以看到 128 64-ar是声音的频率 22050 基本都是这个。-qscale是视频输出质量，后边的值越小质量越高，但是输出文件就越“肥”-s是输出文件的尺寸大小！-r帧率是视频文件中每一秒的帧数，肉眼想看到连续移动图像至少需要15帧。-b码率(比特率):是一个确定整体视频/音频质量的参数，秒为单位处理的字节数，码率和视频质量成正比，在视频文件中中比特率用bps来表达。资料FFmpeg官网： http://www.ffmpeg.orgFFmpeg doc : http://www.ffmpeg.org/documentation.htmlFFmpeg wiki : https://trac.ffmpeg.org/wikiFFmpeg基础: http://wenku.baidu.com/view/296eefcaf90f76c661371af1.htmlFFmpeg基本用法 : http://blog.csdn.net/doublefi123/article/details/24325159FFmpeg参数详解 : http://www.cuplayer.com/player/PlayerCode/FFmpeg/2014/0706/1399.html","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","permalink":"luoxiao.cf/tags/FFmpeg/"}]},{"title":"搭建ftp服务器","slug":"2017-7-23-搭建ftp服务器","date":"2017-07-23T00:00:00.000Z","updated":"2019-11-13T04:56:08.671Z","comments":true,"path":"2017/07/23/2017-7-23-搭建ftp服务器/","link":"","permalink":"luoxiao.cf/2017/07/23/2017-7-23-搭建ftp服务器/","excerpt":"","text":"想要在不同操作系统之间传送数据是不是很头疼，lz操作系统是ubuntu 1.6.04，室友的操作系统是window，想给我传个文件困难重重，而且慢满的要死哟。ftp只要在同一个局域网就可以用呢，而且传输速度那个嗖嗖的，来搞吧！ 安装vsftpd服务安装之前先看看自己有没有安装过类似的软件呢which is vsftpd使用ubuntu的apt-get安装vsftpdsudo apt-get install vsftpd 配置vvsftpd.conf一路安装下去,成功之后去配置一下ftp的配置文件!sudo vim /etc/vsftpd.conf看看我的vsftpd.conf吧！123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168 # Example config file /etc/vsftpd.conf # # The default compiled in settings are fairly paranoid. This sample file # loosens things up a bit, to make the ftp daemon more usable. # Please see vsftpd.conf.5 for all compiled in defaults. # # READ THIS: This example file is NOT an exhaustive list of vsftpd options. # Please read the vsftpd.conf.5 manual page to get a full idea of vsftpd&apos;s # capabilities. # # # Run standalone? vsftpd can run either from an inetd or as a standalone # daemon started from an initscript. listen=NO # # This directive enables listening on IPv6 sockets. By default, listening # on the IPv6 &quot;any&quot; address (::) will accept connections from both IPv6 # and IPv4 clients. It is not necessary to listen on *both* IPv4 and IPv6 # sockets. If you want that (perhaps because you want to listen on specific # addresses) then you must run two copies of vsftpd with two configuration # files. listen_ipv6=YES # # Allow anonymous FTP? (Disabled by default). anonymous_enable=YES # # Uncomment this to allow local users to log in. local_enable=YES # # Uncomment this to enable any form of FTP write command. write_enable=YES # # Default umask for local users is 077. You may wish to change this to 022, # if your users expect that (022 is used by most other ftpd&apos;s) #local_umask=022 # # Uncomment this to allow the anonymous FTP user to upload files. This only # has an effect if the above global write enable is activated. Also, you will # obviously need to create a directory writable by the FTP user. anon_upload_enable=YES # # Uncomment this if you want the anonymous FTP user to be able to create # new directories. anon_mkdir_write_enable=YES # # Activate directory messages - messages given to remote users when they # go into a certain directory. dirmessage_enable=YES # # If enabled, vsftpd will display directory listings with the time # in your local time zone. The default is to display GMT. The # times returned by the MDTM FTP command are also affected by this # option. use_localtime=YES # # Activate logging of uploads/downloads. xferlog_enable=YES # # Make sure PORT transfer connections originate from port 20 (ftp-data). connect_from_port_20=YES # # If you want, you can arrange for uploaded anonymous files to be owned by # a different user. Note! Using &quot;root&quot; for uploaded files is not # recommended! #chown_uploads=YES #chown_username=whoever # # You may override where the log file goes if you like. The default is shown # below. #xferlog_file=/var/log/vsftpd.log # # If you want, you can have your log file in standard ftpd xferlog format. # Note that the default log file location is /var/log/xferlog in this case. #xferlog_std_format=YES # # You may change the default value for timing out an idle session. #idle_session_timeout=600 # # You may change the default value for timing out a data connection. #data_connection_timeout=120 # # It is recommended that you define on your system a unique user which the # ftp server can use as a totally isolated and unprivileged user. #nopriv_user=ftpsecure # # Enable this and the server will recognise asynchronous ABOR requests. Not # recommended for security (the code is non-trivial). Not enabling it, # however, may confuse older FTP clients. #async_abor_enable=YES # # By default the server will pretend to allow ASCII mode but in fact ignore # the request. Turn on the below options to have the server actually do ASCII # mangling on files when in ASCII mode. # Beware that on some FTP servers, ASCII support allows a denial of service # attack (DoS) via the command &quot;SIZE /big/file&quot; in ASCII mode. vsftpd # predicted this attack and has always been safe, reporting the size of the # raw file. # ASCII mangling is a horrible feature of the protocol. #ascii_upload_enable=YES #ascii_download_enable=YES # # You may fully customise the login banner string: #ftpd_banner=Welcome to blah FTP service. # # You may specify a file of disallowed anonymous e-mail addresses. Apparently # useful for combatting certain DoS attacks. #deny_email_enable=YES # (default follows) #banned_email_file=/etc/vsftpd.banned_emails # # You may restrict local users to their home directories. See the FAQ for # the possible risks in this before using chroot_local_user or # chroot_list_enable below. #chroot_local_user=YES # # You may specify an explicit list of local users to chroot() to their home # directory. If chroot_local_user is YES, then this list becomes a list of # users to NOT chroot(). # (Warning! chroot&apos;ing can be very dangerous. If using chroot, make sure that # the user does not have write access to the top level directory within the # chroot) #chroot_local_user=YES #chroot_list_enable=YES # (default follows) #chroot_list_file=/etc/vsftpd.chroot_list # # You may activate the &quot;-R&quot; option to the builtin ls. This is disabled by # default to avoid remote users being able to cause excessive I/O on large # sites. However, some broken FTP clients such as &quot;ncftp&quot; and &quot;mirror&quot; assume # the presence of the &quot;-R&quot; option, so there is a strong case for enabling it. #ls_recurse_enable=YES # # Customization # # Some of vsftpd&apos;s settings don&apos;t fit the filesystem layout by # default. # # This option should be the name of a directory which is empty. Also, the # directory should not be writable by the ftp user. This directory is used # as a secure chroot() jail at times vsftpd does not require filesystem # access. secure_chroot_dir=/var/run/vsftpd/empty # # This string is the name of the PAM service vsftpd will use. pam_service_name=vsftpd # # This option specifies the location of the RSA certificate to use for SSL # encrypted connections. rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem rsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key ssl_enable=NO # # Uncomment this to indicate that vsftpd use a utf8 filesystem. #utf8_filesystem=YES ``` * 扎心了老铁！看看英语有多重要，其实需要改动的地方有5个 ``` anonymous_enable=YES #设置匿名可登录 local_enable=YES #本地用户允许登录 write_enable=YES #用户是否有写的权限 anon_upload_enable=YES #允许匿名用户上传 anon_mkdir_write_enable=YES #允许匿名用户创建目录文件重启服务sudo service vsftpd restart 指定上传和下载目录其实现在已经可以访问了，lz的ip是192.168.1.19，打开浏览器输入ftp://192.168.1.19，我们访问后发现是空。原因是匿名访问默认会访问/srv/ftp,我们磁盘上本身就没动西呢！我们需要上传和下载功能，在/srv/ftp/ 创建了两个文件夹，update和download。把upload的权限设置为可读可写，把download权限设置为可读不可写。上传的文件夹，其他人可以上传到这个文件夹，但是不能有删除权限，试想如果有匿名登录进来的给你删除掉了别人好心给你分享的文件还行吗？下载的文件夹，其他人只能读取这个文件夹，但是不能修改这个文件夹，也就是你要分享给其他人的文件可以放到这里面。sudo mkdir /srv/ftp/uploadsudo mkdir /srv/ftp/downloadsudo chmod -R 777 /srv/ftp/updatesudo chmod -R 755 /srv/ftp/download现在下面的事情就可以实现了：室友有一个非常好看的电影，发网盘又太慢，怎么办？你就把你的FTP地址扔给他让他上传到ftp://&lt;你的IP地址&gt;/upload 文件夹下吧。你剪辑了一个很不错的视频短片，想分享给学校里的小伙伴，你就可以把它扔到你的FTP服务器 /srv/ftp/download 目录下，然后对小伙伴们说，我的视频已经共享了，你们到 ftp://&lt;你的IP地址&gt;/download 里面去下载吧。恩，只要你配置好了FTP，就能享受FTP局域网高速传输，分分钟搞定~~!","categories":[{"name":"Linux","slug":"Linux","permalink":"luoxiao.cf/categories/Linux/"}],"tags":[]},{"title":"github-jekyll搭建个人博客网站","slug":"2017-7-20-github-jekyll-ubuntu-阿里云","date":"2017-07-20T00:00:00.000Z","updated":"2019-11-13T04:56:08.670Z","comments":true,"path":"2017/07/20/2017-7-20-github-jekyll-ubuntu-阿里云/","link":"","permalink":"luoxiao.cf/2017/07/20/2017-7-20-github-jekyll-ubuntu-阿里云/","excerpt":"","text":"使用github和jekyll搭建的博客,运行环境是ubuntu 1.6.04使用的域名是阿里云,听说.tk的也不错，但是我注册的域名在.tk很贵推荐: github Pages和Jekyll入门,Git教程 附录jekyllruby安装rubyGems安装jekyll安装创建并发布博客无样式的博客有样式的博客注册域名 jekyllJekyll是一个简单的博客形态的静态站点生产机器。它有一个模版目录，其中包含原始文本格式的文档，通过 Markdown （或者 Textile） 以及 Liquid 转化成一个完整的可发布的静态网站，你可以发布在任何你喜爱的服务器上。Jekyll 也可以运行在 GitHub Page 上，也就是说，你可以使用 GitHub 的服务来搭建你的项目页面、博客或者网站，而且是完全免费的。安装jekyll之前呢，需要准备下面三样东西。RubyRubyGemsUbuntu 1.6.04 ruby安装具体怎么可以到Ruby官网下载tar.gz安装包安装，这里只提供思路，不要直接粘贴下面代码，我们是有智慧的程序员。（注意有时候make install 报错的话，可能是权限问题，我就碰到咯，只要在代码前面加上sudo 就行了）123456789$ wget https://cache.ruby-lang.org/pub/ruby/2.4/ruby-2.4.1.tar.gz$ tar -zxvf ruby-2.4.1.tar.gz$ cd ruby-2.4.1/$ ./configure$ make$ make install$ make clean$ sudo make distclean$ ruby -v rubyGems安装具体怎么可以到RubyGems官网,和Ruby安装是一样的，不做详细说明12345678$ wget https://rubygems.org/rubygems/rubygems-2.6.12.tgz$ tar -zxvf rubygems-2.6.12.tgz$ cd rubygems-2.6.12$ ./configure$ make$ make install$ make clean$ sudo make distclean jekyll安装jekyll官网,jekyll的doc一栏中也提供了安装的参考1$ sudo gem install jekyll可能会出现一些错误，这是因为少安装了一些插件，这里我也卡了很久，搜索了很多资料，试了很多次1234ERROR: Loading command: update (LoadError)cannot load such file -- zlibERROR: While executing gem ... (NoMethodError)undefined method `invoke_with_build_args&apos; for nil:NilClass解决办法，如果我的方法不行，这个就要摆脱google了,这里面$?是上一个执行命令的执行结果，如果返回0就代表没问题123456789101112131415161718$ sudo apt-get update$ sudo apt-get install git-core curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev python-software-properties$ sudo apt-get install libgdbm-dev libncurses5-dev automake libtool bison libffi-dev$ sudo apt-get install zlib1g-dev$ cd ruby-2.4.1/ext/zlib/$ ruby extconf.rb $ make$ echo $?$ cd /usr/ruby-2.4.1/$ ./configure$ echo $?$ make$ echo $?$ make install$ sudo make install$ echo $?$ make clean$ make distclean 创建并发布博客 无样式的博客创建一个无样式的博客123456$ jekyll new blog$ cd blog$ git init $ git add .$ git commit -m &quot;fist jekyll program&quot;$ git status将博客关联到github远程仓库登录github选择New repository填写Repository name为username.github.io （注意这里的格式，这里的username就是github.com/username中的username）Create repository创建玩之后会给你一个https的链接，比如我的username是Demo233，给我的链接就是https://github.com/Demo233/Demo233.github.io.git将无样式的blog项目远程推送到github仓库中,这里以我的为例，我的username为Demo233,注意修改成自己的123$ cd blog/$ git remote add origin https://github.com/Demo233/Demo233.github.io.git$ git push origin master配置github pages,并运行进入到github.com/Demo233/Demo233.github.io项目中，在Settings选项卡中找到GitHub Pages面板，在Source中选择master branch，并save通过demo233.github.io就可以成功访问我们的界面了！ 有样式的博客本次lz用到的样式是hyde，以这个为例子进行推送部署。在github上网有学多jekyll的样式，你也可以下载并部署,样式链接12$ git clone https://github.com/poole/hyde.git$ cd hyde/我们需要修改一些东西，不然会报错，之前lz同样卡在这里半天，网上搜索了半天资料才解决。修改_config.yml文件中的relative_permalinks: false,如果不改，会报XXX过期的错误，记不得了删除CNAME中的内容，如果不改会提示域名已经存在，因为这个是别人的项目，人家已经在github上注册了，后面会介绍怎么配置自己的域名123456$ rm -rm .git/ # 删除原本的.git 文件使用自己的.git$ git init$ git add .$ git commit -m &quot;beautiful jekyll theme&quot;$ git remote add https://github.com/Demo233/Demo233.github.io.git # 记得退回历史版本之后再进行这里的操作不然会报错的$ git push origin master后面的和无样式博客发布是一样的，只要注意修改_config.yml文件，其他应该没什么大碍了。Tip : 项目里面已经有一个.gitignore我们可以使用它来忽略上传内容 注册域名登录阿里云注册一个帐号，然后选择一个自己喜欢的域名并购买即可。我们买好域名以后，我们就可以去绑定github pages了。lz的域名是zonegood.com，在CNAME中写入zonegood.com并保存12$ cd hyde/$ vim ./CNAME推送更改信息，更新项目123$ git add .$ git commit -m &quot;modify CNAME file ,add zonegood.com&quot;$ git push origin master登录阿里云域名控制台在云解析DNS选项卡中找到自己购买的域名选项，并点击“解析”在“解析设置”一栏中选择“添加解析”，记录类型填写为CNAME，主机记录填写www，记录值在这里以我的为例填写demo233.github.io之后保存。推荐一片博文解析域名的时候不同的项目代表什么含义？主机记录、记录类型、线路类型、记录值、MX优先级、TTL推荐 : http://jmcglone.com/guides/github-pages/","categories":[{"name":"其他","slug":"其他","permalink":"luoxiao.cf/categories/其他/"}],"tags":[]}]}